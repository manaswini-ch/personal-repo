import "templates/artifact_ingestion/datatype_mapping.stg"

table_metadata_query(templateData)::=<<
{
  "input_data": $generate_metadata_input(templateData.query_input)$,
   "sequential_templates":[
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "fetch_source_type",
        "query_output_key": "fetch_source_type",
        "query_type":"select"
      },
       {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "fetching_script_inputs",
         "query_output_key": "spark_script_inputs",
         "query_type":"select"
       },
       {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "get_table_adv_opt_partition_details",
          "query_output_key": "table_adv_opt_details_partition_details",
          "query_type":"select"
       },
       {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "get_remove_duplicates_data",
          "query_output_key": "remove_duplicates_data",
          "query_type":"select"
       },
       {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "get_sort_columns_data",
           "query_output_key": "sort_columns_data",
           "query_type":"select"
       },
     {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "get_table_adv_opt_details",
         "query_output_key": "table_adv_opt_details",
         "query_type":"select"
       },
      {
            "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
            "query_template_name": "advanced_options_tables_flags",
            "query_output_key": "advanced_options_tables_flags",
            "query_type":"select"
       },
       {
                   "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
                   "query_template_name": "advanced_options_tables_flags_status",
                   "query_output_key": "advanced_options_tables_flags_status",
                   "query_type":"select"
        },
	   {
               "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
               "query_template_name": "sort_by_columns_data",
               "query_output_key": "sort_by_columns_data",
               "query_type":"select"
       },
      {
             "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
             "query_template_name": "incremental_load_details",
             "query_output_key": "incremental_load_details",
             "query_type":"select"
       },
      {
            "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
            "query_template_name": "get_place_holders_with_default_and_max_values",
            "query_output_key": "place_holders_with_default_and_max_values",
            "query_type":"select"
      },
      {
            "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
            "query_template_name": "replace_place_holders_in_wherecondition_with_default_and_max_values",
            "query_output_key": "updated_wherecondition_replaced_with_default_and_max_values",
            "query_type":"select"
      },
      {
           "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
           "query_template_name": "get_incremental_max_columns_with_column_ids",
           "query_output_key": "incremental_max_columns_with_column_ids",
           "query_type":"select"
      },
      {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "generate_list_of_maps_of_incremental_columns",
          "query_output_key": "list_of_maps_of_incremental_columns",
          "query_type":"select"
      },
      {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "get_already_renamed_condition",
          "query_output_key": "get_already_renamed_condition",
          "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_ingestion_information",
        "query_output_key": "table_metadata_output",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_ingestion_mode",
        "query_output_key": "ingestion_mode",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_cdc_type",
        "query_output_key": "get_cdc_type",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_cdc_info",
        "query_output_key": "get_cdc_info",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_prev_cdc_id",
        "query_output_key": "get_prev_cdc_id",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_new_cdc_id",
        "query_output_key": "get_new_cdc_id",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "cdc_first_time",
        "query_output_key": "cdc_first_time",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "update_cdc_xref",
        "query_output_key": "update_cdc_xref",
        "query_type":"update"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_cdc_xref",
        "query_output_key": "insert_cdc_xref",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_cdc_audit_cols",
        "query_output_key": "get_cdc_audit_cols",
        "query_type":"select"
      },
       {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_view_info",
        "query_output_key": "get_view_info",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_primary_key_cols",
        "query_output_key": "get_primary_key_cols",
        "query_type":"select"
      },
      {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "get_already_ingested_condition",
         "query_output_key": "get_already_ingested_condition",
         "query_type":"select"
      },
       {
       "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
       "query_template_name": "get_partition_information_test",
       "query_output_key": "table_partition_output",
       "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "fetch_object_column_metadata",
        "query_output_key": "column_metadata_output",
        "query_type":"select"
      },
      {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "fetch_destination_transformation_query_details",
          "query_output_key": "destination_transformation_query_details_output",
          "query_type":"select"
      },
      {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "fetch_virtual_column_details",
         "query_output_key": "virtual_column_metadata_output",
         "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_partition_cols",
        "query_output_key": "get_partition_cols",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_list_of_maps_of_partition_columns",
        "query_output_key": "get_list_of_maps_of_partition_columns",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_non_partition_cols",
        "query_output_key": "get_non_partition_cols",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_into_process_id_table_map",
        "query_output_key": "insert_into_process_id_table_map",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "generate_credential_json",
        "query_output_key": "base64_encrypted_json_creds",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "BASE64"
                                }
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "convert_base64_encrypted_json_creds_to_rsa",
        "query_output_key": "base64_rsa_compressed_encrypted_creds",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "RSA_BASE64"
                               }
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "generate_source_input_json_for_checkpoint",
        "query_output_key": "input_json_for_checkpoint_status",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "BASE64"
                               }
      },
      {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "generate_almaren_artifact",
         "query_output_key": "encrypted_script",
         "query_type":"json",
         "encryption_details" : {
                                     "encryption" : "yes",
                                     "encryption_type" : "GZIP_BASE64"
                                 }
      }

   ],
   "output_keys":["get_list_of_maps_of_partition_columns","advanced_options_tables_flags_status","get_partition_cols","get_non_partition_cols","table_partition_output","fetch_source_type","table_metadata_output","ingestion_mode","get_already_ingested_condition","column_metadata_output","encrypted_script","input_data","table_adv_opt_details","incremental_load_details", "place_holders_with_default_and_max_values", "updated_wherecondition_replaced_with_default_and_max_values", "incremental_max_columns_with_column_ids", "list_of_maps_of_incremental_columns",
   "base64_rsa_compressed_encrypted_creds","input_json_for_checkpoint_status","spark_script_inputs","remove_duplicates_data","sort_by_columns_data","destination_transformation_query_details_output","get_cdc_info","get_cdc_type","get_cdc_audit_cols","cdc_first_time","get_view_info"]
}
>>

get_already_ingested_condition(templateData)::=<<
with ingested_object_count as(
select
	count(object_id) as count
from
	nabu.structured_jobtotable
where
	data_movement_id = $templateData.input_data.data_movement_id$
	and dataplace_id =
	 $if(first(templateData.fetch_source_type).is_source_filetype)$
     $first(templateData.table_metadata_output).source_dataplace_id$
     $else$ $first(templateData.table_metadata_output).dataplace_id$ $endif$
	and object_id = $templateData.input_data.where_condition$ )
select
	case
		when count>0 then true
		else false
	end as already_ingested
from
	ingested_object_count
>>

get_ingestion_mode(templateData)::=<<
$if(first(templateData.table_metadata_output).schema_drift_flag)$
select
	data_movement_id ,
	'Overwrite' as save_mode,
	'overwrite' as mode,
	(advance_option_details->'advanced_option_flags'->\>'ingestion_mode')::bool as ingestion_mode_key,
	true as drop_create_condition
from
	nabu.advanced_options_object_details
where
	data_movement_id = $templateData.input_data.data_movement_id$
	and
valid_to_ts = '9999-12-31'
	and
object_id = $templateData.input_data.where_condition$
$else$
select
	data_movement_id ,
	case
		when lower((advance_option_details->'advanced_table_options'->\>'ingestion_mode')::json->\>'mode')= 'drop & re-create' then 'Overwrite'
		else (advance_option_details->'advanced_table_options'->\>'ingestion_mode')::json->\>'mode'
	end as save_mode,
	case
		when lower((advance_option_details->'advanced_table_options'->\>'ingestion_mode')::json->\>'mode')= 'drop & re-create' then 'overwrite'
		else lower((advance_option_details->'advanced_table_options'->\>'ingestion_mode')::json->\>'mode')
	end as mode,
	(advance_option_details->'advanced_option_flags'->\>'ingestion_mode')::bool as ingestion_mode_key,
	case
		when lower((advance_option_details->'advanced_table_options'->\>'ingestion_mode')::json->\>'mode') = 'drop & re-create' then true
		else false
	end as drop_create_condition
from
	nabu.advanced_options_object_details
where
	data_movement_id = $templateData.input_data.data_movement_id$
	and
valid_to_ts = '9999-12-31'
	and
object_id = $templateData.input_data.where_condition$
$endif$
>>

partition_column_metadata(templateData)::=<<
    select
        column_data->\>'field_name' as field_name,
        column_data->\>'column_name' as column_name,
        column_data->\>'column_id' as column_id,
        column_data->\>'source_datatype_name' as source_datatype_name,
        column_data->\>'ingested_colorder' as ingested_colorder,
        column_data->\>'destination_datatype_name' as destination_datatype_name,
        (column_data->\>'is_partitioned_col')::boolean as is_partitioned_col,
        (column_data->\>'is_virtual')::boolean as is_virtual
    from(select json_array_elements(('[$templateData.destination_transformation_query_details_output:generate_json();separator=","$]')::json) as column_data)column_metadata
>>

generate_json(col)::=<<
{
    "field_name": "$col.field_name$" ,
    "column_name": "$col.column_name$",
    "column_id": "$col.column_id$",
    "source_datatype_name": "$col.source_datatype_name$",
    "ingested_colorder": "$col.ingested_colorder$",
    "destination_datatype_name": "$col.destination_datatype_name$",
    "is_partitioned_col": $if(!col.is_virtual)$ $col.is_partitioned_col$ $else$ false $endif$,
    "is_virtual":$col.is_virtual$
}
>>

get_partition_cols(templateData)::=<<
$if(first(templateData.fetch_source_type).is_source_filetype)$select 'null' $else$
    select field_name,column_name,column_id, source_datatype_name,ingested_colorder, destination_datatype_name, is_partitioned_col,is_virtual,false :: boolean as is_auditcol, row_number() over (order by column_name)+5 as pos
from($partition_column_metadata(templateData)$
   )partition_col_data where is_partitioned_col = true and is_virtual = false
    order by ingested_colorder
$endif$
>>

get_list_of_maps_of_partition_columns(templateData)::=<<
$if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
select replace(encode('{"DistinctColumnMapList":[$templateData.get_partition_cols:generate_column_metadata_json();separator=","$]}'::text::bytea,'base64'),E'\n','') as encoded_partition_cols $else$
select 'null' $endif$
>>

generate_column_metadata_json(map)::=<<
{"column_id":$map.column_id$, "column_name":"$map.field_name$"}
>>

get_non_partition_cols(templateData)::=<<
$if(first(templateData.fetch_source_type).is_source_filetype)$select 'null' $else$
    select field_name,column_name,column_id, source_datatype_name,ingested_colorder, destination_datatype_name, is_partitioned_col,is_virtual,false :: boolean as is_auditcol
from($partition_column_metadata(templateData)$
    )partition_col_data where is_partitioned_col = false
    order by ingested_colorder
$endif$
>>

get_table_adv_opt_partition_details(templateData)::=<<
Select t.*,case when ((((advance_option_details ->\>'advanced_table_options')::json ->\> 'parallel_ingestion')::json)
->\> 'enable') = 'true'
           then true
           else false end as is_parallel_ingestion,case when ((((advance_option_details ->\>'advanced_table_options')::json ->\>'parallel_ingestion')::json) ->\> 'enable') = 'true'
           then ((((advance_option_details ->\>'advanced_table_options')::json ->\> 'parallel_ingestion')::json) ->\> 'max_connections')
           else null end as maximum_parallel_connections from nabu.advanced_options_object_details t where object_id =$templateData.input_data.where_condition$ and
data_movement_id=$templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
>>

fetch_virtual_column_details(templateData)::=<<
 select decoded_expression|| ' as ' || column_name || '' as spark_expression from
(
    select *,lower(convert_from(decode(spark_sql_expression,'base64'), 'UTF8')) as decoded_expression from
	(
	    SELECT (jsonarray ->\>'column_name')::text as column_name,(jsonarray ->\>'column_type')::text as column_type,(jsonarray ->\>'spark_sql_expression')::text as spark_sql_expression
	from
	(   select json_array_elements((advance_option_details->'advanced_table_options'->'columns_options'-> 'virtual_column')) :: json as jsonarray
		from nabu.advanced_options_object_details
		where data_movement_id =$templateData.input_data.data_movement_id$ and object_id=$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31')a)b)c

>>


get_remove_duplicates_data(templateData) ::=<<
select
case when((partition_columns ->\>'renamed_column')='')
then (partition_columns ->\>'column_name')
else
(partition_columns ->\>'renamed_column')
end as partition_column_name
from(
select json_array_elements (advance_option_details ->'advanced_table_options'->'remove_duplicates'->'key_columns' )::json as partition_columns from nabu.advanced_options_object_details where data_movement_id =$templateData.input_data.data_movement_id$ and
valid_to_ts ='9999-12-31' and object_id in ($templateData.input_data.where_condition$)
) jsonarray
>>

get_sort_columns_data(templateData) ::=<<
SELECT
(sort_columns ->\>'column_name') as sort_column_name,
concat((sort_columns ->\>'column_name'),'_tmp_rm') as sort_column_order_by_name,
(sort_columns ->\> 'sort_by') as sort_order,
case when((sort_columns ->\> 'sort_null_values')= '')
then (sort_columns ->\>'')
else
(sort_columns->\> 'sort_null_values')
end as sort_null_value
from
(
select
json_array_elements (advance_option_details ->'advanced_table_options'->'remove_duplicates'->'sort_columns' ) as sort_columns from nabu.advanced_options_object_details where data_movement_id =$templateData.input_data.data_movement_id$ and
valid_to_ts ='9999-12-31' and object_id in ($templateData.input_data.where_condition$)
) sort
>>

get_table_adv_opt_details(templateData)::=<<
Select *, (advance_option_details->'advanced_table_options'->\>'dataplace_id')::int as dataplace_id from nabu.advanced_options_object_details where object_id =$templateData.input_data.where_condition$ and data_movement_id=$templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
>>

advanced_options_tables_flags(templateData)::=<<
select advance_option_details ->\> 'advanced_option_flags'  as advanced_options_flags
from nabu.advanced_options_object_details
where data_movement_id= $templateData.input_data.data_movement_id$
and valid_to_ts ='9999-12-31'
and object_id in ($templateData.input_data.where_condition$)
>>

advanced_options_tables_flags_status(templateData)::=<<
$if(first(templateData.advanced_options_tables_flags).advanced_options_flags)$
select * from json_to_record(
'$first(templateData.advanced_options_tables_flags).advanced_options_flags$'
) as x(
$generate_flags(templateData)$
)
$else$
select $generate_flags(templateData)$
$endif$
>>

generate_flags(templateData)::=<<
$["incremental_load",
  "columns_options",
  "modified_columns",
  "excluded_column_ids",
  "virtual_column",
  "reorder_columns",
  "remove_duplicates",
  "sort_by_columns",
	"table_partitions",
  "new_table_name",
  "parallel_ingestion",
  "CDC",
  "ingest_using_query",
  "ingestion_mode"]:generate_flag_status(templateData);separator=",\n"$
>>

generate_flag_status(flag_name,templateData)::=<<
$if(first(templateData.advanced_options_tables_flags).advanced_options_flags)$$flag_name$ boolean $else$ false as $flag_name$ $endif$
>>

sort_by_columns_data(templateData)::=<<
select
columns_list ->\>'order_by' as order_by,
case when((columns_list ->\> 'sort_null_values')= '')
        then (columns_list ->\>'')
    else concat('_',replace((columns_list->\> 'sort_null_values'),' ','_'))
end as sort_null_value,
case when(columns_list ->\>'renamed_column' ='')
        then columns_list ->\>'column_name'
	else columns_list ->\>'renamed_column'
end as final_column_name
from (
select
json_array_elements(((advance_option_details ->\>'advanced_table_options')::json->\>'sort_by_columns')::json) as columns_list
from nabu.advanced_options_object_details
where data_movement_id= $templateData.input_data.data_movement_id$ and valid_to_ts ='9999-12-31' and object_id in ($templateData.input_data.where_condition$)
)as advance_option_flags
>>

incremental_details_template()::=<<
select regexp_replace(incremental_load_where_clause,'\''', '''''','g') as incremental_load_where_clause, condition_values from (
    select convert_from (decode(incremental_load_json->\>'where_clause','base64'),'utf-8') as incremental_load_where_clause,
    incremental_load_json->'condition_values' as condition_values
    from (
        select (('$first(templateData.table_adv_opt_details).advance_option_details$')::json->\>'advanced_table_options')::json->'incremental_load' as incremental_load_json
    ) a
) b
>>

incremental_load_details(templateData)::=<<
$if(first(templateData.advanced_options_tables_flags_status).incremental_load)$
$incremental_details_template()$
$else$
select 1
$endif$
>>

get_records_from_incremental_load_conditions_template()::=<<
get_records_from_incremental_load_condition_values_json as (
    select incremental_load_where_clause, place_holder, default_value, max_value as max_incremental_load_flag, max_column_name as column_name, max_column_id as column_id from incremental_load_details,
    json_to_recordset(incremental_load_details.condition_values) as x("place_holder" text, "default_value" text, "max_value" boolean, "max_column_name" text, "max_column_id" int8)
)
>>

get_place_holders_with_default_and_max_values(templateData)::=<<
$if(first(templateData.incremental_load_details).incremental_load_where_clause)$
with incremental_load_details as (
$incremental_details_template()$
),
$get_records_from_incremental_load_conditions_template()$
,
get_place_holders_and_its_default_values as (
    select incremental_load_where_clause, place_holder, default_value, max_incremental_load_flag, column_name, column_id
    from get_records_from_incremental_load_condition_values_json
    where max_incremental_load_flag is false
),
get_max_columns_and_placeholders as (
    select incremental_load_where_clause, place_holder, default_value, max_incremental_load_flag, column_name, column_id
    from get_records_from_incremental_load_condition_values_json
    where max_incremental_load_flag is true
),
get_columns_and_maxvalues_json_from_checkpoint as (
    select process_id, additional_info->'IncrementalColumnMapResultList' as columns_and_max_values_placeholders_json
    from nabu.advanced_options_obj_checkpoint
    where data_movement_id = $first(templateData.table_adv_opt_details).data_movement_id$
    and source_dataplace_id = $first(templateData.table_adv_opt_details).dataplace_id$
    and object_id = $first(templateData.table_adv_opt_details).object_id$
    and advanced_option_obj_type_id = 6
    and process_id in (select process_id from nabu.flow_task_status
                       where data_movement_id = $first(templateData.table_adv_opt_details).data_movement_id$
                       and object_id = $first(templateData.table_adv_opt_details).object_id$
                       and status_code_id = 15
                       order by flow_task_ts desc limit 1)
    and valid_to_ts = '9999-12-31'
),
get_records_from_columns_and_maxvalues_json as (
    select  column_id, column_name, max_value as max_value_of_column from get_columns_and_maxvalues_json_from_checkpoint,
           json_to_recordset(get_columns_and_maxvalues_json_from_checkpoint.columns_and_max_values_placeholders_json) as x("column_id" int8, "column_name" text, "max_value" text)
),
get_place_holders_and_its_max_values as (
    select incremental_load_where_clause, place_holder,
    case when b.max_value_of_column is null then a.default_value else b.max_value_of_column end as place_holder_value,
    max_incremental_load_flag, a.column_name, a.column_id
    from get_max_columns_and_placeholders a
    left outer join get_records_from_columns_and_maxvalues_json b
    on a.column_id = b.column_id
)
select '<%'||place_holder||'%>' as place_holder, place_holder_value, incremental_load_where_clause, max_incremental_load_flag, column_name, column_id
from get_place_holders_and_its_max_values
union all
select '<%'||place_holder||'%>' as place_holder, default_value as place_holder_value, incremental_load_where_clause, max_incremental_load_flag, column_name, column_id
from get_place_holders_and_its_default_values
$else$
select 1
$endif$
>>

replace_place_holders_in_wherecondition_with_default_and_max_values(templateData)::=<<
$if(first(templateData.place_holders_with_default_and_max_values).place_holder)$
select $templateData.place_holders_with_default_and_max_values:getRegexReplace()$'$first(templateData.place_holders_with_default_and_max_values).incremental_load_where_clause$', $templateData.place_holders_with_default_and_max_values:getPlaceHoldersAndItsValues("place_holder","place_holder_value");separator=","$ as incremental_load_where_clause
$elseif(first(templateData.incremental_load_details).incremental_load_where_clause)$
select '$first(templateData.incremental_load_details).incremental_load_where_clause$' as incremental_load_where_clause
$else$
select 1
$endif$
>>

getRegexReplace(map)::=<<
regexp_replace(
>>

getPlaceHoldersAndItsValues(map,key1,key2)::=<<
'$map.(key1)$','$map.(key2)$')
>>

get_incremental_max_columns_with_column_ids(templateData)::=<<
$if(first(templateData.place_holders_with_default_and_max_values).column_name)$
with incremental_load_details as (
$incremental_details_template()$
),
$get_records_from_incremental_load_conditions_template()$
select column_name, column_name||'_tmp_inc' as incremental_max_column_name, column_id
                     from get_records_from_incremental_load_condition_values_json
where max_incremental_load_flag is true
$else$
select 1
$endif$
>>

generate_list_of_maps_of_incremental_columns(templateData)::=<<
$if(first(templateData.incremental_max_columns_with_column_ids).column_name)$
select replace(encode('{"IncrementalColumnMapList":[$templateData.incremental_max_columns_with_column_ids:getIdsAndNamesOfColumns();separator=","$]}'::text::bytea, 'base64'),E'\n','') as encoded_list_of_maps_of_incremental_columns
$else$
select 1
$endif$
>>

getIdsAndNamesOfColumns(map)::=<<
{"column_id":$map.column_id$, "column_name":"$map.incremental_max_column_name$", "original_column_name":"$map.column_name$"}
>>

fetch_source_type(templateData)::=<<
with CTE1 as (
select distinct dm.data_movement_id,dcml.metadata_category as source_type, dscl.dataplace_sub_component_type as source from nabu.data_movement_details_physical dm inner join nabu.dataplace_physical dp on dp.dataplace_id=dm.source_dataplace_id
inner join  nabu.dataplace_component_mapping_lookup dcml on dcml.dataplace_sub_component_id =dp.dataplace_sub_component_id
inner join nabu.dataplace_sub_component_lookup dscl on dcml.dataplace_sub_component_id =dscl.dataplace_sub_component_id
where dm.data_movement_id=$templateData.input_data.data_movement_id$ and dp.valid_to_ts='9999-12-31 00:00:00' and dm.valid_to_ts ='9999-12-31 00:00:00'
),
CTE2 as (
select distinct dm.data_movement_id,dcml.metadata_category as destination_type, dscl.dataplace_sub_component_type as destination,dscl.dataplace_sub_component_id as id from nabu.data_movement_details_physical dm inner join nabu.dataplace_physical dp on dp.dataplace_id=dm.destination_dataplace_id
inner join  nabu.dataplace_component_mapping_lookup dcml on dcml.dataplace_sub_component_id =dp.dataplace_sub_component_id
inner join nabu.dataplace_sub_component_lookup dscl on dcml.dataplace_sub_component_id =dscl.dataplace_sub_component_id
where dm.data_movement_id=$templateData.input_data.data_movement_id$ and dp.valid_to_ts='9999-12-31 00:00:00' and dm.valid_to_ts ='9999-12-31 00:00:00'
)
select a.data_movement_id,a.source_type as source_metadata_category,b.destination_type as destination_metadata_category, a.source , b.destination  ,
 case when(a.source_type = 'file') then true else false end as is_source_filetype ,
 case when(b.destination_type = 'file') then true else false end as is_destination_filetype
,case when b.id in (7,11,25,27) then true else false end as is_ordinalpos_startwith_zero from CTE1 a
inner join
CTE2 b on a.data_movement_id=b.data_movement_id
>>

get_cdc_type(templateData)::=<<
select case when (advance_option_details->'advanced_table_options'->'CDC'->\>'add_audit_columns')::boolean
then true else false end as cdctype2,
'$templateData.input_data.process_id$'||'_$first(templateData.table_metadata_output).destination_table_name$' as temp_path,
(advance_option_details->'advanced_option_flags'->\>'CDC')::boolean as cdc_applied,
case when json_array_length(advance_option_details->'advanced_table_options'->'CDC'->'columns_ids')>0 then true else false end as is_primary
from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
>>

get_prev_cdc_id(templateData)::=<<
select case when process_id=$templateData.input_data.process_id$ then prev_process_id else process_id end as prev_process_id from nabu.cdc_xref cx where
data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$
and cdc_end_ts!='9999-12-31'
order by cdc_start_ts desc limit 1
>>


get_new_cdc_id(templateData)::=<<
select $templateData.input_data.process_id$ as process_id,$getPreviousCDCId(first(templateData.get_prev_cdc_id))$ as prev_process_id
>>

getPreviousCDCId(templateData)::=<<
$if(templateData.prev_process_id)$$templateData.prev_process_id$$else$cast((extract(epoch from now())*10000) as bigint)$endif$
>>

update_cdc_xref(templateData)::=<<
$if(first(templateData.get_cdc_type).cdc_applied)$
update nabu.cdc_xref
set valid_to_ts = current_timestamp
where data_movement_id=$templateData.input_data.data_movement_id$
and dataplace_id=$first(templateData.table_metadata_output).dataplace_id$
and object_id=$templateData.input_data.where_condition$
and valid_to_ts='9999-12-31'
$endif$
>>

insert_cdc_xref(templateData)::=<<
$if(first(templateData.get_cdc_type).cdc_applied)$
INSERT INTO nabu.cdc_xref
(data_movement_id, dataplace_id, advanced_options_obj_type_id, object_id, process_id, prev_process_id, cdc_start_ts, cdc_end_ts, additional_info, valid_from_ts, valid_to_ts)
select $templateData.input_data.data_movement_id$,$first(templateData.table_metadata_output).dataplace_id$,
8,$templateData.input_data.where_condition$,$first(templateData.get_new_cdc_id).process_id$,$first(templateData.get_new_cdc_id).prev_process_id$,current_timestamp,TIMESTAMP '9999-12-31 00:00:00','{}'::json,current_timestamp,TIMESTAMP '9999-12-31 00:00:00'
$endif$
>>


get_cdc_info(templateData)::=<<
select case when lower(advance_option_details->'advanced_table_options'->'CDC'->\>'hash_type') like '%sha%' then 'sha2' else lower(advance_option_details->'advanced_table_options'->'CDC'->\>'hash_type') end as hash_type,
(advance_option_details->'advanced_table_options'->'CDC'->\>'add_audit_columns')::boolean as add_audit_columns,
(advance_option_details->'advanced_table_options'->'CDC'->\>'create_view')::boolean as create_view,
advance_option_details->'advanced_table_options'->'CDC'->\>'view_name' as view_name,
case when lower(advance_option_details->'advanced_table_options'->'CDC'->\>'hash_type') like '%sha%' then true else false end as is_sha2,
$replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'start_date_column_name'")$ as start_date_column_name,
$replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'end_date_column_name'")$ as end_date_column_name,
$replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'process_id_column_name'")$ as job_id_column_name,
$replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'prev_process_id_column_name'")$ as prev_job_id_column_name
from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
>>

get_cdc_audit_cols(templateData)::=<<
select a.field_name ,b.destination_datatype_name, true::boolean as is_auditcol from(
select $replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'start_date_column_name'")$ as field_name, 'string' as intermediate_datatype_name
from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
union all
select $replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'end_date_column_name'")$ as field_name,'string' as intermediate_datatype_name
from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
union all
select $replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'process_id_column_name'")$ as field_name,'long' as intermediate_datatype_name
from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
union all
select $replacing_special_characters("advance_option_details->'advanced_table_options'->'CDC'->'audit_columns'->\>'prev_process_id_column_name'")$ as field_name,'long' as intermediate_datatype_name
from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
)a inner join (
select intermediate_datatype_name ,destination_datatype_name  from nabu.datatype_mapping dm where source_type ='virtual' and destination_type ='$first(templateData.fetch_source_type).destination$' and intermediate_type='$first(templateData.table_metadata_output).intermediate_type$'
)b
on a.intermediate_datatype_name=b.intermediate_datatype_name
>>

get_view_info(templateData)::=<<
$if(first(templateData.get_cdc_type).cdc_applied)$
select * from nabu.dataplace_table_metadata_physical dtmp where schema_id =$first(templateData.table_metadata_output).destination_schema_id$
and table_name ='$first(templateData.get_cdc_info).view_name$'
and dataplace_id =$first(templateData.table_metadata_output).destination_dataplace_id$
and valid_to_ts ='9999-12-31'
$else$
select 1 as id
$endif$
>>

cdc_first_time(templateData)::=<<
select case when count(*)>0 then false else true end as cdc_first_time from nabu.advanced_options_obj_checkpoint
where data_movement_id =$templateData.input_data.data_movement_id$
and source_dataplace_id =$if(first(templateData.fetch_source_type).is_source_filetype)$$first(templateData.table_metadata_output).source_dataplace_id$$else$ $first(templateData.table_metadata_output).dataplace_id$ $endif$
and object_id =$templateData.input_data.where_condition$
and advanced_option_obj_type_id =8
and valid_to_ts ='9999-12-31'
>>

get_primary_key_cols(templateData)::=<<
with col_ids as(
select json_array_elements_text(advance_option_details->'advanced_table_options'->'CDC'->'columns_ids')::int8 as column_id
from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
),
get_field_names as(
select $replacing_special_characters("dcmp.column_name")$ as field_name from nabu.dataplace_column_metadata_physical dcmp
inner join col_ids ci on dcmp.column_id=ci.column_id
where dcmp.valid_to_ts ='9999-12-31'
)
select * from get_field_names
>>

advanced_table_options_util(templateData)::=<<
{
  "input_data": $generate_cdc_required_input(templateData.data.data.templateData.query_input)$,
   "sequential_templates":[
 	  {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "get_distinct_values_from_kosh",
         "query_output_key": "get_distinct_values_from_kosh",
         "query_type":"select"
      },
 	  {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "get_conditions_for_alter",
         "query_output_key": "get_conditions_for_alter",
         "query_type":"select"
      }
$if(first(templateData.data.data.templateData.query_input.get_cdc_type).cdc_applied)$
	  ,{
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "update_cdc_endts_xref",
        "query_output_key": "update_cdc_endts_xref",
        "query_type":"update"
      }
$endif$
   ],
   "output_keys":["get_conditions_for_alter"]
}
>>

get_distinct_values_from_kosh(templateData)::=<<
$if(templateData.input_data.table_partitions)$
with partition_cols as(
select json_array_elements(additional_info->'DistinctColumnMapResultList') as partition_cols_distinct_vals
from nabu.advanced_options_obj_checkpoint aooc
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts ='99991231'
and object_id in ($templateData.input_data.object_id$)
and process_id = $templateData.input_data.process_id$
),
distinct_values as(
select (partition_cols_distinct_vals->\>'column_id')::int as column_id,
partition_cols_distinct_vals->\>'column_name' as column_name,
json_array_elements(partition_cols_distinct_vals->'distinct_value')::text as distinct_value
from partition_cols
),
all_distinct_values as(
select replace(column_name||'='''''||distinct_value||'''''','"','') as partition_col_value,
column_name,column_id
from distinct_values
group by column_name,distinct_value,column_id
)
select column_id,
column_name,
string_agg(partition_col_value,',') as cte_content
from all_distinct_values
group by column_name,column_id
order by column_id
$else$
select 1 as id where 1=0
$endif$
>>

get_conditions_for_alter(templateData)::=<<
$if(templateData.input_data.table_partitions)$
with $generate_cte(templateData.get_distinct_values_from_kosh)$
select concat($templateData.get_distinct_values_from_kosh:get_cte_names();separator=",',',"$) as partition_condition,
replace(concat($templateData.get_distinct_values_from_kosh:get_cte_names();separator=",'/',"$),'''','') as partition_path
from $templateData.get_distinct_values_from_kosh:get_cte_names();separator=","$
$else$
select 1 as id where 1=0
$endif$
>>

get_cte_names(col)::=<<
$col.column_name$
>>

generate_cte(column_metadata)::=<<
$column_metadata:{
col |
$col.column_name$ as (
select unnest(string_to_array(replace('$col.cte_content$','null','__HIVE_DEFAULT_PARTITION__'),',')) as $col.column_name$
)};separator=","$
>>

generate_cdc_required_input(templateData)::=<<
{
"data_movement_id" : "$first(templateData.table_metadata_output).data_movement_id$",
"dataplace_id": "$first(templateData.table_metadata_output).dataplace_id$",
"advanced_options_obj_type_id": "2",
"object_id": "$first(templateData.table_metadata_output).where_condition$",
"process_id" : "$first(templateData.table_metadata_output).process_id$",
"table_partitions" : $first(templateData.advanced_options_tables_flags_status).table_partitions$
}
>>

update_cdc_endts_xref(templateData)::=<<
update nabu.cdc_xref
set cdc_end_ts=now()
where data_movement_id=$templateData.input_data.data_movement_id$
and object_id=$templateData.input_data.object_id$
and valid_to_ts='9999-12-31'
>>

insert_cdc_advanced_options_obj_checkpoint(templateData)::=<<
INSERT INTO nabu.advanced_options_obj_checkpoint
(data_movement_id, source_dataplace_id, object_id, advanced_option_obj_type_id, process_id, additional_info, valid_from_ts, valid_to_ts)
select $templateData.input_data.data_movement_id$,(select distinct dataplace_id from nabu.dataplace_table_metadata_physical dtmp where table_id =$templateData.input_data.where_condition$), $templateData.input_data.where_condition$, 8, $templateData.input_data.process_id$, '{}'::json, now(), '9999-12-31 00:00:00.000';
>>

update_cdc_advanced_options_obj_checkpoint(templateData)::=<<
update nabu.advanced_options_obj_checkpoint
set valid_to_ts=now()
where data_movement_id=$templateData.input_data.data_movement_id$
and object_id=$templateData.input_data.where_condition$
and advanced_option_obj_type_id=8
and valid_to_ts='9999-12-31'
>>

table_metadata_query_almaran_spark(templateData)::=<<
{
  "input_data": $generate_curation_metadata_input(templateData)$,
   "sequential_templates":[
       {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "fetching_script_inputs",
         "query_output_key": "spark_script_inputs_for_almaren_pyspark",
         "query_type":"select"
       },
      {
       "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
       "query_template_name": "get_source_type",
       "query_output_key": "get_source_type",
       "query_type":"select"
       },
      {
           "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
           "query_template_name": "generate_source_input_json_for_curation",
           "query_output_key": "input_json_for_curation",
           "query_type":"json",
           "encryption_details" : {
                                       "encryption" : "yes",
                                       "encryption_type" : "BASE64"
                                  }
       },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_metadata_information_pyspark_almaren",
        "query_output_key": "table_metadata_output_almaren_pyspark",
        "query_type":"select"
      },
       {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_into_process_id_table_map",
        "query_output_key": "insert_into_process_id_table_map",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "generate_spark_submit_command",
        "query_output_key": "generate_spark_submit_command",
        "query_type":"json",
        "encryption_details" : {
                                    "encryption" : "yes",
                                    "encryption_type" : "BASE64"
                               }
      }
   ],
   "output_keys":["spark_script_inputs_for_almaren_pyspark","table_metadata_output_almaren_pyspark","get_source_type","table_metadata_output","column_metadata_output","input_data","generate_spark_submit_command"]
}
>>

table_metadata_query_for_cleanup(templateData)::=<<
{
  "input_data": $generate_metadata_input(templateData.query_input)$,
   "sequential_templates":[
       {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "fetching_script_inputs",
         "query_output_key": "spark_script_inputs_for_cleanup",
         "query_type":"select"
       },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_metadata_information_for_cleanup",
        "query_output_key": "table_metadata_output_for_cleanup",
        "query_type":"select"
      }
   ],
   "output_keys":["spark_script_inputs_for_cleanup","table_metadata_output_for_cleanup"]
}
>>

generate_curation_metadata_input(templateData)::=<<
{
"data_movement_id" : $templateData.query_input.data_movement_id$,
"flow_number" : $templateData.query_input.flow_number$,
"where_condition":"$templateData.query_input.where_condition$",
"process_id" : $templateData.query_input.process_id$,
"jwt_token" : "$templateData.query_input.botLogicOutputMap.jwt_token$",
"end_point" : "$templateData.query_input.botLogicOutputMap.end_point$",
"batch_id": $templateData.query_input.batch_id$,
"batch_name": "$templateData.query_input.batch_name$",
"job_type_id": "$templateData.query_input.job_type_id$",
"job_scheduled_user_id": "$templateData.query_input.job_scheduled_user_id$",
"job_schedule_id": $templateData.query_input.job_schedule_id$,
"last_run_timestamp": "$templateData.query_input.last_run_timestamp$",
"metadata_category" : "$templateData.query_input.metadata_category$",
"source": "$templateData.query_input.source$",
"datamovement_engine_type" : "$templateData.query_input.datamovement_engine_type$",
"retry_attempt" : "$templateData.retry_attempt$"
}
>>

query_for_status_insertion(templateData)::=<<
{
  "input_data": $generate_status_insertion_required_input(templateData.query_input)$,
   "sequential_templates":[
       {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "insert_into_spark_job_result",
         "query_output_key": "insert_into_spark_job_result",
         "query_type":"insert"
       }
   ],
   "output_keys":[]
}
>>

fetching_script_inputs(templateData)::=<<
with fetch_ssh_configuration as (
select  case when c.config_json->\>'hostname' = '' or c.config_json->\>'hostname' is null then 'null' else c.config_json->\>'hostname' end as ssh_host,
        case when c.config_json->\>'username' = '' or c.config_json->\>'username' is null then 'null' else c.config_json->\>'username' end as ssh_username,
       case when b.compute_engine_info->\>'nabu_external_path'= '' or b.compute_engine_info->\>'nabu_external_path' is null  then 'null' else b.compute_engine_info->\>'nabu_external_path' end as spark_location,
        c.config_json->\>'kerberos' as kerberos,
        a.data_movement_id , b.engine_mapping_id,b.compute_engine_info
      from nabu.data_movement_physical a
inner join nabu.compute_engine b on a.compute_engine_id = b.compute_engine_id
inner join nabu.config c on  b.compute_engine_config_id = c.config_id
where a.data_movement_id =$templateData.input_data.data_movement_id$
)
select ssh_host,ssh_username,kerberos::boolean,d.spark_location,
case when engine_sub_type ~*'cdh|cdp' and kerberos ='false'
          then additional_properties ->\> 'non_kerberos_sub_engine_path'
          else additional_properties ->\> 'default_sub_engine_path' end as engine_path,replace(encode(compute_engine_info::text::bytea,'base64'),E'\n','') as compute_engine_info
from fetch_ssh_configuration d
inner join nabu.engine_mapping_lookup e on d.engine_mapping_id = e.engine_mapping_id
inner join nabu.engine_sub_type_lookup f on f.engine_sub_type_id = e.engine_sub_type_id
>>

generate_spark_submit_command(templateData)::=<<
$([first(templateData.get_source_type).dataplace_sub_component_type,"_spark_submit_command_generation"])(templateData)$
>>

almaren_spark_submit_command_generation(templateData)::=<<
$if(first(templateData.table_metadata_output_almaren_pyspark).is_cde)$
$(first(templateData.table_metadata_output_almaren_pyspark).cde_submit)$ $(first(templateData.table_metadata_output_almaren_pyspark).git_file_path)$ $(first(templateData.table_metadata_output_almaren_pyspark).jobname_conf)$ $(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).cluster_endpoint)$
$(first(templateData.table_metadata_output_almaren_pyspark).credentials_filepath)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$$(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).input_json)$
$else$
$(first(templateData.table_metadata_output_almaren_pyspark).cat_for_almaren)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$$(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).input_json)$$endif$
>>

pyspark_spark_submit_command_generation(templateData)::=<<
$if(first(templateData.table_metadata_output_almaren_pyspark).is_cde)$
$(first(templateData.table_metadata_output_almaren_pyspark).cde_submit)$ $(first(templateData.table_metadata_output_almaren_pyspark).git_file_path)$ $(first(templateData.table_metadata_output_almaren_pyspark).jobname_conf)$ $(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).cluster_endpoint)$
$(first(templateData.table_metadata_output_almaren_pyspark).credentials_filepath)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$$(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).input_json)$
$else$
$(first(templateData.table_metadata_output_almaren_pyspark).cat_for_pyspark)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_default_command_options)$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_bots_token_command_options)$
$if((first(templateData.spark_script_inputs_for_almaren_pyspark).kerberos))$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_kerberos_command_options)$
$endif$
$(first(templateData.table_metadata_output_almaren_pyspark).spark_fireshots_url)$$(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).input_json)$$(first(templateData.table_metadata_output_almaren_pyspark).endline)$
$(first(templateData.table_metadata_output_almaren_pyspark).git_file_path)$$endif$
>>

get_source_type(templateData)::=<<
select b.dataplace_sub_component_type from nabu.data_movement_physical a
inner join nabu.dataplace_sub_component_lookup b on a.dataplace_sub_component_id  = b.dataplace_sub_component_id
where data_movement_id  = $templateData.input_data.data_movement_id$ and valid_to_ts  = '9999-12-31'
>>

get_metadata_information_pyspark_almaren(templateData)::=<<
with cte as (
select $templateData.input_data.process_id$ as process_id,data_movement_id,'$templateData.input_data.jwt_token$' as jwt_token,
 convert_from(decode(((data_movement_additional_info->'flow_details')->'spark_config')->\>'spark_kerberos_command_options'||'==','base64'), 'utf-8') as spark_kerberos_command_options,
 regexp_replace(convert_from(decode(((data_movement_additional_info->'flow_details')->'spark_config')->\>'spark_bots_token_command_options'||'=','base64'), 'utf-8'),'<%bots_token%>','"$templateData.input_data.jwt_token$"'||' \') as spark_bots_token_command_options,
 regexp_replace(convert_from(decode(((data_movement_additional_info->'flow_details')->'spark_config')->\>'spark_default_command_options','base64'), 'utf-8'),'<%pipeline_name%>',data_movement_name) as spark_default_command_options,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_branch_or_tag' as git_branch_or_tag,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_file_path' as git_file_path,
'$(first(templateData.get_source_type).dataplace_sub_component_type)$' as curation_type,
'$templateData.input_data.retry_attempt$' as retry_attempt,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_url' as git_url,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'project_name' as project_name,
data_movement_name as pipeline_name,'--conf spark.nabu.fireshots_url="$templateData.input_data.end_point$"' as spark_fireshots_url,
'--conf spark.driver.nabu_inputjson="$templateData.input_json_for_curation$"' as input_json
from nabu.data_movement_physical where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts ='9999-12-31'
)
,
fetch_ssh_configuration as (
select
case when estl.engine_sub_type~*'cde' then true else false end as is_cde,
case when estl.engine_sub_type~*'standalone' then true else false end as is_standalone,
data_movement_id ,b.compute_engine_info
from nabu.data_movement_physical a
inner join nabu.compute_engine b on a.compute_engine_id = b.compute_engine_id
inner join nabu.engine_mapping_lookup eml on eml.engine_mapping_id =b.engine_mapping_id
inner join nabu.engine_sub_type_lookup estl on estl.engine_sub_type_id =eml.engine_sub_type_id
where a.data_movement_id =$templateData.input_data.data_movement_id$ and a.valid_to_ts ='9999-12-31'
),
spark_submit_details as(
select cte.*,fs.is_cde as is_cde,case when fs.is_cde
then
	concat('--vcluster-endpoint ',fs.compute_engine_info->\>'cluster_endpoint',' \') else null end as cluster_endpoint,
	case when fs.is_cde
then
	concat('--credentials-file ',fs.compute_engine_info->\>'credentials_filepath',' \') else null end as credentials_filepath,
	case when fs.is_cde
then
	concat('--user ',fs.compute_engine_info->\>'cde_user',' \') else null end as cde_user,
case when fs.is_standalone then concat(fs.compute_engine_info->\>'spark_home_path','/bin/','spark-submit  \') else 'spark-submit  \' end as cat_for_pyspark,
	case when fs.is_cde
then
	'cde spark submit'  end as cde_submit,
	        case when fs.is_cde
then
    'nabu-'||cte.curation_type||'-job-'||(select replace(to_char(current_timestamp ,'yyyy-mm-dd'),'-','') ||'-'||replace(to_char(current_timestamp ,'HH24-MI-SS'),'-','') ||'-'|| (SELECT floor(random() * 100000 + 1)::int))
    else null end as job_name,
      case when fs.is_cde
then	concat(cte.process_id,'_',cte.retry_attempt,'/',cte.project_name,'/',cte.git_file_path) else null end as gitfilepath ,
	'cat '||git_file_path||' | ' || case when fs.is_standalone then concat(fs.compute_engine_info->\>'spark_home_path','/bin/','spark-shell \') else 'spark-shell \' end as cat_for_almaren ,' \' as endline
from cte inner join fetch_ssh_configuration fs on cte.data_movement_id =fs.data_movement_id
)
select *,concat('--job-name ',job_name) as jobname_conf from spark_submit_details
>>

get_metadata_information_for_cleanup(templateData)::=<<
select b.process_id as process_id,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_branch_or_tag' as git_branch_or_tag,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_file_path' as git_file_path,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'git_url' as git_url,
(((data_movement_additional_info->'flow_details')->'git_info'))->\>'project_name' as project_name,
data_movement_name as pipeline_name
from nabu.data_movement_physical a
inner join nabu.process_id_table_map b
on a.data_movement_id = b.data_movement_id and batch_id = $templateData.input_data.batch_id$
where a.data_movement_id = $templateData.input_data.data_movement_id$
>>

// if case is for file as a destination and else for relational as a destination
insert_destination_info_into_nabu(templateData)::=<<
$if(templateData.query_input.data.data.data.is_create_table)$
$(["insert_destination_info_into_nabu_for_",templateData.query_input.data.query_input.source_metadata_category,"_to_",templateData.query_input.data.query_input.destination_metadata_category])(templateData)$
$else$
$insert_destination_info_into_nabu_for_relational_to_file(templateData)$
$endif$
>>

insert_destination_info_into_nabu_for_file_to_relational(templateData)::=<<
{
  "input_data": $generate_destination_required_input(templateData.query_input)$,
   "sequential_templates":[
	  {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_data_movement_info",
        "query_output_key": "data_movement_details",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_source_info_file",
        "query_output_key": "get_source_info_file",
        "query_type":"select"
      },
	  $if(first(templateData.query_input.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$
    	  {
            "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
            "query_template_name": "insert_rename_details_file",
            "query_output_key": "insert_rename_details_file",
            "query_type":"insert"
          },
	  $endif$
	  $if(first(templateData.query_input.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$
	   {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "update_destination_file_metadata",
        "query_output_key": "update_destination_file_metadata",
        "query_type":"update"
      },
	  $else$
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_destination_file_metadata",
        "query_output_key": "insert_destination_table_metadata",
        "query_type":"insert"
      },
	  $endif$
	  {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_destination_job_metadata_file",
        "query_output_key": "insert_destination_job_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_structured_jobtotable_file",
        "query_output_key": "insert_structured_jobtotable",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_metadata_for_data_lineage_file",
        "query_output_key": "get_metadata_for_data_lineage",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_data_entity_lineage_file",
        "query_output_key": "insert_data_entity_lineage",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_data_movement_errors",
        "query_output_key": "insert_data_movement_errors",
        "query_type":"insert"
      },
      {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "advanced_options_mapping_details",
         "query_output_key": "advanced_options_mapping_details",
         "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_data_movement_schema_drift_details_file",
        "query_output_key": "insert_data_movement_schema_drift_details",
        "query_type":"insert"
      }
	]
}
>>

insert_destination_info_into_nabu_for_relational_to_file(templateData)::=<<
{
  "input_data": $generate_destination_required_input(templateData.query_input)$,
   "sequential_templates":[
	  {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_data_movement_info",
        "query_output_key": "data_movement_details",
        "query_type":"select"
      },

      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_data_movement_errors",
        "query_output_key": "insert_data_movement_errors",
        "query_type":"insert"
      }
	],
       "output_keys":[]
}
>>

insert_destination_info_into_nabu_for_relational_to_relational(templateData)::=<<
{
  "input_data": $generate_destination_required_input(templateData.query_input)$,
   "sequential_templates":[
      {
            "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
            "query_template_name": "fetch_source_type",
            "query_output_key": "fetch_source_type",
            "query_type":"select"
      },
     {
           "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
           "query_template_name": "advanced_options_tables_flags",
           "query_output_key": "advanced_options_tables_flags",
           "query_type":"select"
      },
      {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "advanced_options_tables_flags_status",
          "query_output_key": "advanced_options_tables_flags_status",
          "query_type":"select"
       },
       {
           "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
           "query_template_name": "get_cdc_type",
           "query_output_key": "get_cdc_type",
           "query_type":"select"
      },
     {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "get_table_adv_opt_details",
         "query_output_key": "table_adv_opt_details",
         "query_type":"select"
     },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_ingestion_information",
        "query_output_key": "table_metadata_output",
        "query_type":"select"
      },
      {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "fetch_object_column_metadata",
          "query_output_key": "column_metadata_output",
          "query_type":"select"
        },
        {
          "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
          "query_template_name": "fetch_destination_transformation_query_details",
          "query_output_key": "destination_transformation_query_details_output",
          "query_type":"select"
        },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_data_movement_info",
        "query_output_key": "data_movement_details",
        "query_type":"select"
      },
       $if(first(templateData.query_input.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_source_info",
        "query_output_key": "get_source_info",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "update_rename_details",
        "query_output_key": "update_rename_details",
        "query_type":"insert"
      },
	  {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_rename_details",
        "query_output_key": "insert_rename_details",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_destination_table_rename_details",
        "query_output_key": "insert_destination_table_rename_details",
        "query_type":"insert"
      },
	  $else$
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_destination_table_metadata",
        "query_output_key": "insert_destination_table_metadata",
        "query_type":"insert"
      },
      $endif$
   $if(first(templateData.query_input.data.query_input.metadata.data.data.templateData.query_input.get_cdc_info).create_view)$
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_destination_view_metadata",
        "query_output_key": "insert_destination_view_metadata",
        "query_type":"insert"
      },
   $endif$
  $if(first(templateData.query_input.data.query_input.metadata.data.data.templateData.query_input.get_cdc_type).cdc_applied)$
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "update_cdc_advanced_options_obj_checkpoint",
        "query_output_key": "update_cdc_advanced_options_obj_checkpoint",
        "query_type":"update"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_cdc_advanced_options_obj_checkpoint",
        "query_output_key": "insert_cdc_advanced_options_obj_checkpoint",
        "query_type":"insert"
      },
   $endif$
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_ingested_table_details",
        "query_output_key": "get_ingested_table_details",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_invalidated_columns",
        "query_output_key": "get_invalidated_columns",
        "query_type":"select"
    },
    {
      "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
      "query_template_name": "update_destination_column_metadata",
      "query_output_key": "update_destination_column_metadata",
      "query_type":"update"
    },
    {
      "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
      "query_template_name": "fetch_new_objects_metadata",
      "query_output_key": "fetch_new_objects_metadata",
      "query_type":"select"
    },
    {
      "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
      "query_template_name": "insert_destination_column_metadata",
      "query_output_key": "insert_destination_column_metadata",
      "query_type":"insert"
    },
    {
      "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
      "query_template_name": "update_objects_metadata",
      "query_output_key": "update_objects_metadata",
      "query_type":"update"
    },
    {
       "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
       "query_template_name": "table_column_metadata_with_column_id",
       "query_output_key": "table_column_metadata_with_column_id",
       "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_destination_job_metadata",
        "query_output_key": "insert_destination_job_metadata",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_structured_jobtotable",
        "query_output_key": "insert_structured_jobtotable",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "get_metadata_for_data_lineage",
        "query_output_key": "get_metadata_for_data_lineage",
        "query_type":"select"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_data_entity_lineage",
        "query_output_key": "insert_data_entity_lineage",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_column_data_lineage",
        "query_output_key": "insert_column_data_lineage",
        "query_type":"insert"
      },
      {
        "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
        "query_template_name": "insert_data_movement_errors",
        "query_output_key": "insert_data_movement_errors",
        "query_type":"insert"
      },
      {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "advanced_options_mapping_details",
         "query_output_key": "advanced_options_mapping_details",
         "query_type":"select"
      },
      {
         "query_template_group": "templates/artifact_ingestion/ingestion_templates.stg",
         "query_template_name": "insert_data_movement_schema_drift_details",
         "query_output_key": "insert_data_movement_schema_drift_details",
         "query_type":"insert"
      }

    ],
  "output_keys":[]
}
>>

get_ingested_table_details(templateData)::=<<
with source_table_details as (
select  table_id,b.dataplace_id,table_name,
case when dataplace_info->\>'connection_type' = 'hive' or dataplace_info->\>'connection_type' = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
schema_name
from (
select
table_id,dataplace_id,schema_id,schema_name,table_name from
nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)   and valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value'||'_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
,
source_details as (
select *,$if(templateData.input_data.rename_table_condition)$ $replacing_special_characters("coalesce(new_table_name,'')")$ as dest_table_name_2 $else$
$replacing_special_characters("coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name")$ as dest_table_name_2
$endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table_name,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_'  else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details

,
destination_schema_details as (
select a.*,g.schema_name as destination_schema_name
from source_details a
inner join nabu.dataplace_relational_component_physical g on g.dataplace_id = a.destination_dataplace_id
and g.schema_id = a.destination_schema_directory_id
),
destination_dataplace_details as(
select a.*,f.dataplace_component_type_id
from destination_schema_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
inner join nabu.dataplace_sub_component_lookup e on f.dataplace_sub_component_id = e.dataplace_sub_component_id
),
destination_table_name as(
select b.table_name,b.table_id  from
destination_dataplace_details a
inner join nabu.dataplace_table_metadata_physical b on a.destination_dataplace_id = b.dataplace_id and a.destination_schema_directory_id = b.schema_id
and a.dest_table_name_2 = b.table_name and valid_to_ts = '9999-12-31'
)
select * from destination_table_name
>>

get_invalidated_columns(templateData)::=<<
with dest_cols as(
  select * from nabu.dataplace_column_metadata_physical dcmp where dcmp.table_id=$first(templateData.get_ingested_table_details).table_id$ and valid_to_ts='9999-12-31'
)
select * from
(
      select *
      from(
        $fetch_destination_transformation_query_details(templateData)$
      )a
)sc
right outer join dest_cols dc on
sc.field_name = dc.column_name
where sc.column_name is null
>>

update_destination_column_metadata(templateData)::=<<
$if(templateData.get_invalidated_columns)$
update nabu.dataplace_column_metadata_physical set valid_to_ts=now()
where table_id=$first(templateData.get_ingested_table_details).table_id$ and valid_to_ts='9999-12-31'
$endif$
>>

fetch_new_objects_metadata(templateData)::=<<
select dataplace_id,schema_id,dataplace_component_type_id,schema_name as schema_name,table_owner,table_name,table_id,column_id,column_name,column_comment,
data_type,column_format,data_length,data_precision,data_scale,is_nullable,ordinal_position,avg_col_length,valid_from_ts,
case when source='O' then current_timestamp else valid_to_ts end as valid_to_ts,source
from
    (
        select count(*) over (partition by c.dataplace_id,c.dataplace_component_type_id,c.schema_id,c.schema_name,c.table_owner,c.table_name,
        coalesce(c.column_name ,''),coalesce(c.data_type,''),ordinal_position) as cnt,c.* from
        (
            select * from (

                with dest_cols as(
                    select * from nabu.dataplace_column_metadata_physical dcmp where dcmp.table_id=$first(templateData.get_ingested_table_details).table_id$
                    and valid_to_ts='9999-12-31'
                )
                 $if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
                ,
                partitioned_col_details as(
                    select field_name,column_name,column_id,source_datatype_name,ingested_colorder,destination_datatype_name,is_partitioned_col,is_virtual,is_auditcol
                    from(
                        $get_partition_cols(templateData)$
                        )a
                ),
                non_partitioned_col_details as(
                select field_name,column_name,column_id,source_datatype_name,ingested_colorder,destination_datatype_name,is_partitioned_col,is_virtual,is_auditcol
                    from(
                        $get_non_partition_cols(templateData)$
                        )a
                )
                $endif$
                $if(first(templateData.get_cdc_type).cdctype2)$
                ,
                cdc_auditcol_details as (
                $if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
                select a.field_name,null as column_name,null as column_id,null as source_datatype_name,null as ingested_colorder, a.destination_datatype_name,
                false ::boolean as is_partitioned_col,false ::boolean as is_virtual,is_auditcol
                $else$
                select false ::boolean as is_virtual,null as column_name,field_name,destination_datatype_name
                $endif$
                from
                    (
                        $get_cdc_audit_cols(templateData)$
                    )a
                )
                $endif$
               select 'N'::char as source,
               $templateData.input_data.destination_dataplace_id$ as dataplace_id,
               $templateData.input_data.destination_schema_id$ as schema_id,
               $templateData.input_data.destination_dataplace_component_type_id$ as dataplace_component_type_id,
               '$templateData.input_data.destination_schema_name$' as schema_name,
               $first(templateData.get_ingested_table_details).table_id$ as table_id,
               '$first(templateData.get_ingested_table_details).table_name$' as table_name,
               dc.column_id,dc.column_comment,dc.column_format,dc.data_length,dc.data_precision,dc.data_scale,dc.is_nullable,
               dc.table_owner as table_owner,dc.avg_col_length,current_timestamp as valid_from_ts,'9999-12-31'::date valid_to_ts,
               $if(first(templateData.fetch_source_type).is_ordinalpos_startwith_zero)$ sc.order-1 $else$ sc.order $endif$ as ordinal_position,
               case when sc.is_virtual is true then sc.column_name else sc.field_name end as column_name,
               sc.destination_datatype_name as data_type
               from
                 (
                     select *,row_number() over () as order from
                     (
                          $if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
                             select * from non_partitioned_col_details
                             union all
                             select * from partitioned_col_details
                             $if(first(templateData.get_cdc_type).cdctype2)$
                                 union all
                                 select * from cdc_auditcol_details
                             $endif$
                          $else$
                            select is_virtual,column_name,field_name,destination_datatype_name
                            from
                            (
                                 $fetch_destination_transformation_query_details(templateData)$
                            )column_metadata
                            $if(first(templateData.get_cdc_type).cdctype2)$
                                union all
                                select * from cdc_auditcol_details
                            $endif$
                          $endif$
                     )a
                 )sc
               left outer join dest_cols dc on dc.column_name=sc.field_name
               order by ordinal_position
            )b

            union all

            select * from (
                with dest_cols as(
                    select * from nabu.dataplace_column_metadata_physical dcmp where dcmp.table_id=$first(templateData.get_ingested_table_details).table_id$
                    and valid_to_ts='9999-12-31'
                )
              select 'O'::char as source,
              $templateData.input_data.destination_dataplace_id$ as dataplace_id,
              $templateData.input_data.destination_schema_id$ as schema_id,
              $templateData.input_data.destination_dataplace_component_type_id$ as dataplace_component_type_id,
              '$templateData.input_data.destination_schema_name$' as schema_name,
              dc.table_id as table_id,dc.table_name as table_name,dc.column_id, dc.column_comment,dc.column_format,dc.data_length,dc.data_precision,dc.data_scale,
              dc.is_nullable,dc.table_owner as table_owner, dc.avg_col_length,dc.valid_from_ts,dc.valid_to_ts,dc.ordinal_position ,
              dc.column_name,dc.data_type
              from dest_cols dc
            )a

        ) c
    )d
where cnt=1;

>>

insert_destination_column_metadata(templateData)::=<<
insert into nabu.dataplace_column_metadata_physical( dataplace_id,schema_id, dataplace_component_type_id, schema_name, table_id, table_name,table_owner ,column_id ,column_name, column_comment ,data_type,data_length,data_precision,data_scale,is_nullable,ordinal_position,avg_col_length,column_format,valid_from_ts,valid_to_ts)

select b.dataplace_id,b.schema_id,b.dataplace_component_type_id,trim(b.schema_name) as schema_name, b.table_id,trim(b.table_name) as table_name,
trim(b.table_owner) as table_owner,case when column_id is null then nextval('nabu.dataplace_column_metadata_physical_column_id_seq') else column_id end as column_id,
b.column_name,b.column_comment,b.data_type,b.data_length,b.data_precision,b.data_scale,b.is_nullable,b.ordinal_position,b.avg_col_length,
b.column_format,b.valid_from_ts,b.valid_to_ts
from(
    select
    column_data->\>'source' as source,
    (column_data->\>'dataplace_id')::int4 as dataplace_id,
    (column_data->\>'dataplace_component_type_id')::int2 as dataplace_component_type_id,
    column_data->\>'schema_name' as schema_name,
    (column_data->\>'schema_id')::int8 as schema_id,
    (column_data->\>'table_id')::int8 as table_id,
    column_data->\>'table_owner' as table_owner,
    column_data->\>'table_name' as table_name,
    (column_data->\>'column_id')::int8 as column_id,
    column_data->\>'column_name' as column_name,
    column_data->\>'column_comment' as column_comment,
    column_data->\>'data_type' as data_type,
    column_data->\>'column_format' as column_format,
    (column_data->\>'data_length')::int8 as data_length,
    (column_data->\>'data_precision')::int4 as data_precision,
    (column_data->\>'data_scale')::int4 as data_scale,
    (column_data->\>'is_nullable')::boolean as is_nullable,
    (column_data->\>'ordinal_position')::int4 as ordinal_position,
    (column_data->\>'avg_col_length')::int8 as avg_col_length,
    (column_data->\>'valid_from_ts')::timestamp as valid_from_ts,
    (column_data->\>'valid_to_ts')::timestamp as valid_to_ts
    from(select json_array_elements(('[$templateData.fetch_new_objects_metadata:generate_column_json();separator=","$]')::json) as column_data)column_metadata
)b
inner join(select dataplace_id,schema_id,dataplace_component_type_id,schema_name,table_id,table_name,table_owner from nabu.dataplace_table_metadata_physical
where valid_to_ts='9999-12-31'::date) c on
b.dataplace_id=c.dataplace_id and b.dataplace_component_type_id=c.dataplace_component_type_id and b.schema_id=c.schema_id and b.schema_name=c.schema_name
and b.table_name=c.table_name
where b.source ='N'
>>

generate_column_json(col)::=<<
{
"source": "$col.source$",
"dataplace_id": $col.dataplace_id$,
"schema_id": $col.schema_id$,
"dataplace_component_type_id": $col.dataplace_component_type_id$,
"schema_name": "$col.schema_name$",
"table_id": $col.table_id$,
"table_owner": $if(col.table_owner)$"$col.table_owner$"$else$null$endif$,
"table_name": "$col.table_name$",
"column_id": $if(col.column_id)$$col.column_id$$else$null$endif$,
"column_name": $if(col.column_name)$"$col.column_name$"$else$null$endif$,
"column_comment": $if(col.column_comment)$"$col.column_comment$"$else$null$endif$,
"data_type": $if(col.data_type)$"$col.data_type$"$else$null$endif$,
"column_format": $if(col.column_format)$"$col.column_format$"$else$null$endif$,
"data_length": $if(col.data_length)$$col.data_length$$else$null$endif$,
"data_precision": $if(col.data_precision)$$col.data_precision$$else$null$endif$,
"data_scale": $if(col.data_scale)$$col.data_scale$$else$null$endif$,
"is_nullable": $if(col.is_nullable)$$col.is_nullable$$else$false$endif$,
"ordinal_position": $if(col.ordinal_position)$$col.ordinal_position$$else$null$endif$,
"avg_col_length": $if(col.avg_col_length)$$col.avg_col_length$$else$null$endif$,
"valid_from_ts": $if(col.valid_from_ts)$"$col.valid_from_ts$"$else$null$endif$,
"valid_to_ts": $if(col.valid_to_ts)$"$col.valid_to_ts$"$else$null$endif$
}
>>

update_objects_metadata(templateData)::=<<
update nabu.dataplace_column_metadata_physical a
set valid_to_ts=b.valid_to_ts
from (
      select
      column_data->\>'source' as source,
      (column_data->\>'dataplace_id')::int4 as dataplace_id,
      (column_data->\>'dataplace_component_type_id')::int2 as dataplace_component_type_id,
      column_data->\>'schema_name' as schema_name,
      (column_data->\>'schema_id')::int8 as schema_id,
      (column_data->\>'table_id')::int8 as table_id,
      column_data->\>'table_owner' as table_owner,
      column_data->\>'table_name' as table_name,
      (column_data->\>'column_id')::int8 as column_id,
      column_data->\>'column_name' as column_name,
      column_data->\>'column_comment' as column_comment,
      column_data->\>'data_type' as data_type,
      column_data->\>'column_format' as column_format,
      (column_data->\>'data_length')::int8 as data_length,
      (column_data->\>'data_precision')::int4 as data_precision,
      (column_data->\>'data_scale')::int4 as data_scale,
      column_data->\>'is_nullable' as is_nullable,
      (column_data->\>'ordinal_position')::int4 as ordinal_position,
      (column_data->\>'avg_col_length')::int8 as avg_col_length,
      (column_data->\>'valid_from_ts')::timestamp as valid_from_ts,
      (column_data->\>'valid_to_ts')::timestamp as valid_to_ts
      from(select json_array_elements(('[$templateData.fetch_new_objects_metadata:generate_column_json();separator=","$]')::json) as column_data)column_metadata
    )b
where  a.dataplace_id=b.dataplace_id
and a.dataplace_component_type_id=b.dataplace_component_type_id
and a.schema_id=b.schema_id
and a.schema_name=b.schema_name
and a.table_id=b.table_id
and a.column_name=b.column_name
and a.valid_from_ts=b.valid_from_ts
and b.source='O'
and a.valid_to_ts = '9999-12-31';
>>

table_column_metadata_with_column_id(templateData)::=<<
with dest_col_details as(
select cm.*,dtmp2.table_name as dest_table_name ,dtmp2.table_id as dest_table_id,drcp.schema_name as dest_schema_name,
case when (dscl.dataplace_sub_component_type='hive' or dscl.dataplace_sub_component_type='cloudera_hive')  then trim(((dp.dataplace_info->\>'jdbc_info')::json)->\>'database_name')
else trim(dp.dataplace_info->\>'database_name') end as destination_db
from nabu.dataplace_column_metadata cm inner join nabu.dataplace_table_metadata_physical dtmp2 on cm.table_id=dtmp2.table_id
inner join nabu.dataplace_relational_component_physical drcp on dtmp2.schema_id =drcp.schema_id
inner join nabu.dataplace_physical dp on drcp.dataplace_id =dp.dataplace_id
inner join nabu.dataplace_sub_component_lookup dscl on dp.dataplace_sub_component_id =dscl.dataplace_sub_component_id
where cm.table_id =$first(templateData.get_ingested_table_details).table_id$
and cm.valid_to_ts='9999-12-31' and dtmp2.valid_to_ts ='9999-12-31' and drcp.valid_to_ts ='9999-12-31' and dp.valid_to_ts ='9999-12-31'
order by ordinal_position
),
partitioned_col_details as(
    select field_name,column_name,column_id,source_datatype_name,ingested_colorder,destination_datatype_name,is_partitioned_col,is_virtual,$templateData.input_data.source_table_id$ as table_id,is_auditcol
    from
    (
    $get_partition_cols(templateData)$
    )a
),
non_partitioned_col_details as(
 select field_name,column_name,column_id,source_datatype_name,ingested_colorder,destination_datatype_name,is_partitioned_col,is_virtual,$templateData.input_data.source_table_id$ as table_id,is_auditcol
    from
    (
    $get_non_partition_cols(templateData)$
    )a
)
$if(first(templateData.get_cdc_type).cdctype2)$
,
cdc_auditcol_details as (
$if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
select a.field_name,null as column_name,null as column_id,null as source_datatype_name,null as ingested_colorder, a.destination_datatype_name,
false ::boolean as is_partitioned_col,false ::boolean as is_virtual,$templateData.input_data.source_table_id$ as table_id,is_auditcol
$else$
select false ::boolean as is_virtual,null as source_datatype_name,null ::int as column_id,null as column_name,field_name,destination_datatype_name,is_auditcol,null ::int as table_id
$endif$
from
    (
        $get_cdc_audit_cols(templateData)$
    )a
)
$endif$
,
details as(
select
dtmp2.table_id as src_table_id,dtmp2.table_name as src_table_name ,drcp.schema_name as src_schema_name,
case when (dscl.dataplace_sub_component_type='hive' or dscl.dataplace_sub_component_type='cloudera_hive')  then trim(((dp.dataplace_info->\>'jdbc_info')::json)->\>'database_name')
else trim(dp.dataplace_info->\>'database_name') end as source_db from nabu.dataplace_table_metadata_physical dtmp2
left outer join nabu.dataplace_relational_component_physical drcp on dtmp2.schema_id =drcp.schema_id
left outer join nabu.dataplace_physical dp on drcp.dataplace_id =dp.dataplace_id
left outer join nabu.dataplace_sub_component_lookup dscl on dp.dataplace_sub_component_id =dscl.dataplace_sub_component_id
where dtmp2.table_id = $templateData.input_data.source_table_id$ and dtmp2.valid_to_ts ='9999-12-31' and drcp.valid_to_ts ='9999-12-31'and dp.valid_to_ts ='9999-12-31'
),
src_col_details as (
select d.*,
case when sc.is_virtual = true then 'virtual' when sc.is_auditcol = true then 'audit' else 'source' end as column_type,sc.source_datatype_name as src_datatype,
row_number() over () as ingested_colorder,sc.is_virtual,
case when sc.is_virtual is true then 0
when sc.is_auditcol is true then 0
else sc.column_id::int end as src_col_id,
case when sc.is_virtual is true then null else sc.column_name end as column_name,
sc.destination_datatype_name as dest_datatype, sc.field_name as field_name,sc.is_auditcol
from
    (
         $if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
            select * from non_partitioned_col_details
            union all
            select * from partitioned_col_details
            $if(first(templateData.get_cdc_type).cdctype2)$
                union all
                select * from cdc_auditcol_details
            $endif$
         $else$
           select is_virtual,source_datatype_name,column_id,column_name,field_name,destination_datatype_name,false :: boolean as is_auditcol,table_id
           from
           (
                $fetch_destination_transformation_query_details(templateData)$
           )column_metadata
           $if(first(templateData.get_cdc_type).cdctype2)$
               union all
               select * from cdc_auditcol_details
           $endif$
         $endif$
    )sc
left outer join details d on sc.table_id=d.src_table_id
)
select sc.*,dc.dest_table_id,dc.dest_table_name,dc.dest_schema_name,dc.column_name as dest_col_name,dc.column_id as dest_col_id,dc.destination_db,current_timestamp as req_ts
from src_col_details sc left outer join dest_col_details dc on sc.field_name =dc.column_name
order by sc.ingested_colorder
>>

insert_column_data_lineage(templateData)::=<<
insert into nabu.column_data_lineage(data_movement_id,source_column_id,destination_column_id,lineage_uuid, lineage_document, cru_by, cru_ts)
values
$templateData.table_column_metadata_with_column_id:forEachColumnInsert(templateData);separator=",\n"$;
>>

forEachColumnInsert(columnData,templatedata)::=<<
($templateData.input_data.data_movement_id$,$columnData.src_col_id$,$columnData.dest_col_id$,uuid_in((md5((random())::text))::cstring),'$generate_column_data_lineage(columnData,templateData)$','Modak',current_timestamp)
>>

generate_column_data_lineage(map,queryInput)::=<<
{"data_lineage":{ "meta_data": { "created_by": "lineage_bot","requested_by": "$lineage_requested_by(map)$","requested_timestamp": "$map.req_ts$","executed_timestamp": "$map.req_ts$","tags": {}},"source_to_target_info":{"target_table_id": $map.dest_table_id$,"target_column_id" : $map.dest_col_id$,"target_database": "$map.destination_db$","target_schema": "$map.dest_schema_name$","target_table": "$map.dest_table_name$","target_column_name": "$map.dest_col_name$","target_column_type": "$map.dest_datatype$","column_type": "$map.column_type$","transformation": {"transformation_ops": "","src_columns": [{"src_table_id": $if(map.src_table_id)$$map.src_table_id$$else$null$endif$,"src_column_id" :$if(map.src_col_id)$$map.src_col_id$$else$null$endif$,"src_database": $if(map.source_db)$ "$map.source_db$"$else$null$endif$,"src_schema": $if(map.src_schema_name)$"$map.src_schema_name$"$else$null$endif$,"src_table": $if(map.src_table_name)$"$map.src_table_name$"$else$null$endif$,"src_column_name": $if(map.column_name)$"$map.column_name$"$else$null$endif$,"src_column_type": $if(map.src_datatype)$"$map.src_datatype$"$else$null$endif$}],"details": null}}}}
>>

generate_lineage_data(templateData)::=<<
select
        (column_data->\>'data_movement_id')::int8 as data_movement_id,
        (column_data->\>'src_col_id')::int8 as src_col_id,
        (column_data->\>'dest_col_id')::int8 as dest_col_id,
        (column_data->\>'dest_table_id')::int8 as dest_table_id,
        (column_data->\>'src_table_id')::int8 as src_table_id,
        column_data->\>'destination_db' as destination_db,
        column_data->\>'source_db' as source_db,
        column_data->\>'dest_schema_name' as dest_schema_name,
        column_data->\>'src_schema_name' as src_schema_name,
        column_data->\>'src_table_name' as src_table_name,
        column_data->\>'dest_table_name' as dest_table_name,
        column_data->\>'dest_col_name' as dest_col_name,
        column_data->\>'column_name' as column_name,
        column_data->\>'dest_datatype' as dest_datatype,
        column_data->\>'src_datatype' as src_datatype,
        (column_data->\>'src_table_id')::int8 as src_table_id,
        $column_data->\>'source_stg_function' as source_stg_function,
        column_data->\>'intermediate_stg_function' as intermediate_stg_function
    from(select json_array_elements(('[$templateData.table_column_metadata_with_column_id:generate_lineage_json();separator=","$]')::json) as column_data)column_metadata

>>

generate_lineage_json(templateData)::=<<
{
"data_movement_id":$templateData.data_movement_id$,
"src_col_id":$templateData.src_col_id$,
"dest_col_id":$templateData.dest_col_id$,
"dest_table_id":$templateData.dest_table_id$,
"src_table_id":$templateData.src_table_id$,
"destination_db":"$templateData.destination_db$",
"source_db":"$templateData.source_db$",
"dest_schema_name":"$templateData.dest_schema_name$",
"src_schema_name":"$templateData.src_schema_name$",
"src_table_name":"$templateData.src_table_name$",
"dest_table_name":"$templateData.dest_table_name$",
"dest_col_name":"$templateData.dest_col_name$",
"column_name":"$templateData.column_name$",
"dest_datatype":"$templateData.dest_datatype$",
"src_datatype":$templateData.src_datatype$,
"src_table_id":$templateData.src_table_id$,
"source_stg_function":"$templateData.source_stg_function$",
"intermediate_stg_function":"$templateData.intermediate_stg_function$"
}
>>

forEachColumnDetails(templateData)::=<<
select templateData.data_type,templateData.data_length,templateData.ordinal_position,templateData.is_nullable,templateData.table_name,templateData.schema_name,templateData.is_mysql,templateData.is_source_hive,templateData.source_type,templateData.source_datatype_name,templateData.source_cast_type,templateData.intermediate_datatype_name,templateData.source_stg_function,templateData.data_scale,templateData.data_precision,templateData.destination_datatype_name,templateData.intermediate_stg_function,templateData.inconsistent_datatype,templateData.field_name,templateData.istimezone,templateData.user_text
>>


get_source_info(templateData)::=<<
with source_table_details as (
select  table_id,b.dataplace_id,e.dataplace_sub_component_id as source_dataplace_sub_component_id ,schema_name,table_name,
case when dataplace_sub_component_type = 'hive' or dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name
from (
select
table_id,a.dataplace_id,schema_id,schema_name,table_name,dataplace_sub_component_type
from
nabu.dataplace_table_metadata_physical a
inner join nabu.dataplace_physical  b on a.dataplace_id = b.dataplace_id
inner join nabu.dataplace_sub_component_lookup  c on b.dataplace_sub_component_id  = c.dataplace_sub_component_id
where table_id in ($templateData.input_data.where_condition$) and a.valid_to_ts = '9999-12-31' and b.valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
select *,
$replacing_special_characters(" coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') ||  coalesce(table_name,'') ")$ as dest_table_name_for_update
from (
select a.*,c.*,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_'  else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a

>>

get_source_info_file(templateData)::=<<
with source_table_details as (
select file_id,b.dataplace_id,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value'||'_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
select
$replacing_special_characters_file("coalesce(ingestion_table_format,'') || file_name")$ as dest_file_name
from (
select a.*,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
>>

insert_data_movement_errors(templateData)::=<<
INSERT INTO nabu.data_movement_errors
(process_id, bot_uuid, process_context, error_json, created_ts, bot_type)
select process_id,'$templateData.input_data.bot_uuid$' as bot_uuid,'spark_ingestion' as process_context,error_msg as erro_json,current_timestamp as created_ts,
'SparkScriptBot' as bot_type
 from nabu.checkpoint_status
where job_type in ('nabu-ingestion-bot','nabu-sparkbot-ingestion') and status = 'ERROR' and valid_to_ts = '9999-12-31'
and data_movement_id = $templateData.input_data.data_movement_id$
>>

advanced_options_mapping_details(templateData)::=<<
select
case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') is null then 'drop_create_table'
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') end as schema_drift_option,
case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type_id')::int is null then 4
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type_id')::int end as schema_drift_option_id,data_movement_id
from nabu.data_movement_physical
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts = '9999-12-31'
>>

insert_data_movement_schema_drift_details(templateData)::=<<
INSERT INTO nabu.data_movement_schema_drift_details
(data_movement_id, dataplace_id, schema_directory_id, object_id, advanced_options_sub_type_id, advanced_options_sub_type, cru_by, cru_ts, batch_id, process_id)
select $templateData.input_data.data_movement_id$ as data_movement_id, a.dataplace_id, a.schema_id as schema_directory_id, a.table_id as object_id,
$first(templateData.advanced_options_mapping_details).schema_drift_option_id$ as advanced_options_sub_type_id,
'$first(templateData.advanced_options_mapping_details).schema_drift_option$' as advanced_options_sub_type, 'Modak', now(),
$templateData.input_data.batch_id$, $templateData.input_data.process_id$
from nabu.dataplace_table_schema_drift_details a
where a.valid_to_ts = '9999-12-31'
and a.table_id in ($templateData.input_data.where_condition$)
and $templateData.input_data.schema_drift_flag$ = true

>>

insert_data_movement_schema_drift_details_file(templateData)::=<<
INSERT INTO nabu.data_movement_schema_drift_details
(data_movement_id, dataplace_id, schema_directory_id, object_id, advanced_options_sub_type_id, advanced_options_sub_type, cru_by, cru_ts, batch_id, process_id)

select $templateData.input_data.data_movement_id$ as data_movement_id, a.dataplace_id, a.directory_id as schema_directory_id, a.file_id as object_id,
$first(templateData.advanced_options_mapping_details).schema_drift_option_id$ as advanced_options_sub_type_id,
'$first(templateData.advanced_options_mapping_details).schema_drift_option$' as advanced_options_sub_type, 'Modak', now(),
$templateData.input_data.batch_id$, $templateData.input_data.process_id$
from nabu.semi_structured_file_schema_drift a
where a.file_id in ($templateData.input_data.where_condition$)
and $templateData.input_data.schema_drift_flag$ = true

>>

get_data_movement_info(templateData)::=<<
select a.data_movement_id,b.destination_schema_directory_id,b.destination_dataplace_id,b.source_dataplace_id,b.source_schema_directory_id
from nabu.data_movement_physical a
inner join nabu.data_movement_details_physical b on a.data_movement_id = b.data_movement_id
and a.data_movement_id = $templateData.input_data.data_movement_id$
where a.valid_to_ts = '9999-12-31' and b.valid_to_ts = '9999-12-31'
>>

insert_rename_details(templateData)::=<<
INSERT INTO nabu.advanced_options_obj_checkpoint
(data_movement_id, source_dataplace_id, object_id, advanced_option_obj_type_id, process_id, additional_info, valid_from_ts, valid_to_ts)
select  $templateData.input_data.data_movement_id$,$first(templateData.data_movement_details).source_dataplace_id$,table_id,
(SELECT advanced_options_obj_type_id FROM nabu.advanced_options_obj_lookup WHERE advanced_options_obj_type = 'Renaming Table'),
 $templateData.input_data.process_id$,'{"renamed_table_name" : "$templateData.input_data.new_table_name$"}'::json,now(),'9999-12-31'
from (
select
table_id,dataplace_id,schema_id,schema_name,table_name from
nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)   and valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
>>

update_rename_details(templateData)::=<<
update nabu.advanced_options_obj_checkpoint
set valid_to_ts = now()
where
data_movement_id = $templateData.input_data.data_movement_id$ and
source_dataplace_id = (select dataplace_id
                       from nabu.dataplace_table_metadata_physical
                       where table_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31') and
object_id in ($templateData.input_data.where_condition$) and
advanced_option_obj_type_id = (SELECT advanced_options_obj_type_id
                                FROM nabu.advanced_options_obj_lookup
                                WHERE advanced_options_obj_type = 'Renaming Table') and
valid_to_ts ='9999-12-31'
>>

update_rename_details_file(templateData)::=<<
update nabu.advanced_options_obj_checkpoint
set valid_to_ts = now()
where
data_movement_id = $templateData.input_data.data_movement_id$ and
source_dataplace_id = (select dataplace_id
                       from nabu.dataplace_file_metadata
                       where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31') and
object_id in ($templateData.input_data.where_condition$) and
advanced_option_obj_type_id = (SELECT advanced_options_obj_type_id
                                FROM nabu.advanced_options_obj_lookup
                                WHERE advanced_options_obj_type = 'Renaming Table') and
valid_to_ts ='9999-12-31'
>>

insert_rename_details_file(templateData)::=<<
INSERT INTO nabu.advanced_options_obj_checkpoint
(data_movement_id, source_dataplace_id, object_id, advanced_option_obj_type_id, process_id, additional_info, valid_from_ts, valid_to_ts)
select $templateData.input_data.data_movement_id$, $first(templateData.data_movement_details).source_dataplace_id$,file_id,
(SELECT advanced_options_obj_type_id
 FROM nabu.advanced_options_obj_lookup
 WHERE advanced_options_obj_type = 'Renaming Table'),
 $templateData.input_data.process_id$, '{}',now(),'9999-12-31'
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
>>

insert_destination_table_rename_details(templateData)::=<<
INSERT INTO nabu.dataplace_table_metadata_physical
(dataplace_id, dataplace_component_type_id, schema_id, schema_name, table_name, create_table_ts, table_type, table_format,table_layer,valid_from_ts, valid_to_ts)
select
a.dataplace_id ,a.dataplace_component_type_id ,b.destination_schema_directory_id ,c.schema_name ,'$templateData.input_data.new_table_name$',current_timestamp as create_table_ts, 'T' as table_type,'ingestion' as table_format,
'source' as table_layer,current_timestamp as valid_from_ts,'9999-12-31 00:00:00' :: timestamp as valid_to_ts
from
nabu.data_movement_details_physical b
inner join nabu.dataplace_physical a
on a.dataplace_id = b.destination_dataplace_id
inner join nabu.dataplace_relational_component c
on a. dataplace_id = c.dataplace_id and b.destination_schema_directory_id = c.schema_id
where b.data_movement_id =$first(templateData.get_source_info).data_movement_id$
and b.destination_dataplace_id = $first(templateData.get_source_info).destination_dataplace_id$
and b.destination_schema_directory_id = $first(templateData.get_source_info).destination_schema_directory_id$
and b.valid_to_ts = '9999-12-31' and a.valid_to_ts = '9999-12-31' and c.valid_to_ts = '9999-12-31'
>>

insert_destination_table_metadata(templateData)::=<<
INSERT INTO nabu.dataplace_table_metadata_physical
(dataplace_id, dataplace_component_type_id, schema_id, schema_name, table_name, create_table_ts, table_type, table_format,table_layer,valid_from_ts, valid_to_ts)
with source_table_details as (
select  table_id,b.dataplace_id,table_name,
case when dataplace_sub_component_type = 'hive' or dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
schema_name
from (
select
table_id,a.dataplace_id,schema_id,schema_name,table_name,dataplace_sub_component_type
from
nabu.dataplace_table_metadata_physical a
inner join nabu.dataplace_physical  b on a.dataplace_id = b.dataplace_id
inner join nabu.dataplace_sub_component_lookup  c on b.dataplace_sub_component_id  = c.dataplace_sub_component_id
where table_id in ($templateData.input_data.where_condition$)   and a.valid_to_ts = '9999-12-31' and b.valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value'||'_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
,
source_details as (
select *,$if(templateData.input_data.rename_table_condition)$ $replacing_special_characters("coalesce(new_table_name,'')")$ as dest_table_name_2 $else$
$replacing_special_characters("coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name")$ as dest_table_name_2
$endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table_name,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_'  else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details

,
destination_schema_details as (
select a.*,g.schema_name as destination_schema_name
from source_details a
inner join nabu.dataplace_relational_component_physical g on g.dataplace_id = a.destination_dataplace_id
and g.schema_id = a.destination_schema_directory_id
),
destination_dataplace_details as(
select a.*,f.dataplace_component_type_id
from destination_schema_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
inner join nabu.dataplace_sub_component_lookup e on f.dataplace_sub_component_id = e.dataplace_sub_component_id
),
destination_table_exists as(
select * from (
select a.*, case when a.dest_table_name_2 = b.table_name then true else false end as table_exists
from
destination_dataplace_details a
left outer join nabu.dataplace_table_metadata_physical b on a.destination_dataplace_id = b.dataplace_id and a.destination_schema_directory_id = b.schema_id
and a.dest_table_name_2 = b.table_name and valid_to_ts = '9999-12-31'
)a
where table_exists is false
),
destination_entry_table_metadata as  (
select * from (
select a.*,  case when table_id = d.source_table_id and dest_table_id is not null and table_exists is true then true else false end as flag
from destination_table_exists a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.table_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where
flag is false and table_exists is false
)
select  destination_dataplace_id as dataplace_id, dataplace_component_type_id,destination_schema_directory_id as   schema_id,
destination_schema_name, dest_table_name_2 as table_name, current_timestamp  as  create_table_ts, 'T' as table_type,'ingestion' as table_format,
'source' as table_layer,current_timestamp as valid_from_ts,'9999-12-31 00:00:00' :: timestamp as valid_to_ts  from destination_entry_table_metadata
>>

insert_destination_view_metadata(templateData)::=<<
INSERT INTO nabu.dataplace_table_metadata_physical
(dataplace_id, dataplace_component_type_id, schema_id, schema_name, table_name, create_table_ts, table_type, table_format,table_layer,valid_from_ts, valid_to_ts)
with data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value'||'_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
,
destination_schema_details as (
select a.*,g.schema_name as destination_schema_name
from data_movement_details a
inner join nabu.dataplace_relational_component_physical g on g.dataplace_id = a.destination_dataplace_id
and g.schema_id = a.destination_schema_directory_id
),
destination_dataplace_details as(
select a.*,'$templateData.input_data.view_name$' as dest_table_name_2 ,f.dataplace_component_type_id
from destination_schema_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
inner join nabu.dataplace_sub_component_lookup e on f.dataplace_sub_component_id = e.dataplace_sub_component_id
),
destination_table_exists as(
select * from (
select a.*, case when a.dest_table_name_2 = b.table_name then true else false end as table_exists
from
destination_dataplace_details a
left outer join nabu.dataplace_table_metadata_physical b on a.destination_dataplace_id = b.dataplace_id and a.destination_schema_directory_id = b.schema_id
and a.dest_table_name_2 = b.table_name and valid_to_ts = '9999-12-31'
)a
where table_exists is false
)
select  destination_dataplace_id as dataplace_id, dataplace_component_type_id,destination_schema_directory_id as   schema_id,
destination_schema_name, dest_table_name_2 as table_name, current_timestamp  as  create_table_ts, 'V' as table_type,'ingestion' as table_format,
'source' as table_layer,current_timestamp as valid_from_ts,'9999-12-31 00:00:00' :: timestamp as valid_to_ts  from destination_table_exists
>>


insert_destination_file_metadata(templateData)::=<<
INSERT INTO nabu.dataplace_table_metadata_physical
(dataplace_id, dataplace_component_type_id, schema_id, schema_name, table_name, create_table_ts, table_type, table_format,table_layer,valid_from_ts, valid_to_ts)
with source_table_details as (
select file_id,b.dataplace_id,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value'||'_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
,
source_details as (
select *,$if(templateData.input_data.rename_table_condition)$ $replacing_special_characters("coalesce(new_table_name,'')")$ as dest_table_name_2 $else$
$replacing_special_characters_file("coalesce(ingestion_table_format,'') || file_name")$ as dest_file_name_2 $endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table_name,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
,
destination_schema_details as (
select a.*,g.schema_name as destination_schema_name
from source_details a
inner join nabu.dataplace_relational_component_physical g on g.dataplace_id = a.destination_dataplace_id
and g.schema_id = a.destination_schema_directory_id
),
destination_dataplace_details as(
select a.*,f.dataplace_component_type_id
from destination_schema_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
inner join nabu.dataplace_sub_component_lookup e on f.dataplace_sub_component_id = e.dataplace_sub_component_id
),
destination_table_exists as(
select * from (
select a.*, case when a.dest_file_name_2 = b.table_name then true else false end as table_exists
from
destination_dataplace_details a
left outer join nabu.dataplace_table_metadata_physical b on a.destination_dataplace_id = b.dataplace_id and a.destination_schema_directory_id = b.schema_id
and a.dest_file_name_2 = b.table_name and valid_to_ts = '9999-12-31'
)a
where table_exists is false
),
destination_entry_table_metadata as  (
select * from (
select a.*,  case when file_id = d.source_table_id and dest_table_id is not null and table_exists is true then true else false end as flag
from destination_table_exists a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.file_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where
flag is false and table_exists is false
)
select  destination_dataplace_id as dataplace_id, dataplace_component_type_id,destination_schema_directory_id as schema_id,
destination_schema_name, dest_file_name_2 as table_name, current_timestamp  as  create_table_ts, 'T' as table_type,'ingestion' as table_format,
'source' as table_layer,current_timestamp as valid_from_ts,'9999-12-31 00:00:00' :: timestamp as valid_to_ts  from destination_entry_table_metadata
>>

insert_destination_job_metadata_file(templateData)::=<<
INSERT INTO nabu.job_metadata
(job_name, job_description, type_of_job, job_added_dt, job_owner, data_movement_id, cru_by, cru_ts)
with source_table_details as (
select file_id,b.dataplace_id,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
)
--select * from data_movement_details
,
source_details as (
select *,
$if(templateData.input_data.rename_table_condition)$
$replacing_special_characters("coalesce(new_table_name,'')")$ || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2 $else$
$replacing_special_characters_file("coalesce(file_name,'') ")$ || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
$endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table_name,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
-- Note : below query for matching dataplace_physical is only for validation
,
validating_dataplace as(
select a.*
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
)
--select * from validating_dataplace;
,
entry_job_metadata as  (
select * from (
select a.*,  case when file_id = d.source_table_id and dest_table_id is not null  then true else false end as flag
from validating_dataplace a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.file_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
select job_name,job_description,type_of_job,job_added_dt,job_owner,data_movement_id,cru_by,cru_ts from (
select job_name_2 as job_name,'Fetch Data for Ingestion' as job_description,'ingestion' as type_of_job,
current_timestamp as job_added_dt,'Modak' as job_owner,data_movement_id,'Modak' as cru_by,current_timestamp as cru_ts  from entry_job_metadata)x
>>

insert_destination_job_metadata(templateData)::=<<
INSERT INTO nabu.job_metadata
(job_name, job_description, type_of_job, job_added_dt, job_owner, data_movement_id, cru_by, cru_ts)
with source_table_details as (
select  table_id,b.dataplace_id,table_name,
case when dataplace_sub_component_type = 'hive' or dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
schema_name
from (
select
table_id,a.dataplace_id,schema_id,schema_name,table_name,dataplace_sub_component_type
from
nabu.dataplace_table_metadata_physical a
inner join nabu.dataplace_physical  b on a.dataplace_id = b.dataplace_id
inner join nabu.dataplace_sub_component_lookup  c on b.dataplace_sub_component_id  = c.dataplace_sub_component_id
where table_id in ($templateData.input_data.where_condition$)   and a.valid_to_ts = '9999-12-31' and b.valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,
$if(templateData.input_data.rename_table_condition)$ $replacing_special_characters("coalesce(newtable,'')")$ || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2 $else$
$replacing_special_characters("coalesce(db_name,'') || coalesce(source_schema_name,'') ||  coalesce(table_name,'') ")$ || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
$endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as newtable,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name||'_' else '' end as db_name,
case when valid_schema_name is true then schema_name||'_' else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
),
-- Note : below query for matching dataplace_physical is only for validation
validating_dataplace as(
select a.*
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
)
,
entry_job_metadata as  (
select * from (
select a.*,  case when table_id = d.source_table_id and dest_table_id is not null  then true else false end as flag
from validating_dataplace a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.table_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
--select * from entry_job_metadata

select  job_name,job_description,type_of_job,job_added_dt,job_owner,data_movement_id,cru_by,cru_ts from (
select job_name_2 as job_name,'Fetch Data for Ingestion' as job_description,'ingestion' as type_of_job,
current_timestamp as job_added_dt,'Modak' as job_owner,data_movement_id,'Modak' as cru_by,current_timestamp as cru_ts  from entry_job_metadata)x
>>

update_destination_table_metadata(templateData)::=<<
update nabu.dataplace_table_metadata_physical
set
table_name = '$templateData.input_data.new_table_name$'
where
dataplace_id in (
select destination_dataplace_id
from nabu.data_movement_details_physical
where
data_movement_id = ($templateData.input_data.data_movement_id$)
and valid_to_ts ='9999-12-31')
and schema_id in (
select destination_schema_directory_id
from nabu.data_movement_details_physical
where
data_movement_id = ($templateData.input_data.data_movement_id$)
and valid_to_ts ='9999-12-31')
and
table_name = '$first(templateData.get_source_info).dest_table_name_for_update$'


>>

update_destination_file_metadata(templateData)::=<<
update nabu.dataplace_table_metadata_physical
set
table_name = '$templateData.input_data.new_table_name$'
where
dataplace_id in (
select destination_dataplace_id
from nabu.data_movement_details_physical
where
data_movement_id = ($templateData.input_data.data_movement_id$)
and valid_to_ts ='9999-12-31')
and schema_id in (
select destination_schema_directory_id
from nabu.data_movement_details_physical
where
data_movement_id = ($templateData.input_data.data_movement_id$)
and valid_to_ts ='9999-12-31')
and
table_name = '$first(templateData.get_source_info_file).dest_file_name$'

>>

insert_structured_jobtotable(templateData)::=<<
INSERT INTO nabu.structured_jobtotable
(dataplace_id, data_movement_id, job_id, object_id, object_type, cru_by, cru_ts)
with source_table_details as (
select  table_id,b.dataplace_id,table_name,
case when dataplace_sub_component_type = 'hive' or dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
schema_name
from (
select
table_id,a.dataplace_id,schema_id,schema_name,table_name,dataplace_sub_component_type
from
nabu.dataplace_table_metadata_physical a
inner join nabu.dataplace_physical  b on a.dataplace_id = b.dataplace_id
inner join nabu.dataplace_sub_component_lookup  c on b.dataplace_sub_component_id  = c.dataplace_sub_component_id
where table_id in ($templateData.input_data.where_condition$)   and a.valid_to_ts = '9999-12-31' and b.valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'

),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,$if(templateData.input_data.rename_table_condition)$
                  job_name
                  $else$
                  case when ingestion_table_format is not null then lower(ingestion_table_format) || job_name else job_name end
                  $endif$
                  as dest_table_name,
job_name || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
from (
select *,$if(templateData.input_data.rename_table_condition)$
$replacing_special_characters("coalesce(new_table,'')")$ as job_name
$else$
$replacing_special_characters("coalesce(db_name,'') || coalesce(source_schema_name,'') ||  coalesce(table_name,'')")$ as job_name
$endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name||'_' else '' end as db_name,
case when valid_schema_name is true then schema_name||'_' else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)b
)
--select * from source_details
,
job_details as (
select a.*,b.job_id
from source_details a
left outer join (select *  from nabu.job_metadata where data_movement_id = $templateData.input_data.data_movement_id$) b
on a.job_name_2 = b.job_name
),
destination_table_id_details as (
select a.*, b.table_id as destination_table_id
from job_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name = b.table_name and  b.valid_to_ts = '9999-12-31'

)
--select * from destination_table_id
,
no_entry_in_table_metadata as  (
select * from (
select a.*,  case when table_id = d.source_table_id and dest_table_id is not null then true else false end as flag
from destination_table_id_details a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.table_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
select dataplace_id,data_movement_id, job_id,table_id as object_id,'S' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata
union
select destination_dataplace_id, data_movement_id, job_id,destination_table_id,'F' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata
>>

insert_structured_jobtotable_file(templateData)::=<<
INSERT INTO nabu.structured_jobtotable
(dataplace_id, data_movement_id, job_id, object_id, object_type, cru_by, cru_ts)
with source_table_details as (
select file_id,b.dataplace_id,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'

),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,case when ingestion_table_format is not null then lower(ingestion_table_format) || job_name else job_name end as dest_table_name,
job_name || '_' || destination_schema_directory_id || '_' || data_movement_id as job_name_2
from (
select *,
$if(templateData.input_data.rename_table_condition)$
$replacing_special_characters("coalesce(new_table,'')")$ as job_name
$else$
$replacing_special_characters_file("coalesce(file_name,'')")$ as job_name
$endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)b
)
--select * from source_details
,
job_details as (
select a.*,b.job_id
from source_details a
left outer join (select *  from nabu.job_metadata where data_movement_id = $templateData.input_data.data_movement_id$) b
on a.job_name_2 = b.job_name
),
destination_table_id_details as (
select a.*, b.table_id as destination_table_id
from job_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name = b.table_name and  b.valid_to_ts = '9999-12-31'

),
no_entry_in_table_metadata as  (
select * from (
select a.*,  case when file_id = d.source_table_id and dest_table_id is not null then true else false end as flag
from destination_table_id_details a
left outer join (  select
job_id,
data_movement_id,
max( case when object_type = 'S' then object_id end )as source_table_id,
max( case when object_type = 'S' then dataplace_id end )as source_dataplace_id,
max( case when object_type = 'F' then object_id end )as dest_table_id,
max( case when object_type = 'F' then dataplace_id end )as dest_dataplace_id
 from nabu.structured_jobtotable
where data_movement_id = $templateData.input_data.data_movement_id$
group by job_id,data_movement_id )  d on a.file_id = d.source_table_id and a.data_movement_id = d.data_movement_id
and a.destination_dataplace_id = d.dest_dataplace_id
) x
where flag is false
)
select dataplace_id,data_movement_id, job_id,a.file_id as object_id,'S' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata a
union
select destination_dataplace_id, data_movement_id, job_id,destination_table_id,'F' as object_type,'Modak' as cru_by,current_timestamp as cru_ts
from no_entry_in_table_metadata
>>

get_metadata_for_data_lineage_file(templateData)::=<<
with source_table_details as (
select file_id,b.dataplace_id,e.dataplace_sub_component_id as source_dataplace_sub_component_id ,file_name
from (
select file_id,dataplace_id,directory_id,file_name from
nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$) and valid_to_ts = '9999-12-31'
and file_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,$if(templateData.input_data.rename_table_condition)$ $replacing_special_characters("coalesce(new_table,'')")$ as dest_table_name_2 $else$
$replacing_special_characters_file(" coalesce(ingestion_table_format,'')  ||  coalesce(file_name,'') ")$ as dest_table_name_2 $endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
,
destination_dataplace_details as(
select a.*, f.dataplace_sub_component_id as destination_dataplace_sub_component_id
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
),
destination_table_id as (
select a.*, b.table_id as destination_table_id
from destination_dataplace_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name_2 = b.table_name and  b.valid_to_ts = '9999-12-31'
),
get_source_destination_type as(
select a.* , f.dataplace_sub_component_type as source_type,g.dataplace_sub_component_type as dest_type
from destination_table_id a
left outer join nabu.dataplace_sub_component_lookup f on a.source_dataplace_sub_component_id  = f.dataplace_sub_component_id
left outer join nabu.dataplace_sub_component_lookup g on a.destination_dataplace_sub_component_id = g.dataplace_sub_component_id
)
select file_id,data_movement_id as data_movement_id, destination_table_id ,  source_type,dest_type,current_timestamp as req_ts
from get_source_destination_type
>>

get_metadata_for_data_lineage(templateData)::=<<
with source_table_details as (
select  table_id,b.dataplace_id,table_name,e.dataplace_sub_component_id as source_dataplace_sub_component_id,
case when dataplace_sub_component_type = 'hive' or dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
schema_name
from (
select
table_id,a.dataplace_id,schema_id,schema_name,table_name,dataplace_sub_component_type
from
nabu.dataplace_table_metadata_physical a
inner join nabu.dataplace_physical  b on a.dataplace_id = b.dataplace_id
inner join nabu.dataplace_sub_component_lookup  c on b.dataplace_sub_component_id  = c.dataplace_sub_component_id
where table_id in ($templateData.input_data.where_condition$)   and a.valid_to_ts = '9999-12-31' and b.valid_to_ts = '9999-12-31'
and table_id not in (select table_id from nabu.checkpoint_status
where data_movement_id = $templateData.input_data.data_movement_id$ and status = 'ERROR' and job_type in ('nabu-ingestion-bot','nabu-federation-bot','nabu-sparkbot-ingestion','nabu-sparkbot-federation') and valid_to_ts = '9999-12-31')
)b
inner join nabu.dataplace_physical e on b.dataplace_id = e.dataplace_id
where e.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,
source_dataplace_id, data_movement_id from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id
from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
)a
),
source_details as (
select *,
$if(templateData.input_data.rename_table_condition)$ $replacing_special_characters("coalesce(new_table,'') ")$ as dest_table_name_2 $else$
$replacing_special_characters(" coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || coalesce(table_name,'') ")$ as dest_table_name_2
$endif$
from (
select a.*,'$(templateData.input_data.new_table_name)$' as new_table,
c.data_movement_id , destination_dataplace_id , destination_schema_directory_id,
ingestion_table_format,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_'  else '' end as source_schema_name
from source_table_details a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)a
)
--select * from source_details
,
destination_dataplace_details as(
select a.*, f.dataplace_sub_component_id as destination_dataplace_sub_component_id
from source_details a
inner join nabu.dataplace_physical f on a.destination_dataplace_id = f.dataplace_id
where f.valid_to_ts = '9999-12-31'
),
destination_table_id as (
select a.*, b.table_id as destination_table_id
from destination_dataplace_details a left outer join  nabu.dataplace_table_metadata_physical b
    on  a.destination_schema_directory_id = b.schema_id  and a.destination_dataplace_id = b.dataplace_id
and a.dest_table_name_2 = b.table_name and  b.valid_to_ts = '9999-12-31'
),
get_source_destination_type as(
select a.* , f.dataplace_sub_component_type as source_type,g.dataplace_sub_component_type as dest_type
from destination_table_id a
left outer join nabu.dataplace_sub_component_lookup f on a.source_dataplace_sub_component_id  = f.dataplace_sub_component_id
left outer join nabu.dataplace_sub_component_lookup g on a.destination_dataplace_sub_component_id = g.dataplace_sub_component_id
)
select table_id,data_movement_id as data_movement_id,destination_table_id ,source_type,dest_type,current_timestamp as req_ts
from get_source_destination_type
>>

insert_data_entity_lineage(templateData)::=<<
$if(templateData.get_metadata_for_data_lineage)$
INSERT INTO nabu.data_entity_lineage_physical
(data_movement_id, source_object_id, destination_object_id, lineage_uuid, lineage_document, cru_by, cru_ts)
VALUES $templateData.get_metadata_for_data_lineage:generate_data_lineage_insert_statement(templateData);separator=","$
$endif$
>>

insert_data_entity_lineage_file(templateData)::=<<
$if(templateData.get_metadata_for_data_lineage)$
INSERT INTO nabu.data_entity_lineage_physical
(data_movement_id, source_object_id, destination_object_id, lineage_uuid, lineage_document, cru_by, cru_ts)
VALUES $templateData.get_metadata_for_data_lineage:generate_data_lineage_insert_statement_file();separator=","$
$endif$
>>

generate_data_lineage_insert_statement(templateData,newData)::=<<
($templateData.data_movement_id$,$templateData.table_id$,$templateData.destination_table_id$,uuid_in((md5((random())::text))::cstring),'$generate_data_lineage(templateData,newData)$','Modak',current_timestamp)
>>

generate_data_lineage_insert_statement_file(templateData)::=<<
($templateData.data_movement_id$,$templateData.file_id$,$templateData.destination_table_id$,uuid_in((md5((random())::text))::cstring),'$generate_data_lineage_file(templateData)$','Modak',current_timestamp)
>>

lineage_requested_by(templateData)::=<<
Modak
>>

generate_data_lineage(templateData,newData) ::=<<
{"data_lineage": { "meta_data": { "created_by": "lineage_bot","requested_by": "$lineage_requested_by(templateData)$","requested_timestamp": "$templateData.req_ts$","executed_timestamp": "$templateData.req_ts$","tags": {}},"source_to_target_info":$get_sources(templateData,newData)$}}
>>

generate_data_lineage_file(templateData) ::=<<
{"data_lineage": { "meta_data": { "created_by": "lineage_bot","requested_by": "$lineage_requested_by(templateData)$","requested_timestamp": "$templateData.req_ts$","executed_timestamp": "$templateData.req_ts$","tags": {}},"source_to_target_info":$get_sources_file(templateData)$}}
>>

get_sources(sourceMap,newData)::=<<
{"src_type": "$templateData.source_type$","src_encrypted": "n","target_type": "$templateData.dest_type$","target_encrypted": "n","src_table_id":$templateData.table_id$,"dest_table_id":$templateData.destination_table_id$,"columns":[$newData.table_column_metadata_with_column_id:target_sources(sourceMap);separator=","$ ]}
>>

target_sources(map,queryInput)::=<<
{"target_table_id": $map.dest_table_id$,"target_database": "$map.destination_db$","target_schema": "$map.dest_schema_name$","target_table": "$map.dest_table_name$","target_column_name": "$map.dest_col_name$","target_column_type": "$map.dest_datatype$","target_column_id":$map.dest_col_id$,"column_type": "$map.column_type$","transformation": {"source_transformation_ops" : "","transformation_ops": "","src_columns": [{"src_table_id": $if(map.src_table_id)$$map.src_table_id$$else$null$endif$,"src_database": $if(map.source_db)$ "$map.source_db$"$else$null$endif$,"src_schema": $if(map.src_schema_name)$"$map.src_schema_name$"$else$null$endif$,"src_table": $if(map.src_table_name)$"$map.src_table_name$"$else$null$endif$,"src_column_name": $if(map.column_name)$"$map.column_name$"$else$null$endif$,"src_column_type": $if(map.src_datatype)$"$map.src_datatype$"$else$null$endif$,"src_column_id":$if(map.src_col_id)$$map.src_col_id$$else$null$endif$}],"details": null}}
>>

get_sources_file(sourceMap)::=<<
{"src_type": "$templateData.source_type$","src_encrypted": "n","target_type": "$templateData.dest_type$","target_encrypted": "n","src_table_id":$templateData.file_id$,"dest_table_id":$templateData.destination_table_id$,"columns":[]}
>>

insert_into_process_id_table_map(templateData)::=<<
 INSERT INTO nabu.process_id_table_map (data_movement_id, process_id, object_id, process_started_ts, batch_id, batch_name) VALUES
 ($templateData.input_data.data_movement_id$, $templateData.input_data.process_id$, $templateData.input_data.where_condition$, current_timestamp, $templateData.input_data.batch_id$, '$templateData.input_data.batch_name$')
 >>

generate_status_insertion_required_input(templateData)::=<<
{
"data_movement_id" : $templateData.data.data.input_data.data_movement_id$,
"bot_uuid" : "$templateData.bot_uuid$",
"process_id": $templateData.process_id$,
"batch_id": $templateData.data.data.input_data.batch_id$,
"table_id": $templateData.data.data.input_data.where_condition$,
"ssh_host": "$templateData.ssh_host$",
"applicationId": "$templateData.data.botLogicOutputMap.applicationId$",
$if(templateData.data.botLogicOutputMap.error_details)$"error_details": "$templateData.data.botLogicOutputMap.error_details$",$endif$
"exit_status": "$templateData.data.botLogicOutputMap.exit_status$",
"status": "$templateData.data.botLogicOutputMap.status$"
}
>>

generate_destination_required_input(templateData)::=<<
$(["generate_destination_required_input_",templateData.destination_metadata_category])(templateData)$
>>

generate_destination_required_input_relational(templateData)::=<<
{
"data_movement_id" : $templateData.data.data.data.data_movement_id$,
"flow_number" : $templateData.data.data.data.flow_number$,
"where_condition" : "$templateData.data.data.data.where_condition$",
"bot_uuid" : "$templateData.bot_uuid$",
"schema_drift_flag": $templateData.data.data.data.schema_drift_flag$,
"process_id": $templateData.process_id$,
"batch_id": $templateData.batch_id$
$if(!first(templateData.data.data.data.data.templateData.query_input.fetch_source_type).is_source_filetype)$,
"job_type_id":$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).job_type_id$,
"destination_dataplace_id":$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_dataplace_id$,
"source_table_id":$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).table_id$,
"destination_dataplace_component_type_id":$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_dataplace_component_type_id$,
"destination_schema_id":$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_id$,
"destination_schema_name":"$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$",
"destination_table_name":"$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$",
"column_metadata":[$templateData.data.query_input.metadata.data.data.templateData.query_input.column_metadata_output:forColumnMetadata();separator=","$],
"new_table_name": $if(first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name)$"$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$"$else$"$first(templateData.data.data.data.templateData.query_input.table_metadata_output).new_table_name$"$endif$,
"rename_table_condition": $if(first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output))$$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition$$else$$first(templateData.data.data.data.templateData.query_input.table_metadata_output).rename_table_condition$$endif$,
"already_ingested": $if(first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output))$$first(templateData.data.query_input.metadata.data.data.templateData.query_input.table_metadata_output).already_ingested$$else$$first(templateData.data.data.data.templateData.query_input.table_metadata_output).already_ingested$$endif$
$endif$
$if(first(templateData.data.query_input.metadata.data.data.templateData.query_input.get_cdc_info))$,
"view_name":"$first(templateData.data.query_input.metadata.data.data.templateData.query_input.get_cdc_info).view_name$",
"create_view":$first(templateData.data.query_input.metadata.data.data.templateData.query_input.get_cdc_info).create_view$
$endif$
}
>>

generate_destination_required_input_file(templateData)::=<<
{
"data_movement_id" : $templateData.data.data.data_movement_id$,
"flow_number" :$templateData.data.data.flow_number$,
"where_condition" :"$templateData.data.data.where_condition$",
"bot_uuid" : "$templateData.bot_uuid$",
"schema_drift_flag": $templateData.data.data.schema_drift_flag$,
"process_id": $templateData.process_id$,
"batch_id": $templateData.batch_id$
}
>>


forColumnMetadata(templateData)::=<<
{
"column_name":"$templateData.field_name$",
"data_type":$if(!templateData.data_type)$null$else$"$templateData.destination_datatype_name$"$endif$,
"data_scale":$if(!templateData.data_scale)$null$else$$templateData.data_scale$$endif$,
"data_length":$if(!templateData.data_length)$null$else$$templateData.data_length$$endif$,
"data_precision":$if(!templateData.data_precision)$null$else$$templateData.data_precision$$endif$,
"is_nullable":$templateData.is_nullable$,
"ordinal_position":$templateData.ordinal_position$
}
>>

generate_metadata_input(templateData)::=<<
{
"data_movement_id" : $templateData.data_movement_id$,
"flow_number" : $templateData.flow_number$,
"where_condition":"$templateData.where_condition$",
"process_id" : $templateData.process_id$,
"jwt_token" : "$templateData.botLogicOutputMap.jwt_token$",
"end_point" : "$templateData.botLogicOutputMap.end_point$",
"batch_id": $templateData.batch_id$,
"batch_name": "$templateData.batch_name$",
"job_type_id": "$templateData.job_type_id$",
"job_scheduled_user_id":"$templateData.job_scheduled_user_id$",
"job_schedule_id":$templateData.job_schedule_id$,
"last_run_timestamp":"$templateData.last_run_timestamp$"
}
>>

get_already_renamed_condition(templateData)::=<<
select
	case
		when a.object_id_count>0 then true
		else false
	end as table_renamed_exists
from
	(
	select
		count(*) as object_id_count
	from
		nabu.advanced_options_obj_checkpoint
	where
		advanced_option_obj_type_id = (
		select
			advanced_options_obj_type_id
		from
			nabu.advanced_options_obj_lookup
		where
			advanced_options_obj_type = 'Renaming Table'
  )
		and
  object_id in ($templateData.input_data.where_condition$)
		and
  data_movement_id = $templateData.input_data.data_movement_id$
		and valid_to_ts = '9999-12-31') a
>>

get_ingestion_information(templateData)::=<<
$(["get_ingestion_information_",first(templateData.fetch_source_type).source_metadata_category,"_to_",first(templateData.fetch_source_type).destination_metadata_category])(templateData)$
>>

get_ingestion_information_relational_to_relational(templateData)::=<<
with flow_tables as (
select table_id,schema_id,table_name,trim(schema_name) as schema_name,dataplace_id,estimated_rows
from nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)
and valid_to_ts = '9999-12-31'
),
data_movement_schema_drift_table_ids as(
select * from(select data_movement_id, dataplace_id, schema_directory_id, object_id, cru_ts, row_number() over(partition by object_id order by cru_ts desc) as rownum
from nabu.data_movement_schema_drift_details
where data_movement_id = $templateData.input_data.data_movement_id$
and object_id in ($templateData.input_data.where_condition$))a
where rownum = 1
),
rename_table_advance_option as(
select a.data_movement_id ,a.object_id , (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' as new_table_for_rename,
((a.advance_option_details->\>'advanced_option_flags')::json->\>'new_table_name') as rename_flag,
case when (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' is null  then false
when (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' = '' then false
else true end as rename_table_condition,
case when b.object_id is null then false else true end as already_ingested,
(
$if(first(templateData.get_already_renamed_condition).table_renamed_exists)$
select
  	case
  		when  (additional_info->\>'renamed_table_name') = $replacing_special_characters("((advance_option_details->\>'advanced_table_options')::json->\>'new_table_name')")$ then false
  		else true
  	end as already_renamed_flag
  from nabu.advanced_options_object_details a
  inner join nabu.advanced_options_obj_checkpoint b
  	on a.object_id =b.object_id and a.data_movement_id = b.data_movement_id
  	where
  	advanced_option_obj_type_id = (
  	select
  		advanced_options_obj_type_id
  	from
  		nabu.advanced_options_obj_lookup
  	where
  		advanced_options_obj_type = 'Renaming Table'
  )
  	and
  a.object_id in ($templateData.input_data.where_condition$) and
  a.data_movement_id = $templateData.input_data.data_movement_id$
  	and a.valid_to_ts = '9999-12-31'
  	and b.valid_to_ts = '9999-12-31'
$else$
select true as already_renamed_flag
$endif$
  	)
from nabu.advanced_options_object_details a
left join nabu.structured_jobtotable b
on a.data_movement_id =b.data_movement_id
and a.object_id =b.object_id
where
a.data_movement_id = $templateData.input_data.data_movement_id$
and a.object_id in ($templateData.input_data.where_condition$)
and a.valid_to_ts = '9999-12-31'
),
get_non_schema_drifted_table_ids as(
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
inner join
data_movement_schema_drift_table_ids b
on a.table_id = b.object_id
and a.schema_id = b.schema_directory_id
and a.dataplace_id = b.dataplace_id
where a.valid_to_ts = '9999-12-31'
and a.crt_ts < b.cru_ts
and b.data_movement_id = $templateData.input_data.data_movement_id$
and table_id in ($templateData.input_data.where_condition$)
union all
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
inner join
nabu.structured_jobtotable b
on a.table_id = b.object_id
and a.dataplace_id = b.dataplace_id
where b.data_movement_id = $templateData.input_data.data_movement_id$
and a.valid_to_ts = '9999-12-31'
and table_id in ($templateData.input_data.where_condition$)
and b.cru_ts > a.crt_ts
and b.object_type = 'S'
union all
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
where table_id in ($templateData.input_data.where_condition$)
and a.valid_to_ts = '9999-12-31'
and table_id not in (select object_id as table_id from nabu.structured_jobtotable b
where b.data_movement_id = $templateData.input_data.data_movement_id$ and b.object_type = 'S'
)
),
dataplace_schema_drifted_tables as(
select * from nabu.dataplace_table_schema_drift_details dtsdd where dataplace_id in
(select source_dataplace_id from nabu.data_movement_details_physical dmdp
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts = '9999-12-31')
and dtsdd.valid_to_ts = '9999-12-31'
),
flag_for_schema_drifted_tables as(
select a.table_id, a.schema_id,a.table_name,a.schema_id,a.dataplace_id,case when b.table_id is null and $templateData.input_data.job_type_id$ != 6
and c.table_id is not null then true
else false
end as schema_drift_flag
from flow_tables a
left join
get_non_schema_drifted_table_ids b
on a.table_id = b.table_id
left join
dataplace_schema_drifted_tables c
on a.table_id = c.table_id
),
source_dataplace_details as (
select a.dataplace_id as source_dataplace_id,
case when '$first(templateData.fetch_source_type).destination_metadata_category$' = 'relational' then true else false end as is_create_table,
case when  (b.dataplace_sub_component_type='mysql') then true else false end as is_mysql,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as source_url,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then trim(((dataplace_info->\>'jdbc_info')::json)->\>'database_name')
when ((b.dataplace_sub_component_type = 'bigquery')) then dataplace_info->\>'bucket_name'
else trim(dataplace_info->\>'database_name') end as source_db,
case when  (lower(b.dataplace_sub_component_type)='sas') then dataplace_info->\>'location_path' end as source_directory,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as oracle_jdbc_url,
case when b.dataplace_sub_component_type = 'hive' or b.dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')) then true else false end as is_source_hive,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_source_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as source_type, b.additional_info->\>'jdbc_driver' as source_jdbc_driver,
a.credential_id ,dataplace_info,
dataplace_info->\>'project_id'  as project,
dataplace_info->\>'bucket_name'  as bucket_name,
additional_info->\>'default_credential' as source_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as source_endpoint
from nabu.dataplace_physical a, nabu.dataplace_sub_component_lookup b
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select dataplace_id from flow_tables)
),
source_credential_details as (
select a.*,b.credential_id as source_credential_id,b.credential_type_id as source_credential_type_id
from source_dataplace_details a left outer join nabu.credential_info b
on a.credential_id = b.credential_id
and b.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,intermediate_type,
destination_file_format,source_dataplace_id, data_movement_id,verification_threshold, schema_drift_option, table_suffix, is_empty_suffix, table_timestamp from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name,
coalesce((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') as destination_file_format,
case
when
coalesce(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'skip_verification'),'false')::boolean = 'false' then COALESCE(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'verification_threshold')::float/100,0.1)
else 1
end as verification_threshold,
lower(coalesce(a.data_movement_additional_info ->'flow_details'->\>'destination_file_format','parquet')) as intermediate_type,
case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') is null then 'drop_create_table'
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') end as schema_drift_option,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix') as table_suffix,
coalesce(TRIM((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix'), '') = '' as is_empty_suffix,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'timestamp')::text as table_timestamp,engine_sub_type
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
inner join nabu.compute_engine c on a.compute_engine_id = c.compute_engine_id
inner join nabu.engine_mapping_lookup d on c.engine_mapping_id = d.engine_mapping_id
inner join nabu.engine_sub_type_lookup e on d.engine_sub_type_id = e.engine_sub_type_id
where a.valid_to_ts = '9999-12-31'
)a
)
--select * from data_movement_details
,
source_details as (
select b.* ,new_table_for_rename,case when (coalesce(rename_table_condition,false) and (already_renamed_flag))=true then true else false end as rename_table_condition,coalesce(already_ingested,false) as already_ingested,already_renamed_flag,rename_flag
from
(select a.*,c.*
from flow_tables a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)b left join rename_table_advance_option c on
b.data_movement_id = c.data_movement_id
)
--select * from source_details
,
source_level_details as (
select *,
case when a.rename_table_condition = true then
$replacing_special_characters("new_table_for_rename")$ else null end as new_table_name,
$replacing_special_characters(" (coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name) ")$ as destination_table_name
--coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name as destination_table_name
from (
select b.*,a.*,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_' else '' end as source_schema_name
from source_credential_details a
inner join source_details b on a.source_dataplace_id = b.dataplace_id
)a
),
file_system_credentials as (
select c.credential_id,d.dataplace_id ,c.credential_name,c.credential_type_id, credential_type,extract(epoch from c.mod_ts) as credential_epoch from nabu.dataplace_physical d left outer join nabu.credential_info c
on (((dataplace_info->\>'filesystem_info')::json)->\>'credential_id')::integer = c.credential_id left join nabu.credential_type_lookup ct on c.credential_type_id = ct.credential_type_id
where d.dataplace_id in (select destination_dataplace_id from data_movement_details)
),
dest_dataplace_details as (
-- For the destination dataplace, get it's details
select a.dataplace_id as destination_dataplace_id, a.dataplace_component_type_id as destination_dataplace_component_type_id,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as destination_url,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as destination_oracle_jdbc_url,
case
when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when (b.dataplace_sub_component_type= 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when (b.dataplace_sub_component_type = 'bigquery') then dataplace_info->\>'ingestion_root_path'
when (b.dataplace_sub_component_type = 'redshift') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when (b.dataplace_sub_component_type = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'ingestion_root_path'))
else null
end as path,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then true else false end as is_destination_hive,
case when (b.dataplace_sub_component_type in ('s3','hdfs','adls_gen2')) then false else true end as apply_transformation,
case when (b.dataplace_sub_component_type in ('mysql','postgres','sql_server','oracle')) then true else false end as is_jdbcmode,
case
when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'endpoint'
else null
end as filesystem_endpoint,
case
when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive')
and dataplace_info->\>'connection_mode' = 'Cluster mode' then lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type')
when (b.dataplace_sub_component_type  = 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
when (b.dataplace_sub_component_type = 'bigquery') then 'gcs'
when (b.dataplace_sub_component_type  = 'redshift' and dmd.intermediate_type = 'parquet') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
when (b.dataplace_sub_component_type in('adls_gen2','s3')) then b.additional_info->\>'file_system_type'
when (b.dataplace_sub_component_type  = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
else null
end as  file_system_type,
case
when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode' and  lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type') = 's3a' then true else false end as is_apache_jdbc_driver,
case
when ((b.dataplace_sub_component_type = 'hive')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type  = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type = 'bigquery')) then  cast(a.credential_id as text)
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
else null end as filesystem_credential_id,
case
when ((b.dataplace_sub_component_type  = 'hive')) then cast(coalesce (fs.credential_type_id,'1') as text)
when ((b.dataplace_sub_component_type = 'azure_synapse')) then cast(fs.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'bigquery')) then cast(ci.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then cast(fs.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'snowflake')) then cast(fs.credential_type_id as text)
else null end as filesystem_credential_type_id,
case
when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
else null
end as adls_gen2_directory_name,
case
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
when ((b.dataplace_sub_component_type = 's3')) then dataplace_info->\>'bucket'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
else null
end as s3_bucket,
case
when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'container'))
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
else null
end as adls_gen2_container,
case
   when ((b.dataplace_sub_component_type = 'bigquery')) then dataplace_info->\>'bucket_name'
   else null
end as gcs_bucket,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'accountName'))
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   else null
end as storage_account,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then ((dataplace_info->\>'jdbc_info')::json)->\>'database_name'
else dataplace_info->\>'database_name' end as destination_db,
case when  (lower(b.dataplace_sub_component_type)='sas') then dataplace_info->\>'location_path' end as destination_directory,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as destination_type,
lower(case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then (select coalesce ((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') from nabu.data_movement_physical dmp where data_movement_id=$templateData.input_data.data_movement_id$) else b.dataplace_sub_component_type end) as target_type,
b.additional_info->\>'jdbc_driver' as destination_jdbc_driver,
b.dataplace_sub_component_id as destination_dataplace_sub_component_id,
a.credential_id as destination_credential_id,
b.additional_info->\>'default_credential' as dest_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as destination_endpoint,
fs.credential_type as filesystem_credential_type,
fs.credential_epoch as filesystem_credential_epoch
from nabu.dataplace_physical a left join nabu.credential_info ci on a.credential_id = ci.credential_id  ,
 nabu.dataplace_sub_component_lookup b, file_system_credentials fs, data_movement_details dmd
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select destination_dataplace_id from data_movement_details)
),
dest_credential_details as (
select a.*,b.credential_type_id as destination_credential_type_id,
case when (a.file_system_type = 's3a') then true else false end as is_file_system_type_s3a,
case when (a.filesystem_credential_type_id = '2' or a.filesystem_credential_id is null) then true else false end as is_filesystem_credential_type_aws,
case when (b.credential_type_id = 8 or b.credential_type_id =9) then true else false end as is_destination_credential_kerberos,
ct.credential_type as destination_credential_type
from dest_dataplace_details a left outer join nabu.credential_info b
on a.destination_credential_id = b.credential_id
left join nabu.credential_type_lookup ct on b.credential_type_id = ct.credential_type_id
and b.valid_to_ts = '9999-12-31'
),
get_engine_type as (
select bcw.data_movement_engine_type as datamovement_engine_type,dmp.data_movement_id from nabu.data_movement_physical dmp
inner join nabu.bot_configuration_workflow bcw on dmp.workflow_id=bcw.workflow_id
where data_movement_id =$templateData.input_data.data_movement_id$
),
intermediate_details as (
select intermediate_type,source_dataplace_id from data_movement_details
)
select
$first(templateData.fetch_source_type).is_destination_filetype$ as is_destination_file,a.dataplace_id,a.schema_id,a.schema_name,a.table_id,
table_timestamp, table_suffix, h.schema_drift_flag,$first(templateData.fetch_source_type).is_source_filetype$ as is_source_filetype,
case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
else destination_table_name end as destination_table_name,
case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
else destination_table_name end as destination_backup_table_name,
 case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix)
          when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else new_table_name end as new_table_name,
     case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix)
          when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else new_table_name end as backup_new_table_name,project,
          a.table_name,source_url,source_credential_id,source_credential_type_id,source_jdbc_driver,source_type,
		  case when (source_type ='bigquery') then true else false end as is_bigquery,
estimated_rows,f.schema_name as destination_schema_name,a.destination_dataplace_id,destination_dataplace_sub_component_id,rename_table_condition,already_ingested,already_renamed_flag,rename_flag,
a.verification_threshold, destination_url,$templateData.input_data.process_id$ as process_id,
$templateData.input_data.batch_id$ as batch_id,
'$templateData.input_data.batch_name$' as batch_name,$templateData.input_data.data_movement_id$ as data_movement_id,
'$templateData.input_data.where_condition$' as where_condition,
path,bucket_name,project,filesystem_endpoint,e.destination_dataplace_component_type_id,f.schema_id as destination_schema_id,f.schema_name as destination_schema_name,'$templateData.input_data.job_type_id$' as job_type_id,
case when (id.intermediate_type in ('default')) then true else false end as is_default,
file_system_type,destination_oracle_jdbc_url,oracle_jdbc_url,dest_default_credential_exists::boolean,source_default_credential_exists::boolean,
is_spark_hive, destination_db,destination_directory, destination_type,target_type, destination_file_format,destination_jdbc_driver,$templateData.input_data.flow_number$ as flow_number,
source_directory, destination_credential_id, destination_credential_type_id,is_destination_credential_kerberos,is_apache_jdbc_driver,ingestion_table_format,source_db,is_source_hive,is_source_spark_hive,
case when schema_drift_option = 'bkp_table' and h.schema_drift_flag = true
then true END as create_backup_table, schema_drift_option,
e.filesystem_credential_id,e.filesystem_credential_type_id,e.filesystem_credential_type,e.is_filesystem_credential_type_aws,e.is_file_system_type_s3a,
e.adls_gen2_directory_name,e.adls_gen2_container,e.storage_account,e.s3_bucket,e.gcs_bucket,is_mysql,a.is_create_table,a.intermediate_type,g.datamovement_engine_type,is_jdbcmode,apply_transformation,is_destination_hive,
'$templateData.input_data.jwt_token$' as jwt_token, '$templateData.input_data.end_point$' as end_point,
case when source_endpoint is null then 'null' else source_endpoint end as source_endpoint,
case when destination_endpoint is null then 'null' else destination_endpoint end as destination_endpoint,
case when (e.filesystem_credential_type = 'Azure Active Directory' or destination_credential_type ='Azure Active Directory') then true else false end as is_azure_oauth,
case when (e.filesystem_credential_type = 'AWS IAM Role' or destination_credential_type ='AWS IAM Role') then true else false end as is_aws_iam,
e.filesystem_credential_epoch
from nabu.dataplace_relational_component_physical f,
source_level_details a,  dest_credential_details e,get_engine_type g,
flag_for_schema_drifted_tables h, intermediate_details id
where a.destination_dataplace_id = e.destination_dataplace_id
and a.data_movement_id = $templateData.input_data.data_movement_id$
and f.dataplace_id = e.destination_dataplace_id
and a.destination_schema_directory_id = f.schema_id
and f.valid_to_ts = '9999-12-31'
and a.table_id = h.table_id
and a.data_movement_id = g.data_movement_id
>>


get_ingestion_information_relational_to_file(templateData)::=<<
with flow_tables as (
select table_id,schema_id,table_name,trim(schema_name) as schema_name,dataplace_id,estimated_rows
from nabu.dataplace_table_metadata_physical
where table_id in ($templateData.input_data.where_condition$)
and valid_to_ts = '9999-12-31'
),
data_movement_schema_drift_table_ids as(
select * from(select data_movement_id, dataplace_id, schema_directory_id, object_id, cru_ts, row_number() over(partition by object_id order by cru_ts desc) as rownum
from nabu.data_movement_schema_drift_details
where data_movement_id = $templateData.input_data.data_movement_id$
and object_id in ($templateData.input_data.where_condition$))a
where rownum = 1
),
rename_table_advance_option as(
select a.data_movement_id ,a.object_id , (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' as new_table_for_rename
,case when (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' is null  then false
when (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' = '' then false
else true end as rename_table_condition,
case when b.object_id is null then false else true end as already_ingested,
(select case when object_id_count = 0 then true else false end as already_renamed_flag from (
SELECT  count(*) as object_id_count
FROM nabu.advanced_options_obj_checkpoint
where
advanced_option_obj_type_id = (SELECT advanced_options_obj_type_id
FROM nabu.advanced_options_obj_lookup
WHERE advanced_options_obj_type = 'Renaming Table'
) and
object_id in ($templateData.input_data.where_condition$) and
data_movement_id = $templateData.input_data.data_movement_id$)a)
from nabu.advanced_options_object_details a
left join nabu.structured_jobtotable b
on a.data_movement_id =b.data_movement_id
and a.object_id =b.object_id
where
a.data_movement_id = $templateData.input_data.data_movement_id$
and a.object_id in ($templateData.input_data.where_condition$)
and a.valid_to_ts = '9999-12-31'
),
get_non_schema_drifted_table_ids as(
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
inner join
data_movement_schema_drift_table_ids b
on a.table_id = b.object_id
and a.schema_id = b.schema_directory_id
and a.dataplace_id = b.dataplace_id
where a.valid_to_ts = '9999-12-31'
and a.crt_ts < b.cru_ts
and b.data_movement_id = $templateData.input_data.data_movement_id$
and table_id in ($templateData.input_data.where_condition$)
union all
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
inner join
nabu.structured_jobtotable b
on a.table_id = b.object_id
and a.dataplace_id = b.dataplace_id
where b.data_movement_id = $templateData.input_data.data_movement_id$
and a.valid_to_ts = '9999-12-31'
and table_id in ($templateData.input_data.where_condition$)
and b.cru_ts > a.crt_ts
and b.object_type = 'S'
union all
select a.dataplace_id,a.schema_id, a.dataplace_component_type_id,a.table_id
from nabu.dataplace_table_schema_drift_details a
where table_id in ($templateData.input_data.where_condition$)
and a.valid_to_ts = '9999-12-31'
and table_id not in (select object_id as table_id from nabu.structured_jobtotable b
where b.data_movement_id = $templateData.input_data.data_movement_id$ and b.object_type = 'S'
)
),
dataplace_schema_drifted_tables as(
select * from nabu.dataplace_table_schema_drift_details dtsdd where dataplace_id in
(select source_dataplace_id from nabu.data_movement_details_physical dmdp
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts = '9999-12-31')
and dtsdd.valid_to_ts = '9999-12-31'
),
flag_for_schema_drifted_tables as(
select a.table_id, a.schema_id,a.table_name,a.schema_id,a.dataplace_id,case when b.table_id is null and $templateData.input_data.job_type_id$ != 6
and c.table_id is not null then true
else false
end as schema_drift_flag
from flow_tables a
left join
get_non_schema_drifted_table_ids b
on a.table_id = b.table_id
left join
dataplace_schema_drifted_tables c
on a.table_id = c.table_id
),
source_dataplace_details as (
select a.dataplace_id as source_dataplace_id,
case when '$first(templateData.fetch_source_type).destination_metadata_category$' = 'relational' then true else false end as is_create_table,
case when  (b.dataplace_sub_component_type='mysql') then true else false end as is_mysql,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as source_url,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then trim(((dataplace_info->\>'jdbc_info')::json)->\>'database_name')
else trim(dataplace_info->\>'database_name') end as source_db,
case when  (lower(b.dataplace_sub_component_type)='sas') then dataplace_info->\>'location_path' end as source_directory,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as oracle_jdbc_url,
case when b.dataplace_sub_component_type = 'hive' or b.dataplace_sub_component_type = 'cloudera_hive' then
(dataplace_info->\>'jdbc_info')::json->\>'database_name'
else dataplace_info->\>'database_name' end as database_name,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')) then true else false end as is_source_hive,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_source_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as source_type, b.additional_info->\>'jdbc_driver' as source_jdbc_driver,
a.credential_id ,dataplace_info,
dataplace_info->\>'project_id'  as project,
dataplace_info->\>'bucket_name'  as bucket_name,
additional_info->\>'default_credential' as source_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as source_endpoint
from nabu.dataplace_physical a, nabu.dataplace_sub_component_lookup b
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select dataplace_id from flow_tables)
),
source_credential_details as (
select a.*,b.credential_id as source_credential_id,b.credential_type_id as source_credential_type_id
from source_dataplace_details a left outer join nabu.credential_info b
on a.credential_id = b.credential_id
and b.valid_to_ts = '9999-12-31'
),
data_movement_details as (
select ingestion_table_format,valid_db_name,valid_schema_name,destination_dataplace_id ,destination_schema_directory_id ,intermediate_type,
destination_file_format,source_dataplace_id, data_movement_id,verification_threshold, schema_drift_option, table_suffix, is_empty_suffix, table_timestamp from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'database_name')::boolean as valid_db_name,
((data_movement_additional_info->\>'ingestion_table_format')::json->\>'schema_name')::boolean as valid_schema_name,
coalesce((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') as destination_file_format,
case
when
coalesce(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'skip_verification'),'false')::boolean = 'false' then COALESCE(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'verification_threshold')::float/100,0.1)
else 1
end as verification_threshold,
lower(coalesce(a.data_movement_additional_info ->'flow_details'->\>'destination_file_format','parquet')) as intermediate_type,
case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') is null then 'drop_create_table'
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') end as schema_drift_option,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix') as table_suffix,
coalesce(TRIM((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix'), '') = '' as is_empty_suffix,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'timestamp')::text as table_timestamp
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
where valid_to_ts = '9999-12-31'
)a
)
--select * from data_movement_details
,
source_details as (
select b.* ,new_table_for_rename,case when (coalesce(rename_table_condition,false) and (already_renamed_flag))=true then true else false end as rename_table_condition,coalesce(already_ingested,false) as already_ingested
from
(select a.*,c.*
from flow_tables a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)b left join rename_table_advance_option c on
b.data_movement_id = c.data_movement_id
)
--select * from source_details
,
source_level_details as (
select *,
case when a.rename_table_condition = true then
$replacing_special_characters("new_table_for_rename")$ else null end as new_table_name,
$replacing_special_characters(" (coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name) ")$ as destination_table_name
--coalesce(ingestion_table_format,'') || coalesce(db_name,'') || coalesce(source_schema_name,'') || table_name as destination_table_name
from (
select b.*,a.*,
case when valid_db_name is true then database_name || '_' else '' end as db_name,
case when valid_schema_name is true then schema_name || '_' else '' end as source_schema_name
from source_credential_details a
inner join source_details b on a.source_dataplace_id = b.dataplace_id
)a
),
file_system_credentials as (
select c.credential_id,d.dataplace_id ,c.credential_name,c.credential_type_id, credential_type,extract(epoch from c.mod_ts) as credential_epoch from nabu.dataplace_physical d left outer join nabu.credential_info c
on (((dataplace_info->\>'filesystem_info')::json)->\>'credential_id')::integer = c.credential_id left join nabu.credential_type_lookup ct on c.credential_type_id = ct.credential_type_id
where d.dataplace_id in (select destination_dataplace_id from data_movement_details)
),
dest_dataplace_details as (
-- For the destination dataplace, get it's details
select a.dataplace_id as destination_dataplace_id,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as destination_url,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as destination_oracle_jdbc_url,
case
when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when (b.dataplace_sub_component_type= 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when (b.dataplace_sub_component_type = 'bigquery') then dataplace_info->\>'ingestion_root_path'
when (b.dataplace_sub_component_type = 'redshift') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when (b.dataplace_sub_component_type = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'ingestion_root_path'))
else null
end as path,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then true else false end as is_destination_hive,
case when (b.dataplace_sub_component_type in ('s3','hdfs','adls_gen2')) then false else true end as apply_transformation,
case when (b.dataplace_sub_component_type in ('mysql','postgres','sql_server','oracle')) then true else false end as is_jdbcmode,
case
when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'endpoint'
else null
end as filesystem_endpoint,
case
when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive')
and dataplace_info->\>'connection_mode' = 'Cluster mode' then lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type')
when (b.dataplace_sub_component_type  = 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
when (b.dataplace_sub_component_type = 'bigquery') then 'gcs'
when (b.dataplace_sub_component_type  = 'redshift' and dmd.intermediate_type = 'parquet') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
when (b.dataplace_sub_component_type in('adls_gen2','s3')) then b.additional_info->\>'file_system_type'
when (b.dataplace_sub_component_type  = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
else null
end as  file_system_type,
case
when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode' and  lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type') = 's3a' then true else false end as is_apache_jdbc_driver,
case
when ((b.dataplace_sub_component_type = 'hive')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type  = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type = 'bigquery')) then  cast(a.credential_id as text)
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
else null end as filesystem_credential_id,
case
when ((b.dataplace_sub_component_type  = 'hive')) then cast(coalesce (fs.credential_type_id,'1') as text)
when ((b.dataplace_sub_component_type = 'azure_synapse')) then cast(fs.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'bigquery')) then cast(ci.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then cast(fs.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'snowflake')) then cast(fs.credential_type_id as text)
else null end as filesystem_credential_type_id,
case
when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
else null
end as adls_gen2_directory_name,
case
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
when ((b.dataplace_sub_component_type = 's3')) then dataplace_info->\>'bucket'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
else null
end as s3_bucket,
case
when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'container'))
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
else null
end as adls_gen2_container,
case
when ((b.dataplace_sub_component_type = 'bigquery')) then dataplace_info->\>'bucket_name'
else null
end as gcs_bucket,
case
when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
when ((b.dataplace_sub_component_type = 'adls_gen2')) then ((dataplace_info->\>'accountName'))
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
else null
end as storage_account,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then ((dataplace_info->\>'jdbc_info')::json)->\>'database_name'
else dataplace_info->\>'database_name' end as destination_db,
case when  (lower(b.dataplace_sub_component_type)='sas') then dataplace_info->\>'location_path' end as destination_directory,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as destination_type,
lower(case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then (select coalesce ((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') from nabu.data_movement_physical dmp where data_movement_id=$templateData.input_data.data_movement_id$) else b.dataplace_sub_component_type end) as target_type,
b.additional_info->\>'jdbc_driver' as destination_jdbc_driver,
b.dataplace_sub_component_id as destination_dataplace_sub_component_id,
a.credential_id as destination_credential_id,
b.additional_info->\>'default_credential' as dest_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as destination_endpoint,
fs.credential_type as filesystem_credential_type,
fs.credential_epoch as filesystem_credential_epoch
from nabu.dataplace_physical a left join nabu.credential_info ci on a.credential_id = ci.credential_id  ,
 nabu.dataplace_sub_component_lookup b, file_system_credentials fs, data_movement_details dmd
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select destination_dataplace_id from data_movement_details)
),
dest_credential_details as (
select a.*,b.credential_type_id as destination_credential_type_id,
case when (a.file_system_type = 's3a') then true else false end as is_file_system_type_s3a,
case when (a.filesystem_credential_type_id = '2' or a.filesystem_credential_id is null) then true else false end as is_filesystem_credential_type_aws,
case when (b.credential_type_id = 8 or b.credential_type_id =9) then true else false end as is_destination_credential_kerberos,
ct.credential_type as destination_credential_type
from dest_dataplace_details a left outer join nabu.credential_info b
on a.destination_credential_id = b.credential_id
left join nabu.credential_type_lookup ct on b.credential_type_id = ct.credential_type_id
and b.valid_to_ts = '9999-12-31'
),
get_engine_type as (
select bcw.data_movement_engine_type as datamovement_engine_type,dmp.data_movement_id from nabu.data_movement_physical dmp
inner join nabu.bot_configuration_workflow bcw on dmp.workflow_id=bcw.workflow_id
where data_movement_id =$templateData.input_data.data_movement_id$
),
intermediate_details as (
select intermediate_type,source_dataplace_id from data_movement_details
)
select
$first(templateData.fetch_source_type).is_destination_filetype$ as is_destination_file,a.dataplace_id,a.schema_id,a.schema_name,a.table_id,
table_timestamp, table_suffix, h.schema_drift_flag,$first(templateData.fetch_source_type).is_source_filetype$ as is_source_filetype,
case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
else destination_table_name end as destination_table_name,
case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
else destination_table_name end as destination_backup_table_name,
 case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix)
          when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else new_table_name end as new_table_name,
     case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
          when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix)
          when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',to_char(current_timestamp ,table_timestamp))
          else new_table_name end as backup_new_table_name,project,a.table_name,source_url,source_credential_id,source_credential_type_id,source_jdbc_driver,source_type,
estimated_rows,a.schema_name as destination_schema_name,
a.destination_dataplace_id,destination_dataplace_sub_component_id,rename_table_condition,already_ingested,
a.verification_threshold, destination_url,$templateData.input_data.process_id$ as process_id,
$templateData.input_data.batch_id$ as batch_id,
'$templateData.input_data.batch_name$' as batch_name,$templateData.input_data.data_movement_id$ as data_movement_id,
'$templateData.input_data.where_condition$' as where_condition,
path,bucket_name,project,filesystem_endpoint,
case when (id.intermediate_type in ('default')) then true else false end as is_default,
file_system_type,destination_oracle_jdbc_url,oracle_jdbc_url,dest_default_credential_exists::boolean,source_default_credential_exists::boolean,
is_spark_hive, destination_db,destination_directory, destination_type,target_type, destination_file_format,destination_jdbc_driver,$templateData.input_data.flow_number$ as flow_number,
source_directory, destination_credential_id, destination_credential_type_id,is_destination_credential_kerberos,is_apache_jdbc_driver,ingestion_table_format,source_db,is_source_hive,is_source_spark_hive,
case when schema_drift_option = 'bkp_table' and h.schema_drift_flag = true
then true END as create_backup_table, schema_drift_option,
e.filesystem_credential_id,e.filesystem_credential_type_id,e.filesystem_credential_type,e.is_filesystem_credential_type_aws,e.is_file_system_type_s3a,
e.adls_gen2_directory_name,e.adls_gen2_container,e.storage_account,e.s3_bucket,e.gcs_bucket,is_mysql,a.is_create_table,a.intermediate_type,g.datamovement_engine_type,is_jdbcmode,apply_transformation,is_destination_hive,
'$templateData.input_data.jwt_token$' as jwt_token, '$templateData.input_data.end_point$' as end_point,
case when source_endpoint is null then 'null' else source_endpoint end as source_endpoint,
case when destination_endpoint is null then 'null' else destination_endpoint end as destination_endpoint,
case when (e.filesystem_credential_type = 'Azure Active Directory' or destination_credential_type ='Azure Active Directory') then true else false end as is_azure_oauth,
case when (e.filesystem_credential_type = 'AWS IAM Role' or destination_credential_type ='AWS IAM Role') then true else false end as is_aws_iam,
e.filesystem_credential_epoch,
f.directory_name as destination_schema_name,upper(destination_table_name) as upper_case_destination_table_name,id.source_dataplace_id,g.datamovement_engine_type,id.intermediate_type,is_create_table
from nabu.dataplace_file_system_component f,
source_level_details a,  dest_credential_details e,get_engine_type g,
flag_for_schema_drifted_tables h, intermediate_details id
where a.destination_dataplace_id = e.destination_dataplace_id
and a.data_movement_id = $templateData.input_data.data_movement_id$
and f.dataplace_id = e.destination_dataplace_id
and a.destination_schema_directory_id = f.directory_id
and f.valid_to_ts = '9999-12-31'
and a.table_id = h.table_id
and a.data_movement_id = g.data_movement_id
>>


get_ingestion_information_file_to_relational(templateData)::=<<
with flow_files as(
select file_id,directory_id, REGEXP_REPLACE(file_name, concat('(.*)\.',file_format), '\1') as file_name,fm.dataplace_id,file_size,
case when file_format like 'csv%' then 'csv'
     when file_format like 'tsv%' then 'csv' else file_format
end as file_format,
additional_info ->\> 'hasHeader' as has_header,
regexp_replace(additional_info ->\> 'delimiter',E'\t','\\\t') as delimiter,
case
	when file_format like 'csv%' then additional_info ->\> 'quoteEscapeChar'
	when file_format like 'tsv%' then additional_info ->\> 'escapeChar'
end as escape_char,
case when file_format like 'csv%' then additional_info ->\> 'quoteChar' end as quote_char,
regexp_replace(regexp_replace(additional_info ->\> 'lineSeparator',E'\n','\\\n'),E'\r','\\\r') as linesep,
file_relative_path,file_absolute_path
from (
select file_id,directory_id,file_name,dataplace_id,size as file_size,file_format,
additional_info,
file_relative_path,file_absolute_path from nabu.dataplace_file_metadata
where file_id in ($templateData.input_data.where_condition$)
and valid_to_ts = '9999-12-31') fm
),
data_movement_schema_drift_file_ids as(
select * from(select data_movement_id, dataplace_id, schema_directory_id, object_id, cru_ts, row_number() over(partition by object_id order by cru_ts desc) as rownum
from nabu.data_movement_schema_drift_details
where data_movement_id = $templateData.input_data.data_movement_id$
and object_id in ($templateData.input_data.where_condition$))a
where rownum = 1
),
rename_table_advance_option as(
select a.data_movement_id ,a.object_id , (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' as new_table_for_rename
,case when (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' is null  then false
when (a.advance_option_details->\>'advanced_table_options')::json->\>'new_table_name' = '' then false
else true end as rename_table_condition, case when b.object_id is null then false else true end as already_ingested,
(select case when object_id_count = 0 then true else false end as already_renamed_flag from (
SELECT  count(*) as object_id_count
FROM nabu.advanced_options_obj_checkpoint
where
advanced_option_obj_type_id = (SELECT advanced_options_obj_type_id
FROM nabu.advanced_options_obj_lookup
WHERE advanced_options_obj_type = 'Renaming Table'
) and
object_id in ($templateData.input_data.where_condition$) and
data_movement_id = $templateData.input_data.data_movement_id$)a)
from nabu.advanced_options_object_details a
left join nabu.structured_jobtotable b
on a.data_movement_id =b.data_movement_id
and a.object_id =b.object_id
where
a.data_movement_id = $templateData.input_data.data_movement_id$
and a.object_id in ($templateData.input_data.where_condition$)
and a.valid_to_ts = '9999-12-31'
),
get_non_schema_drifted_file_ids as(
select a.dataplace_id,a.directory_id, a.dataplace_component_type_id,a.file_id
from nabu.semi_structured_file_schema_drift a
inner join
data_movement_schema_drift_file_ids b
on a.file_id = b.object_id
and a.directory_id = b.schema_directory_id
and a.dataplace_id = b.dataplace_id
and a.cru_ts < b.cru_ts
and b.data_movement_id = $templateData.input_data.data_movement_id$
and file_id in ($templateData.input_data.where_condition$)
union all
select a.dataplace_id,a.directory_id, a.dataplace_component_type_id,a.file_id
from nabu.semi_structured_file_schema_drift a
inner join
nabu.structured_jobtotable b
on a.file_id = b.object_id
and a.dataplace_id = b.dataplace_id
where b.data_movement_id = $templateData.input_data.data_movement_id$
and file_id in ($templateData.input_data.where_condition$)
and b.cru_ts > a.cru_ts
and b.object_type = 'S'
union all
select a.dataplace_id,a.directory_id, a.dataplace_component_type_id,a.file_id
from nabu.semi_structured_file_schema_drift a
where file_id in ($templateData.input_data.where_condition$)
and file_id not in (select object_id as file_id from nabu.structured_jobtotable b
where b.data_movement_id = $templateData.input_data.data_movement_id$ and b.object_type = 'S'
)
),
dataplace_schema_drifted_files as(
select * from nabu.semi_structured_file_schema_drift where dataplace_id in
(select source_dataplace_id from nabu.data_movement_details_physical dmdp
where data_movement_id = $templateData.input_data.data_movement_id$
and valid_to_ts = '9999-12-31')
),
flag_for_schema_drifted_files as(
select a.file_id, a.directory_id,a.file_name,a.dataplace_id,case when b.file_id is null and $templateData.input_data.job_type_id$ != 6
and c.file_id is not null then true
else false
end as schema_drift_flag
from flow_files a
left join
get_non_schema_drifted_file_ids b
on a.file_id = b.file_id
left join
dataplace_schema_drifted_files c
on a.file_id = c.file_id
),
source_dataplace_details as (
select a.dataplace_id as source_dataplace_id,
case when '$first(templateData.fetch_source_type).destination_metadata_category$' = 'relational' then true else false end as is_create_table,
case when (b.dataplace_sub_component_type!='unix' )  then dataplace_info->\>'host_name'else null end as host_name,
case when (b.dataplace_sub_component_type='smb' )  then dataplace_info->\>'share_name' else null end as share_name,
case when (b.dataplace_sub_component_type='smb' )  then dataplace_info->\>'domain' else null end as domain,
case when (b.dataplace_sub_component_type='unix' )  then dataplace_info->\>'path'  else null end as root_location_path,
case when (b.dataplace_sub_component_type = 's3') then dataplace_info->\>'bucket'  else null end as source_s3_bucket,
case when (b.dataplace_sub_component_type = 'adls_gen2') then dataplace_info->\> 'container' else null end as source_adls_gen2_container,
case when (b.dataplace_sub_component_type = 'adls_gen2') then dataplace_info->\> 'accountName' else null end as source_storage_account,
lower(b.dataplace_sub_component_type)  as source_type,
dataplace_info->\>'project_id'  as project,
dataplace_info->\>'bucket_name'  as bucket_name,
a.credential_id ,dataplace_info,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as source_endpoint
from nabu.dataplace_physical a inner join nabu.dataplace_sub_component_lookup b
on a.dataplace_sub_component_id = b.dataplace_sub_component_id
left outer join nabu.credential_info c
on a.credential_id = c.credential_id
inner join flow_files g
on a.dataplace_id = g.dataplace_id
and a.valid_to_ts = '9999-12-31'
)
--select * from source_dataplace_details;
,
source_credential_details as (
select a.*,b.credential_id as source_credential_id,b.credential_type_id as source_credential_type_id,ct.credential_type as source_credential_type
from source_dataplace_details a left outer join nabu.credential_info b
on a.credential_id = b.credential_id
left join nabu.credential_type_lookup ct on b.credential_type_id = ct.credential_type_id
and b.valid_to_ts = '9999-12-31'
)
--select * from source_dataplace_details;
,
data_movement_details as (
select ingestion_table_format,destination_dataplace_id ,destination_schema_directory_id ,intermediate_type,
destination_file_format,source_dataplace_id, data_movement_id,verification_threshold, schema_drift_option, table_suffix, is_empty_suffix, table_timestamp
from (
select destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, a.data_movement_id,
case when ((data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix')::boolean
then (data_movement_additional_info->\>'ingestion_table_format')::json->\>'prefix_value' || '_'
else '' end as ingestion_table_format,
coalesce((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') as destination_file_format,
case
when
coalesce(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'skip_verification'),'false')::boolean = 'false' then COALESCE(((data_movement_additional_info->'flow_details'->\>'verification_details')::json->\>'verification_threshold')::float/100,0.1)
else 1
end as verification_threshold,
lower(coalesce(a.data_movement_additional_info ->'flow_details'->\>'destination_file_format','parquet')) as intermediate_type,
case when ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') is null then 'drop_create_table'
else ((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'advanced_options_sub_type') end as schema_drift_option,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix') as table_suffix,
coalesce(TRIM((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'suffix'), '') = '' as is_empty_suffix,
((((((data_movement_additional_info->\>'flow_details')::json->\>'schema_drift')::json)->\>'config')::json)->\>'timestamp')::text as table_timestamp,
engine_sub_type
from nabu.data_movement_physical a
inner join  (select distinct  destination_dataplace_id ,destination_schema_directory_id , source_dataplace_id, data_movement_id  from nabu.data_movement_details_physical
where valid_to_ts = '9999-12-31' and data_movement_id = $templateData.input_data.data_movement_id$ )b on a.data_movement_id = b.data_movement_id
inner join nabu.compute_engine c on a.compute_engine_id = c.compute_engine_id
inner join nabu.engine_mapping_lookup d on c.engine_mapping_id = d.engine_mapping_id
inner join nabu.engine_sub_type_lookup e on d.engine_sub_type_id = e.engine_sub_type_id
where a.valid_to_ts = '9999-12-31'
)a
)
--select * from data_movement_details
,
source_details as (
select b.* ,new_table_for_rename,case when (coalesce(rename_table_condition,false) and (already_renamed_flag))=true then true else false end as rename_table_condition,coalesce(already_ingested,false) as already_ingested
from
(select a.*,c.*
from flow_files a
inner join data_movement_details c
on a.dataplace_id = c.source_dataplace_id
)b left join rename_table_advance_option c on
b.data_movement_id = c.data_movement_id
)
--select * from source_details
,
source_level_details as (
select *,
case when '$first(templateData.fetch_source_type).source_metadata_category$' = 'file' then true else false end as is_source_filetype,
case when a.rename_table_condition = true then regexp_replace(new_table_for_rename,'\..*\$','') else null end as new_table_name,
case when a.file_name ~ '^[0-9]' and ingestion_table_format = '' then concat('t_',regexp_replace(regexp_replace((a.file_name) ,'\..*\$',''),'\s+', '_'))
when a.file_name ~ '^[0-9]' and ingestion_table_format != '' then concat('ingestion_table_format',regexp_replace(regexp_replace((a.file_name) ,'\..*\$',''),'\s+', '_'))
else regexp_replace(regexp_replace(a.file_name ,'\..*\$',''),'\s+', '_') end as destination_table_name
--coalesce(ingestion_table_format,'') || file_name as destination_table_name
from (
select b.*,a.*
from source_credential_details a
inner join source_details b on a.source_dataplace_id = b.dataplace_id
)a
),
file_system_credentials as (
select c.credential_id,d.dataplace_id ,c.credential_name,c.credential_type_id, credential_type,extract(epoch from c.mod_ts) as credential_epoch from nabu.dataplace_physical d left outer join nabu.credential_info c
on (((dataplace_info->\>'filesystem_info')::json)->\>'credential_id')::integer = c.credential_id
left outer join nabu.credential_type_lookup ct
on c.credential_type_id = ct.credential_type_id
where d.dataplace_id in (select destination_dataplace_id from data_movement_details)
),
dest_dataplace_details as (
-- For the destination dataplace, get it's details
select $templateData.input_data.data_movement_id$ as data_movement_id,a.dataplace_id as destination_dataplace_id,
case when  (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') then ((dataplace_info->\>'jdbc_info')::json)->\>'jdbc_url'
else dataplace_info->\>'host_name' end as destination_url,
drc.schema_name  as destination_schema_name,
case when b.dataplace_sub_component_type = 'oracle' then dataplace_info->\>'jdbc_url' end as destination_oracle_jdbc_url,
case
       when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
        and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type= 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type = 'redshift') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
       when (b.dataplace_sub_component_type = 'bigquery') then ((dataplace_info->\>'filesystem_info')::json)->\>'ingestion_root_path'
        else null
    end as path,
case
       when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')
       and dataplace_info->\>'connection_mode' = 'Cluster mode') then ((dataplace_info->\>'filesystem_info')::json)->\>'endpoint'
        else null
      end as filesystem_endpoint,
case
when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive')
and dataplace_info->\>'connection_mode' = 'Cluster mode' then lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type')
when (b.dataplace_sub_component_type  = 'azure_synapse') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
when (b.dataplace_sub_component_type  = 'redshift' and dmd.intermediate_type = 'parquet') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
when (b.dataplace_sub_component_type  = 'snowflake') then ((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type'
when (b.dataplace_sub_component_type = 'bigquery') then 'gcs'
else null
end as  file_system_type,
case when (b.dataplace_sub_component_type ='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode' and  lower(((dataplace_info->\>'filesystem_info')::json)->\>'file_system_type') = 's3a' then true else false end as is_apache_jdbc_driver,
case
when ((b.dataplace_sub_component_type = 'hive')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type  = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'credential_id'
when ((b.dataplace_sub_component_type = 'bigquery')) then cast(a.credential_id as text)
else null end as filesystem_credential_id,
case
when ((b.dataplace_sub_component_type  = 'hive')) then cast(coalesce (fs.credential_type_id,'1') as text)
when ((b.dataplace_sub_component_type = 'azure_synapse')) then cast(fs.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then cast(fs.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'snowflake')) then cast(fs.credential_type_id as text)
when ((b.dataplace_sub_component_type = 'bigquery')) then cast(ci.credential_type_id as text)
else null end as filesystem_credential_type_id,
case
when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'directory_name'
else null
end as adls_gen2_directory_name,
case
when ((b.dataplace_sub_component_type = 'redshift' and dmd.intermediate_type = 'parquet')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'bucket'
else null
end as s3_bucket,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'container'
   else null
end as adls_gen2_container,
case
   when ((b.dataplace_sub_component_type = 'bigquery')) then dataplace_info->\>'bucket_name'
   else null
end as gcs_bucket,
case
   when ((b.dataplace_sub_component_type = 'azure_synapse')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   when ((b.dataplace_sub_component_type = 'snowflake')) then ((dataplace_info->\>'filesystem_info')::json)->\>'accountName'
   else null
end as storage_account,
case when (b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive')  then ((dataplace_info->\>'jdbc_info')::json)->\>'database_name'
else dataplace_info->\>'database_name' end as destination_db,
dataplace_info->'filesystem_info'->\>'directory' as hdfs_directory,
case when  (lower(b.dataplace_sub_component_type)='sas') then dataplace_info->\>'location_path' end as destination_directory,
case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then true else false end as is_spark_hive,
lower(case when b.dataplace_sub_component_type  = 'cloudera_hive' then 'hive' else b.dataplace_sub_component_type end) as destination_type,
lower(case when ((b.dataplace_sub_component_type='hive' or b.dataplace_sub_component_type='cloudera_hive') and dataplace_info->\>'connection_mode' = 'Cluster mode') then (select coalesce ((data_movement_additional_info->\>'flow_details')::json->\>'destination_file_format','parquet') from nabu.data_movement_physical dmp where data_movement_id=$templateData.input_data.data_movement_id$) else b.dataplace_sub_component_type end) as target_type,
b.additional_info->\>'jdbc_driver' as destination_jdbc_driver,
b.dataplace_sub_component_id as destination_dataplace_sub_component_id,
a.credential_id as destination_credential_id,
b.additional_info->\>'default_credential' as dest_default_credential_exists,
(dataplace_info->\>'filesystem_info')::JSON->\>'endpoint' as destination_endpoint,
fs.credential_type as filesystem_credential_type,
fs.credential_epoch as filesystem_credential_epoch
 from nabu.dataplace_physical a left join nabu.credential_info ci on a.credential_id = ci.credential_id ,
 nabu.dataplace_sub_component_lookup b, file_system_credentials fs, nabu.dataplace_relational_component_physical drc, data_movement_details dmd
where a.dataplace_sub_component_id = b.dataplace_sub_component_id
and a.dataplace_id = fs.dataplace_id
and a.valid_to_ts = '9999-12-31'
and a.dataplace_id in (select destination_dataplace_id from data_movement_details)
and drc.schema_id in (select destination_schema_directory_id from data_movement_details ) and drc.valid_to_ts = '9999-12-31'
)
--select * from dest_dataplace_details;
,
dest_credential_details as (
select a.*,b.credential_type_id as destination_credential_type_id,
case when (a.file_system_type = 's3a') then true else false end as is_file_system_type_s3a,
case when (a.filesystem_credential_type_id = '2' or a.filesystem_credential_id is null) then true else false end as is_filesystem_credential_type_aws,
case when (b.credential_type_id = 8 or b.credential_type_id =9) then true else false end as is_destination_credential_kerberos,
ct.credential_type as destination_credential_type
from dest_dataplace_details a left outer join nabu.credential_info b
on a.destination_credential_id = b.credential_id
left join nabu.credential_type_lookup ct on b.credential_type_id = ct.credential_type_id
and b.valid_to_ts = '9999-12-31'
)
--select * from dest_credential_details;
,
datatypes_to_be_ignored as (
select replace((json_array_elements(data_movement_additional_info->'flow_details'->'ignore_data_types'))::text,'"','') as ignore_data_types
from nabu.data_movement_physical
where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
union
select data_type as ignore_data_types
from nabu.advanced_options_unsupported_mappings
where advanced_options_sub_type_id =16 and data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
),
directory_name_details as(
select b.*,directory_name from nabu.dataplace_file_system_component a,flow_files b
where a.dataplace_id=b.dataplace_id and a.valid_to_ts='9999-12-31 00:00:00'
),
intermediate_details as (
select intermediate_type from data_movement_details
)
select a.is_source_filetype,a.dataplace_id as source_dataplace_id,a.dataplace_id as dataplace_id,a.directory_id,a.file_id,a.file_format,a.has_header,a.delimiter,a.escape_char,a.quote_char,a.linesep,
a.file_relative_path,a.file_absolute_path,a.source_s3_bucket,a.source_credential_id,a.source_credential_type_id,
table_timestamp, table_suffix, h.schema_drift_flag,a.source_adls_gen2_container,a.source_storage_account,i.directory_name,
case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix)
when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',to_char(current_timestamp ,table_timestamp))
else new_table_name end as new_table_name,
case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(new_table_name,'_',table_suffix)
when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(new_table_name,'_',to_char(current_timestamp ,table_timestamp))
else new_table_name end as backup_new_table_name,
case when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'new_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
when schema_drift_option = 'new_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
else destination_table_name end as destination_table_name,
case when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix,'_' ,to_char(current_timestamp ,table_timestamp))
when schema_drift_option = 'bkp_table' and is_empty_suffix= false and table_timestamp = '' and h.schema_drift_flag = true then concat(destination_table_name,'_',table_suffix)
when schema_drift_option = 'bkp_table' and is_empty_suffix= true and table_timestamp <> '' and h.schema_drift_flag = true then concat(destination_table_name,'_',to_char(current_timestamp ,table_timestamp))
else destination_table_name end as destination_backup_table_name,project,a.file_name,source_type,
i.file_size,f.schema_name as destination_schema_name,a.destination_dataplace_id,destination_dataplace_sub_component_id,rename_table_condition,already_ingested,
 a.verification_threshold, destination_url,$templateData.input_data.process_id$ as process_id,
$templateData.input_data.batch_id$ as batch_id,
'$templateData.input_data.batch_name$' as batch_name,$templateData.input_data.data_movement_id$ as data_movement_id,
'$templateData.input_data.where_condition$' as where_condition,
path,bucket_name,project,filesystem_endpoint,
case when (id.intermediate_type in ('default')) then true else false end as is_default,
file_system_type,destination_oracle_jdbc_url,dest_default_credential_exists::boolean,
is_spark_hive, destination_db,destination_directory, destination_type,target_type, destination_file_format,destination_jdbc_driver,$templateData.input_data.flow_number$ as flow_number,
destination_credential_id, destination_credential_type_id,is_destination_credential_kerberos,is_apache_jdbc_driver,ingestion_table_format,
case when schema_drift_option = 'bkp_table' and h.schema_drift_flag = true
then true END as create_backup_table, schema_drift_option,
e.filesystem_credential_id,e.filesystem_credential_type_id,e.filesystem_credential_type,e.is_filesystem_credential_type_aws,e.is_file_system_type_s3a,
e.adls_gen2_directory_name,e.adls_gen2_container,e.storage_account,e.s3_bucket,e.gcs_bucket,
'$templateData.input_data.jwt_token$' as jwt_token, '$templateData.input_data.end_point$' as end_point,
case when source_endpoint is null then 'null' else source_endpoint end as source_endpoint,
case when destination_endpoint is null then 'null' else destination_endpoint end as destination_endpoint,
case when (e.filesystem_credential_type = 'Azure Active Directory' or destination_credential_type ='Azure Active Directory') then true else false end as is_azure_oauth,
case when (e.filesystem_credential_type = 'AWS IAM Role' or destination_credential_type ='AWS IAM Role') then true else false end as is_aws_iam,
case when (source_credential_type ='Azure Active Directory') then true else false end as is_source_azure_oauth,
e.filesystem_credential_epoch,id.intermediate_type,a.is_create_table
from source_level_details a,  dest_credential_details e, nabu.dataplace_relational_component_physical f,
flag_for_schema_drifted_files h,directory_name_details i,intermediate_details id
where a.file_id = i.file_id
and a.dataplace_id = i.dataplace_id
and a.directory_id = i.directory_id
and a.destination_dataplace_id = e.destination_dataplace_id
and a.data_movement_id = $templateData.input_data.data_movement_id$
and f.dataplace_id = e.destination_dataplace_id
and a.destination_schema_directory_id = f.schema_id
and f.valid_to_ts = '9999-12-31'
and a.file_id = h.file_id
>>

command_to_run_spark_almaren_job(templateData)::=<<
["bash","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/curation/spark-almaren.sh $templateData.ssh_username$ $templateData.ssh_host$ $templateData.project_name$ $templateData.git_url$ $templateData.git_branch_or_tag$ $templateData.git_file_path$ $templateData.spark_command$ $templateData.process_id$ $templateData.engine_path$ $templateData.retry_attempt$ $templateData.spark_location$ $templateData.compute_engine_info$ $templateData.job_name$"]
>>

command_to_run_cleanup_job(templateData)::=<<
["sh","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/curation/run_cleanup.sh $templateData.ssh_username$ $templateData.ssh_host$ $templateData.project_name$ $templateData.engine_path$ $templateData.process_id$ $templateData.spark_location$ $templateData.compute_engine_info$"]
>>

command_to_run_almaren_ingestion(templateData)::=<<
["sh","-l","-c","\$NABU_ROOT_INSTALLATION/botworks/scripts/ingestion/run_almaren_job.sh $templateData.data.templateData.query_input.encrypted_script$ $templateData.data.templateData.query_input.base64_rsa_compressed_encrypted_creds$ $templateData.data.templateData.query_input.input_json_for_checkpoint_status$ $templateData.retry_attempt$ $templateData.ssh_host$ $templateData.ssh_username$ $templateData.engine_path$ $templateData.spark_location$ $templateData.compute_engine_info$ $templateData.source_endpoint$ $templateData.destination_endpoint$"]
>>

generate_ddl_and_drop_backup_table_if_exists(templateData)::=<<
drop table if exists $templateData.destination_schema_name$.`$templateData.destination_backup_table_name$`
>>

generate_ddl_and_create_backup_table(templateData)::=<<
create table $templateData.destination_schema_name$.`$templateData.destination_backup_table_name$` as select * from $templateData.destination_schema_name$.`$templateData.destination_table_name$`
>>

generate_ddl_and_drop_backup_table_if_exists_redshift(templateData)::=<<
drop table if exists $templateData.destination_backup_table_name$
>>

generate_ddl_and_create_backup_table_redshift(templateData)::=<<
create table $templateData.destination_backup_table_name$ as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$
>>

drop_hive_view(templateData)::=<<
drop view $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).view_name$`
>>

create_hive_view_on_valid_data(templateData)::=<<
create view if not exists $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).view_name$` as
select $templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:generate_view_columns();separator = ","$
from
$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
where $first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).end_date_column_name$='9999-12-31 00:00:00.000000'
>>

drop_redshift_view(templateData)::=<<
drop view if exists $first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).view_name$
>>

create_redshift_view_on_valid_data(templateData)::=<<
create or replace view $first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).view_name$ as
(select $templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:generate_redshift_view_columns();separator = ","$
from
$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
where $first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).end_date_column_name$='9999-12-31 00:00:00.000000'
)
with no schema binding;
>>

drop_azure_synapse_view(templateData)::=<<
drop view $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).view_name$`
>>

create_azure_synapse_view_on_valid_data(templateData)::=<<
create view if not exists $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).view_name$` as
select $templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:generate_view_columns();separator = ","$
from
$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
where $first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).end_date_column_name$='9999-12-31 00:00:00.000000'
>>

generate_view_columns(templateData)::=<<
`$templateData.field_name$`
>>

generate_redshift_view_columns(templateData)::=<<
$templateData.field_name$
>>

generate_ddl_drop_and_create_backup_table(templateData)::=<<
IF EXISTS(select t.name, s.name from sys.tables as t inner join sys.schemas as s
on t.schema_id = s.schema_id
where s.name='$templateData.destination_schema_name$' and t.name='$templateData.destination_backup_table_name$')
BEGIN
drop table $templateData.destination_schema_name$.$templateData.destination_backup_table_name$;
create table $templateData.destination_schema_name$.$templateData.destination_backup_table_name$ with(HEAP, DISTRIBUTION = ROUND_ROBIN)
as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$;
END
ELSE
BEGIN
create table $templateData.destination_schema_name$.$templateData.destination_backup_table_name$ with(HEAP, DISTRIBUTION = ROUND_ROBIN)
as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$;
END
>>

insert_spark_job_result(templateData)::=<<
INSERT INTO nabu.spark_job_result
(process_id, application_id, error_details, status,exit_status)
VALUES($templateData.query_input.process_id$,
$if(templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.applicationId)$'$templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.applicationId$'$elseif(templateData.query_input.data.data.botLogicOutputMap.applicationId)$'$templateData.query_input.data.data.botLogicOutputMap.applicationId$'$else$null$endif$,
$if(templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.error_details)$'$templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.error_details$'$elseif(templateData.query_input.data.data.botLogicOutputMap.error_details)$'$templateData.query_input.data.data.botLogicOutputMap.error_details$'$else$null$endif$,
$if(templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.status)$'$templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.status$'$elseif(templateData.query_input.data.data.botLogicOutputMap.status)$'$templateData.query_input.data.data.botLogicOutputMap.status$'$else$null$endif$,
$if(templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.exit_status)$'$templateData.query_input.data.query_input.metadata.data.botLogicOutputMap.exit_status$'$elseif(templateData.query_input.data.data.botLogicOutputMap.exit_status)$'$templateData.query_input.data.data.botLogicOutputMap.exit_status$'$else$null$endif$
);
>>

insert_into_spark_job_result(templateData)::=<<
INSERT INTO nabu.spark_job_result
(process_id, application_id, error_details, status,exit_status)
VALUES($templateData.input_data.process_id$,$if(templateData.input_data.applicationId)$'$templateData.input_data.applicationId$'$else$null$endif$,$if(templateData.input_data.error_details)$'$templateData.input_data.error_details$'$else$null$endif$, $templateData.input_data.status$,
$templateData.input_data.exit_status$);
>>

insert_curation_spark_script_bot_status(templateData)::=<<
INSERT INTO nabu.checkpoint_status
( data_movement_id, process_id, status, application_id, job_type, hostname,  start_time,valid_from_ts, valid_to_ts,table_id)
VALUES( $templateData.data.input_data.data_movement_id$,$templateData.process_id$,'$templateData.status$','$templateData.applicationId$','nabu-curation-job','$templateData.ssh_host$',now(),now(),'9999-12-31',$templateData.data.input_data.where_condition$);

>>

update_curation_spark_script_bot_status(templateData)::=<<
update nabu.checkpoint_status
set
status='$templateData.status$',end_time=now()
where data_movement_id=$templateData.data.input_data.data_movement_id$
and process_id=$templateData.process_id$

>>

update_checkpoint_after_latest_status_insert(templateData)::=<<
update nabu.checkpoint_status set valid_to_ts=now()
where data_movement_id=$templateData.data.input_data.data_movement_id$
and valid_to_ts='9999-12-31' and
job_type='nabu-curation-job' and
data_movement_id=$templateData.data.input_data.data_movement_id$ and
application_id <> '$templateData.applicationId$'
>>

insert_spark_application_details(templateData)::=<<
insert into nabu.spark_application_details
(application_id, process_id, additional_info, cru_ts)
values($if(templateData.applicationId)$'$templateData.applicationId$'$else$null$endif$,$templateData.process_id$,'{"job_id":$if(templateData.jobId)$"$templateData.jobId$"$else$ null $endif$ ,"job_name":$if(templateData.jobName)$"$templateData.jobName$"$else$ null $endif$ ,"run_id":$if(templateData.runId)$"$templateData.runId$"$else$ null $endif$}',now())
>>

number_of_tables_per_flow(templateData)::=<<
1
>>

replacing_special_characters(table_or_column_name)::=<<
regexp_replace(lower(trim(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace($table_or_column_name$,'\$','_dlr_' ),'"','') ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'!','_exp_'))),'^[_]', '')
>>

replacing_special_characters_file(file_name)::=<<
regexp_replace(lower(trim(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(replace(a.file_name,'\$','_dlr_' ) ,'|','_pe_' ),'}','_rbr_' ),'{','_lbr_' ),'#','_hash_' ) ,'*','_astk_' ) ,'(','_op_' ) , ')','_cp_' ) ,'@','_at_' ),'.','_dot_' ),' ','_' ),'&','_amp_' ),'+','_pls_' ),'-','_hyp_' ),'?','_qtn_' ),':','_cln_' ),'/','_fs_' ),'''','_sq_' ),'%','_prst_' ),'!','_exp_'))),'^[_]', '')
>>

fetch_object_column_metadata(templateData)::=<<
$([first(templateData.fetch_source_type).source_metadata_category,"_column_metadata"])(templateData)$
>>

file_column_metadata(templateData)::=<<
with datatype_mapping as (
  	select
  		lower(source_type) as source_type ,
  		lower(intermediate_type) as intermediate_type,
  		lower(destination_type) as destination_type ,
  		lower(source_datatype_name) as source_datatype_name ,
  		source_stg_function ,intermediate_stg_function ,destination_datatype_name,
  		lower(datamovement_engine_type) as datamovement_engine_type
  	from nabu.datatype_mapping where lower(source_type) =lower('$first(templateData.table_metadata_output).file_format$') and lower(destination_type)=lower('$first(templateData.fetch_source_type).destination$') and
  	lower(intermediate_type) = lower('$first(templateData.table_metadata_output).intermediate_type$')
 ),
  column_metadata as (
  	select
  		column_name,data_length,ordinal_position,data_scale,data_precision,
  		is_nullable,column_id,file_id ,file_name,
  		$replacing_special_characters("column_name")$ as field_name,
  		lower(data_type) as data_type
  	from nabu.dataplace_semi_structured_column_metadata where file_id = $first(templateData.table_metadata_output).file_id$ AND valid_to_ts='9999-12-31'
  )
  select
  	replace(a.column_name,'"','') as column_name,a.data_type,a.data_length,a.ordinal_position,a.is_nullable,file_name,
  	source_type,b.source_datatype_name,source_stg_function,destination_datatype_name,
  	a.data_scale,a.data_precision,
  	case
        		when ( intermediate_stg_function = 'AsIs') then 'parquet_AsIs'
             	else intermediate_stg_function
        end as intermediate_stg_function,
  	a.field_name,
      	case when ( 'oracle' = lower('$first(templateData.fetch_source_type).source$')) and
          length(a.field_name)>30
          then
          'c_'||a.column_id
          else
          a.field_name
          end
          as intermediate_field_name
  from column_metadata as a left join datatype_mapping as b
  on a.data_type = b.source_datatype_name
  where a.ordinal_position is not null
 -- and b.datamovement_engine_type = lower('$first(templateData.table_metadata_output).datamovement_engine_type$')
  order by ordinal_position
>>

relational_column_metadata(templateData)::=<<
with datatype_mapping as (
  	$if(first(templateData.table_metadata_output).is_create_table)$
  	select
  		lower(source_type) as source_type ,
  		lower(intermediate_type) as intermediate_type ,
  		$first(templateData.table_metadata_output).is_create_table$ as is_create_table,
  		lower(destination_type) as destination_type ,
  		lower(source_datatype_name) as source_datatype_name ,
  		intermediate_datatype_name , destination_datatype_name ,source_cast_type,
  		intermediate_cast_type ,source_stg_function ,intermediate_stg_function ,
  		lower(datamovement_engine_type) as datamovement_engine_type
  	from nabu.datatype_mapping where lower(source_type) =lower('$first(templateData.fetch_source_type).source$') and lower(destination_type)=lower('$first(templateData.fetch_source_type).destination$') and
  	lower(intermediate_type) = lower('$first(templateData.table_metadata_output).intermediate_type$')

  	$else$
  		select
      		lower(source_type) as source_type ,
      		lower(intermediate_type) as intermediate_type ,
  		    $first(templateData.table_metadata_output).is_create_table$ as is_create_table,
      		lower(destination_type) as destination_type ,
      		lower(source_datatype_name) as source_datatype_name ,
      		intermediate_datatype_name ,source_cast_type,
      		intermediate_cast_type ,source_stg_function ,intermediate_stg_function ,
      		lower(datamovement_engine_type) as datamovement_engine_type
      	from(
         select *,row_number() over (partition by source_type ,source_datatype_name ) as rno
         from nabu.datatype_mapping dmt
      	 where lower(source_type) =lower('$first(templateData.fetch_source_type).source$') and
      	 lower(intermediate_type) = lower('$first(templateData.table_metadata_output).intermediate_type$')
      	 )a
      	 where rno=1
    $endif$

  ),
 obj_details as(
      select advance_option_details from nabu.advanced_options_object_details where data_movement_id= $templateData.input_data.data_movement_id$ and valid_to_ts ='9999-12-31' and object_id in ($templateData.input_data.where_condition$)
 ),
  exculde_columns as(
    select unnest(string_to_array(btrim((column_options->\>'excluded_column_ids'),'[||]'),','))::int as excluded_column_ids from
    ( select (advance_option_details->'advanced_table_options'->'columns_options')::JSON as column_options from obj_details )a
    ),
  ignore_datatypes as(
       select coalesce (lower(ignore_data_types),' ') as data_types from (
       select replace((json_array_elements(data_movement_additional_info->'flow_details'->'ignore_data_types'))::text,'"','') as ignore_data_types
       from nabu.data_movement_physical
       where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
       union
      select source_datatype_name as ignore_data_types
      from nabu.user_defined_trans_for_unsupported_datatype
      where advanced_options_sub_type_id =16 and data_movement_id = $templateData.input_data.data_movement_id$ and valid_to_ts='9999-12-31'
       )a
  ),
  column_metadata as (
  select a.*,
  		case when cdc.column_id is not null then true else false end as is_cdc_col,
          b.advanced_options_sub_type_id,
          b.additional_info
   from
    (
  	select
  		cmp.column_name,data_length,cmp.ordinal_position,
  		case when data_scale is null then 0
  		    else data_scale
  		end as data_scale,data_precision,
  		data_type as metadata_datatype,
  		cmp.is_nullable,cmp.column_id,cmp.table_id,cmp.table_name,cmp.schema_name,
  		case
             when ('oracle' = lower('$first(templateData.fetch_source_type).source$')) and data_type ~* 'time zone' and length($replacing_special_characters("column_name")$ || '_utc') > 30
             then 'c_'||column_id
             when ('oracle' = lower('$first(templateData.fetch_source_type).source$')) and length($replacing_special_characters("column_name")$) > 30
             then 'c_'||column_id
          	 else $replacing_special_characters("column_name")$ end as field_name,
  		case
  			when data_type ~* 'INTERVAL' then regexp_replace(lower(data_type),'[^A-Za-z ]', '', 'g')
  			when data_type like 'array%' and dp.dataplace_sub_component_id =7  then regexp_replace(lower(data_type), '^(\w+)\W.*','\1','g')
            when data_type like 'struct%' and dp.dataplace_sub_component_id =7 then regexp_replace(lower(data_type), '^(\w+)\W.*','\1','g')
            when data_type like 'map%' and dp.dataplace_sub_component_id =7 then regexp_replace(lower(data_type), '^(\w+)\W.*','\1','g')
            when data_type like 'varchar%' and dp.dataplace_sub_component_id =7 then 'varchar'
  			else lower(data_type)
  		end as data_type
  	from nabu.dataplace_column_metadata_physical cmp left outer join nabu.dataplace_physical dp on cmp.dataplace_id = dp.dataplace_id where cmp.table_id = $templateData.input_data.where_condition$  and cmp.valid_to_ts='9999-12-31'
  and $if(first(templateData.table_adv_opt_details).object_id)$
     data_type is not null
   $else$
  lower(data_type) not in (select lower(data_types) from ignore_datatypes)
   $endif$
    and cmp.column_id not in (select excluded_column_ids from exculde_columns)
    ) a
    left outer join
         (
         select data_movement_id,
                 lower(source_datatype_name) AS source_datatype,
                 advanced_options_sub_type_id,
                 additional_info
          from nabu.user_defined_trans_for_unsupported_datatype
          where data_movement_id = $templateData.input_data.data_movement_id$
            and valid_to_ts='9999-12-31'
        ) b on a.data_type = b.source_datatype
        left outer join(
        select json_array_elements_text(advance_option_details->'advanced_table_options'->'CDC'->'columns_ids')::int as column_id
		from nabu.advanced_options_object_details aood where data_movement_id =$templateData.input_data.data_movement_id$ and object_id =$templateData.input_data.where_condition$ and valid_to_ts ='9999-12-31'
        )cdc
        on a.column_id=cdc.column_id
   ),
   source_level_casting as(
      select (modified_columns ->\>'column_id')::int as column_id,
      trim(convert_from(decode(modified_columns ->'column_info'->\>'source_casting_exp','base64'), 'UTF8')) as sql_expr,
      lower(modified_columns ->'column_info'->\>'destination_column_type') as destination_column_type,
      (modified_columns ->'column_info'->\>'destination_column_name') as destination_column_name
      from(
          select
          json_array_elements((advance_option_details->'advanced_table_options'->'columns_options'-> 'modified_columns')) :: json as modified_columns
                from obj_details )modified_details
         ),
  initial_column_mapping as (
  select
  	a.column_name,a.data_type,a.data_length,a.ordinal_position,a.is_nullable,table_name,schema_name , a.metadata_datatype, a.column_id,a.table_id,
  	source_type,destination_type,intermediate_type,datamovement_engine_type ,b.source_datatype_name,source_cast_type,b.intermediate_datatype_name,
  	$([first(templateData.fetch_source_type).source,"_",first(templateData.table_metadata_output).intermediate_type,"_",first(templateData.fetch_source_type).destination,"_source_stg_function"])()$ as source_stg_function,
  	case when (a.advanced_options_sub_type_id = 17) then a.additional_info ->\> 'target_value' else null end as user_text,
  	a.data_scale,a.data_precision,
  	$if(first(templateData.table_metadata_output).is_create_table)$
  	$([first(templateData.fetch_source_type).source,"_",first(templateData.table_metadata_output).intermediate_type,"_",first(templateData.fetch_source_type).destination,"_destination_datatype_name"])()$ as destination_datatype_name,
  	$([first(templateData.fetch_source_type).source,"_",first(templateData.table_metadata_output).intermediate_type,"_",first(templateData.fetch_source_type).destination,"_intermediate_stg_function"])()$ as intermediate_stg_function,
	$([first(templateData.fetch_source_type).source,"_",first(templateData.table_metadata_output).intermediate_type,"_",first(templateData.fetch_source_type).destination,"_inconsistent_datatype"])()$ as inconsistent_datatype,
  	$else$$endif$
  	a.field_name,
  	case when ( 'oracle' = lower('$first(templateData.fetch_source_type).source$')) and
              length(a.field_name)>30
              then
              'c_'||a.column_id
              else
              a.field_name
            end as intermediate_field_name, a.is_cdc_col
  from column_metadata as a left join datatype_mapping as b
  on a.data_type = b.source_datatype_name
  where a.ordinal_position is not null
 -- and b.datamovement_engine_type = lower('$first(templateData.table_metadata_output).datamovement_engine_type$')
  ),
  partition_columns as(
  select
  case when(columns_list->\>'renamed_column' = '')
          then columns_list->\>'column_name'
          else columns_list->\>'renamed_column'
  end as final_column_name,
  (columns_list->\>'column_id')::int as column_id
  from(
  select json_array_elements(((advance_option_details->\>'advanced_table_options')::json->\>'table_partitions')::json) as columns_list from nabu.advanced_options_object_details aood
  where data_movement_id= $templateData.input_data.data_movement_id$ and valid_to_ts ='9999-12-31' and object_id in ($templateData.input_data.where_condition$)
  )table_partitions_data
  )
  select false::boolean as is_virtual,a.column_id,
    case when slc.column_id is not null then true else false end as is_source_casting_enabled ,
    case when pc.column_id is not null then true else false end as is_partitioned_col,
    a.column_name,a.data_type,a.data_length,a.ordinal_position,a.is_nullable,table_name,schema_name ,$first(templateData.table_metadata_output).is_mysql$ as is_mysql,$first(templateData.table_metadata_output).is_source_hive$ as is_source_hive,
	a.source_type,a.source_datatype_name,source_cast_type,a.intermediate_datatype_name,a.table_id,
    case when slc.column_id is not null then
    (
    case
            when slc.sql_expr like '%)____%' then slc.sql_expr
            else concat (slc.sql_expr,' as ',slc.destination_column_name)
        end
    )
    else coalesce(c.source_stg_function,a.source_stg_function)
    end as source_stg_function,
	a.data_scale,
	case when slc.column_id is not null then
		(
		case when a.data_precision <= 38 then a.data_precision
			else 38
		end
		)
		else a.data_precision
	end as data_precision,
	$if(first(templateData.table_metadata_output).is_create_table)$
	case  when slc.column_id is not null then
            (
            case
	        when slc.destination_column_type in('decimal','numeric') then
	        (
	        case when a.data_precision <=38 then concat(slc.destination_column_type,'(',a.data_precision,',',a.data_scale,')')
	        	else concat(slc.destination_column_type,'(38,',a.data_scale,')')
	        end
	        )
            when slc.destination_column_type in ('varchar','char') then
            (
            case
	            when data_length is null then concat( slc.destination_column_type,'(255)')
            	else concat( slc.destination_column_type,'(',a.data_length ,')')
            end
            )
            else (slc.destination_column_type)
            end
            )
            else coalesce(c.destination_datatype_name,a.destination_datatype_name)
            end as destination_datatype_name,
    case when slc.column_id is not null then
            (
    		case when slc.destination_column_type in('double','double precision') then 'ParquetStringToDoubleConvert'
    			when slc.destination_column_type in('decimal','money','numeric','smallmoney','number') then 'ParquetStringToDecimalConvert'
    			when slc.destination_column_type in('int','integer','int64') then 'ParquetStringToIntConvert'
    			when slc.destination_column_type in('timestamp','datetime2') then 'ParquetStringToTimestampConvert'
    			when slc.destination_column_type = 'bigint' then 'ParquetStringToBigIntConvert'
    			when slc.destination_column_type = 'tinyint' then 'ParquetStringToTinyIntConvert'
    			when slc.destination_column_type = 'smallint' then 'ParquetStringToSmallIntConvert'
    			when slc.destination_column_type in('float','float64','real') then 'ParquetStringToFloatConvert'
    			when slc.destination_column_type in('char','nchar') then 'ParquetStringToCharConvert'
    			when slc.destination_column_type in('varchar','nvarchar') then 'ParquetStringToVarcharConvert'
    			when slc.destination_column_type in('binary','varbinary','bytes') then 'ParquetStringToBinaryConvert'
    			when slc.destination_column_type = 'date' then 'ParquetStringToDateConvert'
    			when slc.destination_column_type in('string','text') then 'AnyToString'
    			when slc.destination_column_type in('boolean','bool') then 'AnyToBoolean'
    			when slc.destination_column_type = 'array' then 'AnyToArray'
    			when slc.destination_column_type = 'map' then 'AnyToMap'
    			when slc.destination_column_type = 'struct' then 'AnyToStruct'
                else coalesce(c.intermediate_stg_function,a.intermediate_stg_function)
                end
            )
            when coalesce(c.intermediate_stg_function,a.intermediate_stg_function) = 'AsIs' then lower('parquet')||'_AsIs'
            else coalesce(c.intermediate_stg_function,a.intermediate_stg_function)
            end as intermediate_stg_function,
        	a.inconsistent_datatype,
	$else$$endif$
	--renaming of columns options logic
	case
	    when((true = $first(templateData.advanced_options_tables_flags_status).modified_columns$) and (true =$first(templateData.advanced_options_tables_flags_status).columns_options$)  and a.column_id in (select column_id from source_level_casting) )
        then (select slc.destination_column_name from source_level_casting slc where slc.column_id = a.column_id)
	    when pc.column_id is not null then pc.final_column_name
	    else a.field_name
	end as field_name,
	case
		when ('$first(templateData.fetch_source_type).source$' = 'oracle') then
        (
            case
                when data_type ~* 'time zone' and c.option_number = 0 then true
                else false
            end
        )
		when ('$first(templateData.fetch_source_type).source$' = 'postgres') then
        (
        	case
                when data_type ~* 'timestamp with time zone' and c.option_number = 0 then true
                else false
            end
        )
        when ('$first(templateData.fetch_source_type).source$' = 'sql_server') then
        (
        	case
                when data_type = 'datetimeoffset' and c.option_number = 0 then true
                else false
            end
        )
        else false
	end as isTimeZone , user_text, a.is_cdc_col
  from initial_column_mapping a left outer join (select a.*,lower(b.engine_type) as datamovement_engine_type from nabu.user_defined_trans_for_inconsistent_datatype a inner join nabu.engine_lookup b on a.engine_id = b.engine_id where data_movement_id = $templateData.input_data.data_movement_id$ and valid_to='9999-12-31' ) c
  on a.source_type = c.source_type
  and a.intermediate_type = c.intermediate_type
  and a.destination_type = c.destination_type
  and a.datamovement_engine_type = c.datamovement_engine_type
  $if(first(templateData.table_metadata_output).is_create_table)$
  and a.inconsistent_datatype = c.inconsistent_datatype
  $else$$endif$
  left outer join partition_columns pc on a.column_id = pc.column_id
  left outer join source_level_casting slc on a.column_id = slc.column_id and slc.sql_expr !=''
  order by ordinal_position
>>

fetch_destination_transformation_query_details(templateData)::=<<
$(["fetch_destination_transformation_query_details_",first(templateData.fetch_source_type).source_metadata_category])(templateData)$
>>

fetch_destination_transformation_query_details_relational(templateData)::=<<
with obj_details as(
              select advance_option_details from nabu.advanced_options_object_details where data_movement_id= $templateData.input_data.data_movement_id$ and valid_to_ts ='9999-12-31' and object_id in ($templateData.input_data.where_condition$)
 ),exculde_columns as(
   select unnest(string_to_array(btrim((column_options->\>'excluded_column_ids'),'[||]'),','))::int as excluded_column_ids from
   ( select (advance_option_details->'advanced_table_options'->'columns_options')::JSON as column_options from obj_details )a
),
reorder_columns as (
    select * from (
        select reorder_json->\>'column_name' as column_name,cast(reorder_json->\>'column_id' as bigint) as column_id,
        cast(reorder_json->\>'is_virtual' as boolean) as is_virtual,cast(reorder_json->\>'order' as int) as reorder_position
      from (
      select json_array_elements((advance_option_details->'advanced_table_options'->'reorder_columns'))::json as reorder_json from obj_details)aa)RT
        where column_name not in ((select column_name from nabu.dataplace_table_metadata_physical dtmp where column_id in (select * from exculde_columns) ) )
  ) ,
     virtual_columns as(
       select column_name,true as is_virtual,bb.virtual_intermediate_stg_function,bb.virtual_destination_datatype_name ,false as is_partitioned_col from
               (
                   ( select * from
                     (	 SELECT (jsonarray ->\>'column_name')::text as column_name,(jsonarray ->\>'column_type')::text as column_type,(jsonarray ->\>'spark_sql_expression')::text as spark_sql_expression
                     	 from (  select json_array_elements((advance_option_details->'advanced_table_options'->'columns_options'-> 'virtual_column')) :: json as jsonarray from obj_details )x
                     )y
                   )z	left join (select datatype_name,spark_sql_datatype from nabu.spark_datatypes)d on d.datatype_name = z.column_type
               )aa
               inner join ( select intermediate_datatype_name,intermediate_stg_function as virtual_intermediate_stg_function , destination_datatype_name as virtual_destination_datatype_name from nabu.datatype_mapping
               where source_type= 'virtual' and lower(destination_type)=lower('$first(templateData.fetch_source_type).destination$') and
               lower(intermediate_type) = lower('$first(templateData.table_metadata_output).intermediate_type$') )bb  on bb.intermediate_datatype_name = aa.spark_sql_datatype
  )
  $if(first(templateData.advanced_options_tables_flags_status).reorder_columns && !first(templateData.fetch_source_type).is_destination_filetype)$
  select rc.column_name,rc.column_id,a.data_type,a.data_length,is_mysql,is_source_hive,a.source_type,a.source_datatype_name,a.is_nullable,a.schema_name,a.table_name,
    a.is_source_casting_enabled, a.is_partitioned_col,a.intermediate_datatype_name,a.source_stg_function,a.data_scale,a.data_precision,a.table_id,
    a.inconsistent_datatype,a.ordinal_position,a.istimezone,a.user_text,rc.is_virtual,rc.reorder_position,
    row_number() over (order by rc.reorder_position) as ingested_colorder
        $if(first(templateData.advanced_options_tables_flags_status).virtual_column)$
              ,case when rc.is_virtual = true then rc.column_name else a.field_name end as field_name
              ,case when rc.is_virtual = true then vc.virtual_intermediate_stg_function else a.intermediate_stg_function end as intermediate_stg_function,
              case when rc.is_virtual = true then vc.virtual_destination_datatype_name else a.destination_datatype_name end as destination_datatype_name
        $else$ ,a.field_name,a.intermediate_stg_function,a.destination_datatype_name $endif$
  $else$
    select *, row_number() over (order by ordinal_position) as ingested_colorder
  $endif$
  from
  --calling the column metadata query here
  ( $fetch_object_column_metadata(templateData)$)a
  $if(first(templateData.advanced_options_tables_flags_status).reorder_columns )$
       right join reorder_columns rc on  a.column_name = rc.column_name
       $if(first(templateData.advanced_options_tables_flags_status).virtual_column)$
             left join virtual_columns vc on vc.is_virtual = rc.is_virtual and vc.column_name = rc.column_name
        $endif$
       order by rc.reorder_position
  $endif$
>>

fetch_destination_transformation_query_details_file(templateData)::=<<
select null ;
>>

bigquery_default_postgres_intermediate_stg_function()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
   else b.intermediate_stg_function
end
>>

bigquery_default_postgres_destination_datatype_name()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'text'
   else b.destination_datatype_name
end
>>

bigquery_default_postgres_inconsistent_datatype() ::=<<
null
>>

bigquery_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

bigquery_default_oracle_intermediate_stg_function()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
   else b.intermediate_stg_function
end
>>

bigquery_default_oracle_destination_datatype_name()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
   else b.destination_datatype_name
end
>>

bigquery_default_oracle_inconsistent_datatype() ::=<<
null
>>

bigquery_default_oracle_source_stg_function()::=<<
$get_source_stg_function()$
>>

bigquery_default_redshift_relational_intermediate_stg_function()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
   else b.intermediate_stg_function
end
>>

bigquery_default_redshift_relational_destination_datatype_name()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'varchar(max)'
   else b.destination_datatype_name
end
>>

bigquery_default_redshift_relational_inconsistent_datatype() ::=<<
null
>>

bigquery_default_redshift_relational_source_stg_function()::=<<
$get_source_stg_function()$
>>

bigquery_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>

bigquery_parquet_hive_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

bigquery_parquet_hive_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

bigquery_parquet_hive_inconsistent_datatype() ::=<<
null
>>

bigquery_parquet_bigquery_intermediate_stg_function()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
   else b.intermediate_stg_function
end
>>

bigquery_parquet_bigquery_destination_datatype_name()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
   else b.destination_datatype_name
end
>>

bigquery_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>

bigquery_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

bigquery_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

bigquery_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'varchar(max)'
	else b.destination_datatype_name
END
>>

bigquery_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

bigquery_parquet_azure_synapse_inconsistent_datatype() ::=<<
null
>>

bigquery_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

bigquery_parquet_redshift_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'varchar(max)'
	else b.destination_datatype_name
END
>>

bigquery_parquet_redshift_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

bigquery_parquet_redshift_inconsistent_datatype() ::=<<
null
>>

hive_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name = 'date') then 'date_to_string'
	else null
end
>>

oracle_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name in ('number','decimal')) then
	(
		case
			when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
			else 'number_to_string'
		end
	)
	when (b.source_datatype_name = 'float') then
	(
		case
			when (a.data_precision <=53) then null
			else 'float_to_string'
		end
	)
	when (b.source_datatype_name ~* 'INTERVAL') then 'interval_to_string'
	when (b.source_datatype_name in ('rowid','urowid','xmltype','float'))then concat(b.source_datatype_name,'_to_string')
	when (b.source_datatype_name ~* 'LOCAL TIME ZONE') then
	(
		case
			when (a.data_scale <= 6) then 'timestamp_with_local_time_zone_to_timestamp'
			else 'timestamp_with_local_time_zone_to_string'
		end
	)
	when (b.source_datatype_name ~* 'TIME ZONE') then
	(
		case
			when (a.data_scale <= 6) then 'timestamp_with_time_zone_to_timestamp'
			else 'timestamp_with_time_zone_to_string'
		end
	)
	when (b.source_datatype_name ~* 'TIMESTAMP') then
	(
		case
			when (a.data_scale <= 6) then null
			else 'timestamp_to_string'
		end
	)
	else null
end
>>

oracle_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
    when (data_type in ('number','decimal')) then
    (
        case
            when (data_scale <= 37 and (data_precision <= 38)) then null
            else 'number_to_varchar'
        end
    )
    when (data_type ~* 'interval year') then 'interval_year_to_month_to_varchar'
    when (data_type ~* 'interval day') then 'interval_day_to_second_to_varchar'
    when (data_type in ('rowid','urowid','xmltype','binary_float','binary_double','date'))then concat(data_type,'_to_varchar')
    when (data_type = 'float') then
    (
        case
            when (data_precision <=53) then null
            else 'float_to_varchar'
        end
    )
    when (data_type ~* 'local time zone') then 'timestamp_with_local_time_zone_to_varchar'
    when (data_type ~* 'time zone') then 'timestamp_with_time_zone_to_varchar'
    when (data_type ~* 'timestamp') then 'timestamp_to_varchar'
    else null
end
>>

sql_server_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name in ('time','date','bit')) then concat(b.source_datatype_name,'_to_string')
	when (b.source_datatype_name in ('datetime2')) then
	(
		case
			when (a.data_precision <= 6) then null
			else 'datetime2_to_string'
		end
	)
	when (b.source_datatype_name = 'datetimeoffset') then
	(
		case
			when (a.data_precision <= 6) then 'datetimeoffset_to_timestamp'
			else 'datetimeoffset_to_string'
		end
	)
	else null
end
>>

sql_server_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
	when (data_type in ('time','date','bit','datetime2','datetimeoffset','datetime','smalldatetime')) then concat(data_type,'_to_varchar')
    else null
end
>>

postgres_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name = 'numeric') then
    (
		case
			when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'numeric_to_string'
        end
    )
    when (b.source_datatype_name = 'bit varying') then 'bit_varying_to_string'
    when (b.source_datatype_name = 'time without time zone') then 'time_without_time_zone_to_string'
    when (b.source_datatype_name = 'time with time zone') then 'time_with_time_zone_to_string'
    when (b.source_datatype_name = 'timestamp with time zone') then 'timestamp_with_time_zone_to_string'
    when (b.source_datatype_name in ('money','bit','date')) then concat(b.source_datatype_name,'_to_string')
    else null
end
>>

postgres_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
	when (data_type = 'numeric') then
    (
		case
            when (data_scale <= 37 and (data_precision <= 38)) then null
            else 'numeric_to_varchar'
        end
    )
	when (data_type in ('bit varying','bit')) then 'bit_to_varchar'
    when (data_type = 'time without time zone') then 'time_without_time_zone_to_varchar'
    when (data_type = 'time with time zone') then 'time_with_time_zone_to_varchar'
    when (data_type = 'timestamp with time zone') then 'timestamp_with_time_zone_to_varchar'
    when (data_type = 'timestamp without time zone') then 'timestamp_without_time_zone_to_varchar'
    when (data_type in ('money','date','boolean','abstime','real')) then concat(data_type,'_to_varchar')
	when (data_type = 'double precision') then 'double_to_varchar'
    else null
end
>>

mysql_parquet_hive_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name = 'decimal') then
    (
        case
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'decimal_to_string'
        end
    )
    when (b.source_datatype_name in ('year','time','date','bit'))then concat(b.source_datatype_name,'_to_string')
    else null
end
>>

mysql_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
    when (data_type in ('decimal')) then
    (
    	case
            when (data_scale <= 37 and (data_precision <= 38)) then null
            else 'decimal_to_varchar'
        end
    )
    when (data_type in ('year','date','bit','time','timestamp','datetime')) then concat(data_type,'_to_varchar')
    else null
end
>>

oracle_parquet_hive_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'number') then
	(
		CASE
			when (data_scale=0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
            when (data_scale=0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 18 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
        END
	)
	when (b.source_datatype_name = 'float') then
    (
  	    CASE
   	   	  	when (a.data_precision between 0 and 23) then 'ParquetStringToFloatConvert'
   	   	  	when (a.data_precision between 24 and 53) then 'ParquetStringToDoubleConvert'
   	   	  	else 'AsIs'
   	   	END
   	)
   	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
	else b.intermediate_stg_function
end
>>

oracle_parquet_hive_destination_datatype_name()::=<<
case
   when (b.source_datatype_name = 'number') then
   (
      CASE
         when (a.data_scale = 0 and (a.data_precision between 0 and 2)) then 'TINYINT'
         when (a.data_scale = 0 and (a.data_precision between 3 and 4)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 5 and 9)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 10 and 18)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 18 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'STRING'
      END
   )
   when (b.source_datatype_name = 'float') then
   (
   	  case
   	   	 when (a.data_precision between 0 and 23) then 'FLOAT'
   	   	 when (a.data_precision between 24 and 53) then 'DOUBLE'
   	   	 else 'STRING'
   	  END
   )
   when(b.source_datatype_name = 'varchar2' or b.source_datatype_name = 'nvarchar2') then
   (
      CASE
         when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
   when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
   else b.destination_datatype_name
end
>>

oracle_default_postgres_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'number') then
	(
		CASE
            when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
        END
	)
	else b.intermediate_stg_function
end
>>

oracle_default_postgres_destination_datatype_name()::=<<
case
   when (b.source_datatype_name = 'number') then
   (
      CASE
         when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'text'
      END
   )

   when(b.source_datatype_name = 'varchar2' or b.source_datatype_name = 'nvarchar2') then
   (
      CASE
         when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
   when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'text'
   else b.destination_datatype_name
end
>>

sap_hana_default_postgres_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'timestamp') then
	(
		CASE
            when (data_scale <7) then b.intermediate_stg_function else 'AsIs'
        END
	)
	else b.intermediate_stg_function
end
>>

sap_hana_default_postgres_destination_datatype_name()::=<<
case
   when (b.source_datatype_name = 'timestamp') then
   (
      CASE
         when (a.data_scale <7) then concat(b.destination_datatype_name,'(',a.data_scale,')')
         else 'varchar(255)'
      END
   )
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'text'
   else b.destination_datatype_name
end
>>

sap_hana_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>


sap_hana_parquet_hive_destination_datatype_name()::=<<
case
	when b.source_datatype_name = ('nvarchar') then
   (
      CASE
         when (a.data_length between 1 and 5000) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
   when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (a.data_length between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_length,',',a.data_scale,')')
			else 'STRING'
		END
	)
   when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
   when (b.source_datatype_name = 'timestamp') then
   (
      CASE
         when (a.data_scale <7) then concat(b.destination_datatype_name,'(',a.data_scale,')')
         else 'varchar(255)'
      END
   )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
end
>>

sap_hana_parquet_hive_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name ='timestamp') then
	(
		case
			when (a.data_scale <= 6) then null
			else 'timestamp_to_string'
		end
	)
	when (b.source_datatype_name ='time')then concat(b.source_datatype_name,'_to_string')
	else null
end
>>


sap_hana_parquet_hive_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'nchar') then
    (
		CASE
			when (a.data_length between 1 and 255) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name = 'timestamp') then
	(
		CASE
            when (data_scale <7) then b.intermediate_stg_function else 'AsIs'
        END
	)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

sap_hana_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

sap_hana_parquet_redshift_inconsistent_datatype() ::=<<
case
	when (b.source_datatype_name ='timestamp') then
	(
		case
			when (a.data_scale <= 6) then null
			else 'timestamp_to_string'
		end
	)
	when (b.source_datatype_name ='time')then concat(b.source_datatype_name,'_to_string')
	else null
end
>>

sap_hana_parquet_redshift_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'timestamp') then
	(
		CASE
            when (data_scale <7) then b.intermediate_stg_function else 'AsIs'
        END
	)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

sap_hana_parquet_redshift_destination_datatype_name()::=<<

CASE
	when b.source_datatype_name = ('nvarchar') then
   (
      CASE
         when (a.data_length between 1 and 5000) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
   when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (a.data_length between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_length,',',a.data_scale,')')
			else 'STRING'
		END
	)
   when (b.source_datatype_name = 'timestamp') then
   (
      CASE
         when (a.data_scale <7) then concat(b.destination_datatype_name,'(',a.data_scale,')')
         else 'varchar(255)'
      END
   )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(MAX)'
	else b.destination_datatype_name
END
>>

mysql_default_postgres_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

mysql_default_postgres_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'text'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'text'
		END
    )
    when (b.source_datatype_name = 'bit') then concat('VARCHAR','(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'text'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

sap_hana_default_oracle_intermediate_stg_function()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
   else b.intermediate_stg_function
end
>>

sap_hana_default_oracle_destination_datatype_name()::=<<
case
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
   else b.destination_datatype_name
end
>>

sap_hana_default_oracle_inconsistent_datatype() ::=<<
null
>>


sap_hana_default_oracle_source_stg_function()::=<<
$get_source_stg_function()$
>>

sap_hana_default_redshift_relational_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'timestamp') then
	(
		CASE
            when (data_scale <7) then b.intermediate_stg_function else 'AsIs'
        END
	)
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
   else b.intermediate_stg_function
end
>>

sap_hana_default_redshift_relational_destination_datatype_name()::=<<
case
	when b.source_datatype_name = ('nvarchar') then
   (
      CASE
         when (a.data_length between 1 and 5000) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'STRING'
      END
   )
	when (b.source_datatype_name = 'timestamp') then
   (
      CASE
         when (a.data_scale <7) then concat(b.destination_datatype_name,'(',a.data_scale,')')
         else 'varchar(255)'
      END
   )
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'varchar(max)'
   else b.destination_datatype_name
end
>>

sap_hana_default_redshift_relational_inconsistent_datatype() ::=<<
null
>>


sap_hana_default_redshift_relational_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_default_redshift_relational_inconsistent_datatype() ::=<<
null
>>


oracle_default_redshift_relational_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_default_redshift_relational_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'number') then
	(
		CASE
            when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
        END
	)
	else b.intermediate_stg_function
end
>>

oracle_default_redshift_relational_destination_datatype_name()::=<<
case
   when (b.source_datatype_name = 'number') then
   (
      CASE
         when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'text'
      END
   )

   when(b.source_datatype_name = 'varchar2' or b.source_datatype_name = 'nvarchar2') then
   (
      CASE
         when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
   when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
   when(a.advanced_options_sub_type_id in (14,15,17)) then 'varchar(max)'
   else b.destination_datatype_name
end
>>

mysql_default_redshift_relational_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

mysql_default_redshift_relational_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'varchar(max)'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'varchar(max)'
		END
    )
    when (b.source_datatype_name = 'bit') then concat('VARCHAR','(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'varchar(max)'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

sql_server_default_postgres_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <6 then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
	else b.intermediate_stg_function
END
>>

sql_server_default_postgres_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'char') then
	(
		CASE
			when (a.data_length <= 255) then concat('char','(', a.data_length,')')
			when (a.data_length between 256 and 8000) then concat('varchar','(', a.data_length,')')
			else 'text'
		END
	)
	when (b.source_datatype_name = 'varchar') then
	(
		CASE
			when (a.data_length = -1) then 'text'
			else concat('varchar','(', a.data_length,')')
		END
	)
	when (b.source_datatype_name = 'nchar') then
	(
		CASE
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			when (a.data_length <= 4000) then concat('varchar', '(', a.data_length, ')')
			else 'text'
		END
	)
	when (b.source_datatype_name = 'nvarchar') then
	(
		CASE
			when (a.data_length = -1) then 'text'
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			else concat('varchar', '(', a.data_length, ')')
		END
	)
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'text'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'text'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <14 then b.destination_datatype_name
			else 'text'
		END
	)
	when (b.source_datatype_name = 'smallmoney' or b.source_datatype_name = 'money') then
    (
        concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

sql_server_default_redshift_relational_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			 when (data_scale=0 and (data_precision between 0 and 5)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 6 and 10)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 11 and 19)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 20 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <6 then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
	else b.intermediate_stg_function
END
>>

sql_server_default_redshift_relational_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'char') then
	(
		CASE
			when (a.data_length <= 255) then concat('char','(', a.data_length,')')
			when (a.data_length between 256 and 8000) then concat('varchar','(', a.data_length,')')
			else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'varchar') then
	(
		CASE
			when (a.data_length = -1) then 'varchar(max)'
			else concat('varchar','(', a.data_length,')')
		END
	)
	when (b.source_datatype_name = 'nchar') then
	(
		CASE
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			when (a.data_length <= 4000) then concat('varchar', '(', a.data_length, ')')
			else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'nvarchar') then
	(
		CASE
			when (a.data_length = -1) then 'varchar(max)'
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			else concat('varchar', '(', a.data_length, ')')
		END
	)
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (a.data_scale = 0 and (a.data_precision between 0 and 5)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 6 and 10)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 11 and 19)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 20 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <14 then b.destination_datatype_name
			else 'varchar(max)'
		END
	)
	when (b.source_datatype_name = 'smallmoney' or b.source_datatype_name = 'money') then
    (
        concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

postgres_default_postgres_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			else 'varchar(2000)'
		END
	)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

postgres_default_postgres_intermediate_stg_function()::=<<
CASE
	when (b.source_datatype_name='numeric') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

redshift_default_redshift_relational_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

redshift_default_redshift_relational_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

postgres_default_redshift_relational_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

postgres_default_redshift_relational_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

redshift_default_postgres_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

redshift_default_postgres_destination_datatype_name()::=<<
CASE
when(b.source_datatype_name = 'char') then
   (
      CASE
         when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
         else 'text'
      END
   )
END
>>

redshift_default_postgres_inconsistent_datatype() ::=<<
null
>>

oracle_default_postgres_inconsistent_datatype() ::=<<
null
>>

sap_hana_default_postgres_inconsistent_datatype() ::=<<
null
>>

postgres_default_postgres_inconsistent_datatype() ::=<<
null
>>

redshift_default_redshift_relational_inconsistent_datatype() ::=<<
null
>>

postgres_default_redshift_relational_inconsistent_datatype() ::=<<
null
>>

mysql_default_postgres_inconsistent_datatype() ::=<<
null
>>

mysql_default_redshift_relational_inconsistent_datatype() ::=<<
null
>>

sql_server_default_postgres_inconsistent_datatype() ::=<<
null
>>


sql_server_default_redshift_relational_inconsistent_datatype() ::=<<
null
>>

hive_default_oracle_inconsistent_datatype() ::=<<
null
>>

hive_default_oracle_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

hive_default_oracle_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

hive_default_oracle_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

oracle_default_oracle_inconsistent_datatype() ::=<<
null
>>

oracle_default_oracle_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

oracle_default_oracle_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

oracle_default_oracle_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>


mysql_default_oracle_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_default_oracle_intermediate_stg_function()::=<<
CASE
	when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AnyToStringConvert'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

mysql_default_oracle_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'varchar(255)'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

mysql_default_oracle_inconsistent_datatype() ::=<<
null
>>

sql_server_default_oracle_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_default_oracle_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

sql_server_default_oracle_destination_datatype_name()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

sql_server_default_oracle_inconsistent_datatype() ::=<<
null
>>

postgres_default_oracle_intermediate_stg_function()::=<<
CASE
    	when (b.source_datatype_name = 'timestamp without time zone') then
	(
		CASE
			when data_precision <10 then b.intermediate_stg_function
			else 'AsIs'
		END
	)
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

postgres_default_oracle_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'timestamp without time zone') then
	(
		CASE
			when data_precision <10 then b.destination_datatype_name
			else 'varchar2(255)'
		END
	)
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'varchar2(1001)'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

postgres_default_oracle_inconsistent_datatype() ::=<<
null
>>


postgres_default_oracle_source_stg_function()::=<<
$get_source_stg_function()$
>>


oracle_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>

postgres_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>

sql_server_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>

hive_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>

mysql_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>


oracle_parquet_redshift_inconsistent_datatype()::=<<
case
	when (b.source_datatype_name in ('number','decimal')) then
	(
		case
			when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
			else 'number_to_varchar'
		end
	)
	when (b.source_datatype_name ~* 'INTERVAL YEAR TO MONTH') then 'interval_year_to_month_to_varchar'
	when (b.source_datatype_name ~* 'INTERVAL DAY TO SECOND') then 'interval_day_to_second_to_varchar'
	when (b.source_datatype_name in ('rowid','urowid','xmltype','timestamp','date')) then concat(b.source_datatype_name,'_to_varchar')
	when (b.source_datatype_name = 'float') then
	(
		case
			when (a.data_precision <= 53) then null
			else 'float_to_varchar'
		end
	)
	when (b.source_datatype_name ~* 'LOCAL TIME ZONE') then 'timestamp_with_local_time_zone_to_varchar'
	when (b.source_datatype_name ~* 'TIME ZONE') then 'timestamp_with_time_zone_to_varchar'
	else null
end
>>

mysql_parquet_redshift_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name = 'decimal') then
    (
    case
        when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
        else 'decimal_to_varchar'
    end
    )
    when (b.source_datatype_name in ('datetime','bit','timestamp','time','year','blob','mediumblob','longblob','tinyblob','binary','varbinary')) then        concat(b.source_datatype_name,'_to_varchar')
    else null
end
>>

postgres_parquet_redshift_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name = 'numeric') then
    (
        case
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'numeric_to_string'
        end
    )
    when (b.source_datatype_name = 'bit varying') then 'bit_varying_to_string'
    when (b.source_datatype_name = 'time without time zone') then 'time_without_time_zone_to_string'
    when (b.source_datatype_name = 'time with time zone') then 'time_with_time_zone_to_string'
    when (b.source_datatype_name = 'timestamp without time zone') then 'timestamp_without_time_zone_to_string'
    when (b.source_datatype_name = 'timestamp with time zone') then 'timestamp_with_time_zone_to_string'
    when (b.source_datatype_name in ('money','bit','abstime')) then concat(b.source_datatype_name,'_to_string')
    else null
end
>>

sql_server_parquet_redshift_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name in ('bit','time','timestamp','datetime','datetime2','datetimeoffset','smalldatetime','binary','varbinary')) then concat(b.source_datatype_name,'_to_varchar')
    else null
end
>>

oracle_parquet_redshift_destination_datatype_name()::=<<
CASE
when (b.source_datatype_name = 'char') then
(
    case
        when (a.data_length = 0 OR a.data_length = null) then 'char(1)'
        when (a.data_length between 1 and 2000) then concat('char(',a.data_length,')')
    END
)
when (b.source_datatype_name = 'varchar' or b.source_datatype_name = 'varchar2') THEN concat('varchar(',a.data_length,')')
when (b.source_datatype_name = 'nchar') THEN concat('nchar(',a.data_length,')')
when (b.source_datatype_name = 'nvarchar2') THEN concat('nvarchar(',a.data_length,')')
when (b.source_datatype_name = 'raw') THEN concat('varchar(',a.data_length*2,')')
when (b.source_datatype_name = 'urowid' OR b.source_datatype_name = 'xmltype') THEN concat('varchar(',a.data_length,')')
when (b.source_datatype_name = 'long' or b.source_datatype_name = 'clob' or b.source_datatype_name = 'nclob' OR b.source_datatype_name = 'blob' OR b.source_datatype_name = 'xmltype') THEN
CASE
WHEN (a.data_length =0 OR a.data_length = null) THEN 'varchar(5000)'
else
concat('varchar(',a.data_length,')')
end
when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number') then
(
CASE
when (data_scale <= 37 and (data_precision <= 38))
then concat('decimal','(',a.data_precision,',',a.data_scale,')')
else 'varchar(255)' END
)
when (b.source_datatype_name~*'timestamp') then 'varchar(100)'
when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
else b.destination_datatype_name END
>>

oracle_parquet_redshift_intermediate_stg_function()::=<<
case
when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number')
then
(
CASE
when (data_scale <= 37 and (data_precision <= 38)) then 'ParquetStringToDecimalConvert'
else  'AsIs'
END
)
when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
else b.intermediate_stg_function
end
>>

get_source_stg_function()::=<<
case
           when(a.advanced_options_sub_type_id = 17) then 'UserText'
           when(a.advanced_options_sub_type_id = 14) then 'AsIs'
           when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNull'
else b.source_stg_function
end
>>

get_Hive_source_stg_function()::=<<
case
           when(a.advanced_options_sub_type_id = 17) then 'UserText'
           when(a.advanced_options_sub_type_id = 14) then 'AsIs'
           when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNullHive'
else b.source_stg_function
end
>>

get_Mysql_source_stg_function()::=<<
case
           when(a.advanced_options_sub_type_id = 17) then 'UserText'
           when(a.advanced_options_sub_type_id = 14) then 'AsIs'
           when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNullMysql'
else b.source_stg_function
end
>>

redshift_parquet_hive_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'character') then
	(
		CASE
			when (a.data_length between 1 and 255) then concat(b.destination_datatype_name,'(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'character varying') then
	(
		CASE
			when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name,'(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			else 'string'
	END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

redshift_parquet_hive_intermediate_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
	else b.intermediate_stg_function
END
>>

redshift_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>

redshift_parquet_hive_inconsistent_datatype() ::=<<
null
>>

hive_parquet_hive_source_stg_function()::=<<
$get_Hive_source_stg_function()$
>>

oracle_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

sap_hana_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

redshift_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

redshift_default_redshift_relational_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_default_redshift_relational_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_default_postgres_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_default_redshift_relational_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_default_postgres_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_default_redshift_relational_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_adls_gen2_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_redshift_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_parquet_redshift_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_s3_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_s3_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_parquet_adls_gen2_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_parquet_s3_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_adls_gen2_source_stg_function()::=<<
$get_source_stg_function()$
>>

hive_parquet_hive_destination_datatype_name()::=<<
CASE
    when (a.metadata_datatype = 'varchar' and a.data_length is null) then 'STRING'
    when (a.metadata_datatype = 'char' or a.metadata_datatype = 'varchar') then concat(b.destination_datatype_name ,'(',a.data_length,')')
    when ((a.metadata_datatype like 'varchar(%' or a.metadata_datatype like 'char(%'))  then a.metadata_datatype
    when (b.source_datatype_name = 'decimal') then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
    when (b.source_datatype_name = 'array' or b.source_datatype_name = 'struct' or b.source_datatype_name = 'map') then lower(regexp_replace(a.metadata_datatype,'(\w+)(:)','`\1`\2','g'))
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

hive_parquet_hive_intermediate_stg_function()::=<<
CASE
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
	else b.intermediate_stg_function
END
>>

postgres_parquet_redshift_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'character' or b.source_datatype_name = 'bit') then(
    CASE
        when (a.data_length between 1 and 4096) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit varying') then(
    CASE
        when (a.data_length between 1 and 65535) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name = 'numeric') then(
    CASE
        when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
        else 'VARCHAR(MAX)'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
    else b.destination_datatype_name
END
>>

postgres_parquet_redshift_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'character') then(
    CASE
        when (a.data_length between 1 and 4096) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'character varying') then(
    CASE
        when (a.data_length between 1 and 65535) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'numeric') then(
    CASE
        when (a.data_precision between 1 and 38) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

mysql_parquet_redshift_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char' or b.source_datatype_name='binary') then(
    case
        when (a.data_length = 0) then 'VARCHAR(1)'
        when (a.data_length between 1 and 4096) then concat('CHAR','(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name = 'varchar' or b.source_datatype_name='blob' or b.source_datatype_name='mediumblob' or b.source_datatype_name='longblob' or b.source_datatype_name='enum' or b.source_datatype_name='set' or b.source_datatype_name='text' or b.source_datatype_name='mediumtext' or b.source_datatype_name='tinytext' or b.source_datatype_name='longtext') then(
    case
        when (a.data_length = 0) then 'VARCHAR(1)'
        when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
        else 'VARCHAR(MAX)'
    END)
    when (b.source_datatype_name='bit') then concat(b.destination_datatype_name ,'(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
        else 'VARCHAR'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
    else b.destination_datatype_name
END
>>

mysql_parquet_redshift_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'char') then(
    CASE
        when (a.data_length between 1 and 4096) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'varchar') then(
    CASE
        when (a.data_length between 1 and 65535) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when (a.data_precision between 1 and 38) then b.intermediate_stg_function
        else 'AsIs'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

sql_server_parquet_redshift_destination_datatype_name()::=<<
CASE
when (b.source_datatype_name = 'char') then
(
CASE
when (a.data_length between 1 and 4096) then concat('char','(',a.data_length,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'varchar') then
(
CASE
when (a.data_length between 1 and 65535) then concat('varchar','(',a.data_length,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'nchar') then
(
CASE
when (a.data_length between 1 and 4096) then concat('nchar','(',a.data_length,')')
else 'NVARCHAR(MAX)' END
)
when (b.source_datatype_name = 'nvarchar') then
(
CASE
when (a.data_length between 1 and 65535) then concat('nvarchar','(',a.data_length,')')
else 'NVARCHAR(MAX)' END
)
when (b.source_datatype_name = 'decimal') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat('decimal','(',a.data_precision,',',a.data_scale,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'numeric') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat('numeric','(',a.data_precision,',',a.data_scale,')')
else 'VARCHAR(MAX)' END
)
when (b.source_datatype_name = 'binary') then
    (
        case
            when (a.data_length = 0) then 'varchar(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'varchar(MAX)' END
    )
    when (b.source_datatype_name = 'varbinary') then
    (
        case
            when (a.data_length = 0) then 'varchar(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'varchar(max)' END
    )
when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(40)'
else b.destination_datatype_name END
>>

sql_server_parquet_redshift_intermediate_stg_function()::=<<
CASE
when (b.source_datatype_name = 'char') then
(
CASE
when (a.data_length between 1 and 4096) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'varchar') then
(
CASE
when (a.data_length between 1 and 65535) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'nchar') then
(
CASE
when (a.data_length between 1 and 4096) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'nvarchar') then
(
CASE
when (a.data_length between 1 and 65535) then b.intermediate_stg_function
else 'AsIs' end
)
when (b.source_datatype_name = 'decimal') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
else 'AsIs' END
)
when (b.source_datatype_name = 'numeric') then
(
CASE
when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
else 'AsIs' END
)
when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
else b.intermediate_stg_function END
>>

oracle_parquet_azure_synapse_intermediate_stg_function()::=<<
case
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number') then
    (
        CASE
            when (data_scale = 0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
            when (data_scale = 0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
            when (data_scale = 0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
            when (data_scale = 0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
            when (data_scale = 0 and (data_precision between 19 and 38)) then b.intermediate_stg_function
            when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when (b.source_datatype_name = 'float') then
    (
   	    CASE
   	   	  	when (a.data_precision between 0 and 53) then 'ParquetStringToDoubleConvert'
   	   	  	else 'AsIs'
   	   	END
   	)
   	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

oracle_parquet_azure_synapse_destination_datatype_name()::=<<
case
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'number') then
    (
        CASE
            when (data_scale = 0 and (data_precision between 0 and 2)) then 'TINYINT'
            when (data_scale = 0 and (data_precision between 3 and 4)) then 'SMALLINT'
            when (data_scale = 0 and (data_precision between 5 and 9)) then 'INT'
            when (data_scale = 0 and (data_precision between 10 and 18)) then 'BIGINT'
            when (data_scale = 0 and (data_precision between 19 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
            when (data_scale <= 37 and (data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
            else 'VARCHAR(255)'
        END
    )
    when (b.source_datatype_name = 'float') then
    (
        case
       	  	when (a.data_precision between 0 and 53) then 'FLOAT'
   	   	  	else 'VARCHAR(255)'
       	END
    )
	when (b.source_datatype_name in ('char','nchar','varchar2','nvarchar2','raw')) then concat(b.destination_datatype_name,'(',a.data_length,')')
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
end
>>

oracle_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_adls_gen2_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_hive_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'bit') then concat('VARCHAR','(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
    else b.destination_datatype_name
END
>>

mysql_parquet_hive_intermediate_stg_function_old()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

mysql_parquet_hive_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

mysql_parquet_hive_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
        case
            when (a.data_length = 0) then 'VARCHAR(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)'  END
    )
    when (b.source_datatype_name = 'varchar' or b.source_datatype_name = 'enum' or b.source_datatype_name = 'set') then
    (
        case
            when (a.data_length = 0) then 'VARCHAR(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARCHAR(MAX)' END
    )
    when (b.source_datatype_name = 'binary') then
    (
        case
            when (a.data_length = 0) then 'VARBINARY(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARBINARY(MAX)' END
    )
    when (b.source_datatype_name = 'varbinary') then
    (
        case
            when (a.data_length = 0) then 'VARBINARY(1)'
            when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
        else 'VARBINARY(MAX)' END
    )
    when (b.source_datatype_name='bit') then concat(b.destination_datatype_name ,'(',a.data_precision,')')
    when (b.source_datatype_name = 'decimal') then
    (
        CASE
            when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
        else 'VARCHAR(255)' END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
END
>>

mysql_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
        CASE
            when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

mysql_parquet_azure_synapse_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

mysql_parquet_adls_source_stg_function()::=<<
$get_Mysql_source_stg_function()$
>>

sql_server_parquet_hive_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'char') then
	(
		CASE
			when (a.data_length <= 255) then concat('char','(', a.data_length,')')
			when (a.data_length between 256 and 8000) then concat('varchar','(', a.data_length,')')
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'varchar') then
	(
		CASE
			when (a.data_length = -1) then 'string'
			else concat('varchar','(', a.data_length,')')
		END
	)
	when (b.source_datatype_name = 'nchar') then
	(
		CASE
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			when (a.data_length <= 4000) then concat('varchar', '(', a.data_length, ')')
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'nvarchar') then
	(
		CASE
			when (a.data_length = -1) then 'STRING'
			when (a.data_length <= 255) then concat('char', '(', a.data_length, ')')
			else concat('varchar', '(', a.data_length, ')')
		END
	)
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'TINYINT'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'SMALLINT'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'INT'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'BIGINT'
			when (data_scale = 0 and (data_precision between 19 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			when (data_scale <= 37 and (data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'TINYINT'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'SMALLINT'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'INT'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'BIGINT'
			when (data_scale = 0 and (data_precision between 19 and 38)) then b.destination_datatype_name
			when (data_scale <= 37 and (data_precision <= 38)) then b.destination_datatype_name
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <7 then b.destination_datatype_name
			else 'STRING'
		END
	)
	when (b.source_datatype_name = 'smallmoney' or b.source_datatype_name = 'money') then
    (
        concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

sql_server_parquet_hive_intermediate_stg_function()::=<<
case
	when (b.source_datatype_name = 'decimal') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
			when (data_scale = 0 and (data_precision between 19 and 38)) then b.intermediate_stg_function
			when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE
			when (data_scale = 0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
			when (data_scale = 0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
			when (data_scale = 0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
			when (data_scale = 0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
			when (data_scale = 0 and (data_precision between 19 and 38)) then b.intermediate_stg_function
			when (data_scale <= 37 and (data_precision <= 38)) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when (b.source_datatype_name = 'datetime2') then
	(
		CASE
			when data_precision <7 then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
	else b.intermediate_stg_function
END
>>

sql_server_parquet_hive_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_hive_destination_datatype_name()::=<<
CASE
	when (b.source_datatype_name = 'character') then
	(
		CASE
			when (a.data_length between 1 and 255) then concat('char','(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit' or b.source_datatype_name = 'bit varying') then
	(
		CASE
			when (a.data_length between 1 and 65535) then concat('varchar','(',a.data_length,')')
			else 'string'
		END
	)
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
			else 'string'
	END
	)
   when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
		else 'STRING'
		END
    )
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'STRING'
	else b.destination_datatype_name
END
>>

postgres_parquet_hive_intermediate_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'numeric') then
	(
		CASE

			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
	)
	when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
	else b.intermediate_stg_function
END
>>

postgres_parquet_hive_source_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'timestamp without time zone') then (
		CASE
			when (a.data_precision is null) then 'PostgresTimestampOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'timestamp with time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimestamptzOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'time without time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimeOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
    when(a.advanced_options_sub_type_id = 17) then 'UserText'
    when(a.advanced_options_sub_type_id = 14) then 'AsIs'
    when(a.advanced_options_sub_type_id = 15) then 'ReplaceWithNull'
	else b.source_stg_function
END
>>

oracle_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_bigquery_intermediate_stg_function()::=<<
case
    when (b.source_datatype_name in ('number','decimal')) then
    (
        CASE
            when (data_scale=0 and (data_precision between 0 and 2)) then 'ParquetStringToTinyIntConvert'
            when (data_scale=0 and (data_precision between 3 and 4)) then 'ParquetStringToSmallIntConvert'
            when (data_scale=0 and (data_precision between 5 and 9)) then 'ParquetStringToIntConvert'
            when (data_scale=0 and (data_precision between 10 and 18)) then 'ParquetStringToBigIntConvert'
            when (data_scale=0 and (data_precision between 18 and 38)) then 'ParquetStringToDecimalConvert'
            when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when (b.source_datatype_name = 'float') then
    (
        CASE
                 when (a.data_precision between 0 and 23) then 'ParquetStringToFloatConvert'
                 when (a.data_precision between 24 and 53) then 'ParquetStringToDoubleConvert'
                 else 'AsIs'
           END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
end
>>

oracle_parquet_bigquery_destination_datatype_name()::=<<
case
    when (b.source_datatype_name = 'number') then
    (
      CASE
         when (a.data_scale = 0 and (a.data_precision between 0 and 2)) then 'TINYINT'
         when (a.data_scale = 0 and (a.data_precision between 3 and 4)) then 'SMALLINT'
         when (a.data_scale = 0 and (a.data_precision between 5 and 9)) then 'INT'
         when (a.data_scale = 0 and (a.data_precision between 10 and 18)) then 'BIGINT'
         when (a.data_scale = 0 and (a.data_precision between 18 and 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
         else 'STRING'
      END
    )
    when (b.source_datatype_name = 'float') then
    (
           case
                 when (a.data_precision between 0 and 53) then 'FLOAT'
                 else 'STRING'
           END
    )
    else b.destination_datatype_name
end
>>

hive_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

hive_parquet_bigquery_intermediate_stg_function()::=<<

case
    when (b.source_datatype_name ='decimal') then
    (
        CASE
            when (data_scale <= 9 and (a.data_precision <= 38)) then b.intermediate_stg_function
            else 'HiveStringConvert'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
end

>>

hive_parquet_bigquery_destination_datatype_name()::=<<

CASE
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when (a.data_scale <= 9 and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
        else 'string'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'string'
    else b.destination_datatype_name
END


>>

mysql_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_bigquery_intermediate_stg_function()::=<<

case
    when (b.source_datatype_name ='decimal') then
    (
        CASE
            when (data_scale <= 9 and (a.data_precision <= 38)) then b.intermediate_stg_function
            else 'AnyToStringConvert'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
end


>>

mysql_parquet_bigquery_destination_datatype_name()::=<<

CASE
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when (a.data_scale <= 9 and (a.data_length <= 38)) then concat(b.destination_datatype_name,'(',a.data_length,',',a.data_scale,')')
        else 'string'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'string'
    else b.destination_datatype_name
END

>>

sap_hana_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>


sap_hana_parquet_bigquery_intermediate_stg_function()::=<<

CASE
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when ((a.data_length between 1 and 38) and (a.data_scale between 1 and 9)) then 'AsIs'
        else 'PostgresStringConvert'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>


sap_hana_parquet_bigquery_destination_datatype_name()::=<<

CASE
    when (b.source_datatype_name = 'decimal') then(
    CASE
        when ((a.data_length between 1 and 38) and (a.data_scale between 1 and 9)) then concat('NUMERIC','(',a.data_length,',',a.data_scale,')')
        else 'STRING'
    END)
    else b.destination_datatype_name
END
>>

sap_hana_parquet_bigquery_inconsistent_datatype() ::=<<
null
>>

postgres_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_bigquery_intermediate_stg_function()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then 'AsIS'
        else 'PostgresStringConvert'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END


>>
postgres_parquet_bigquery_destination_datatype_name()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then concat('NUMERIC','(',a.data_precision,',',a.data_scale,')')
        else 'STRING'
    END)
    else b.destination_datatype_name
END

>>

sql_server_parquet_bigquery_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_bigquery_intermediate_stg_function()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then 'AsIS'
        else 'SQLServerStringConvert'
    END)
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END


>>

sql_server_parquet_bigquery_destination_datatype_name()::=<<

CASE
    when (b.source_datatype_name = 'NUMERIC') then(
    CASE
        when ((a.data_precision between 1 and 38) and (a.data_scale between 1 and 9)) then concat('NUMERIC','(',a.data_precision,',',a.data_scale,')')
        else 'STRING'
    END)
    else b.destination_datatype_name
END

>>

sap_hana_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'nchar') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('char','(',a.data_length,')')
            else 'VARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'nvarchar') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('varchar','(',a.data_length,')')
            else 'VARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'varbinary') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('varbinary','(',a.data_length,')')
            else 'VARBINARY(MAX)'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
END
>>

sap_hana_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

sap_hana_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

sap_hana_parquet_azure_synapse_inconsistent_datatype() ::=<<
case
	when (data_type in ('time','date','seconddate','timestamp')) then concat(data_type,'_to_varchar')
    else null
end
>>

postgres_parquet_s3_source_stg_function()::=<<
CASE
	when (b.source_datatype_name = 'timestamp without time zone') then(
		CASE
			when (a.data_precision is null) then 'PostgresTimestampOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'timestamp with time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimestamptzOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	when (b.source_datatype_name = 'time without time zone') then
	(
		CASE
			when (a.data_precision is null) then 'PostgresTimeOffsetZeroToStringConvert'
			else b.source_stg_function
		END
	)
	else
	b.source_stg_function
END
>>

sql_server_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('char','(',a.data_length,')')
            else 'VARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'varchar') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('varchar','(',a.data_length,')')
            else 'VARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'nchar') then
    (
        CASE
            when (a.data_length between 1 and 4000) then concat('nchar','(',a.data_length,')')
            else 'NVARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'nvarchar') then
    (
        CASE
            when (a.data_length between 1 and 4000) then concat('nvarchar','(',a.data_length,')')
            else 'NVARCHAR(MAX)'
        END
    )
    when (b.source_datatype_name = 'binary') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('binary','(',a.data_length,')')
            else 'VARBINARY(MAX)'
        END
    )
    when (b.source_datatype_name = 'varbinary') then
    (
        CASE
            when (a.data_length between 1 and 8000) then concat('varbinary','(',a.data_length,')')
            else 'VARBINARY(MAX)'
        END
    )
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
    (
        CASE
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then concat('decimal','(',a.data_precision,',',a.data_scale,')')
            else 'VARCHAR(255)'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
END
>>

sql_server_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
    (
        CASE
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
            else 'AsIs'
        END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

sql_server_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_adls_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_azure_synapse_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'character' or b.source_datatype_name = 'bit') then
    (
       case
          when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
          else 'VARCHAR(MAX)'
       END
    )
    when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit varying') then
    (
       case
          when (a.data_length between 1 and 8000) then concat(b.destination_datatype_name ,'(',a.data_length,')')
          else 'VARCHAR(MAX)'
       END
    )
    when (b.source_datatype_name = 'numeric') then
    (
       CASE
          when (a.data_precision between 1 and 38) then concat(b.destination_datatype_name ,'(',a.data_precision,',',a.data_scale,')')
          else 'VARCHAR(max)'
       END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'VARCHAR(8000)'
    else b.destination_datatype_name
END
>>

postgres_parquet_azure_synapse_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='numeric') then
    (
       CASE
          when (a.data_precision between 1 and 38) then b.intermediate_stg_function
          else 'AsIs'
       END
    )
    when(a.advanced_options_sub_type_id in (14,15,17)) then 'AnyToString'
    else b.intermediate_stg_function
END
>>

postgres_parquet_azure_synapse_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_adls_source_stg_function()::=<<
$get_source_stg_function()$
>>

generate_credential_json(templateData)::=<<
{
  "sourceCredentials": {
    "token": "$templateData.input_data.jwt_token$",
    "credential_id": $if(first(templateData.table_metadata_output).source_credential_id)$$first(templateData.table_metadata_output).source_credential_id$$else$0$endif$ ,
    "credential_type_id": $if(first(templateData.table_metadata_output).source_credential_type_id)$$first(templateData.table_metadata_output).source_credential_type_id$$else$0$endif$ ,
    "end_point": "$templateData.input_data.end_point$"
  },
  "targetCredentials":$if(first(templateData.table_metadata_output).filesystem_credential_id)$ {
    "token": "$templateData.input_data.jwt_token$",
    "credential_id":  $first(templateData.table_metadata_output).filesystem_credential_id$,
    "credential_type_id":  $first(templateData.table_metadata_output).filesystem_credential_type_id$,
    "end_point": "$templateData.input_data.end_point$"
  }
  $else${
    "token": "$templateData.input_data.jwt_token$",
    "credential_id": $if(first(templateData.table_metadata_output).destination_credential_id)$$first(templateData.table_metadata_output).destination_credential_id$$else$0$endif$ ,
    "credential_type_id": $if(first(templateData.table_metadata_output).destination_credential_type_id)$$first(templateData.table_metadata_output).destination_credential_type_id$$else$0$endif$ ,
    "end_point": "$templateData.input_data.end_point$"
  }  $endif$
}
>>

convert_base64_encrypted_json_creds_to_rsa(templateData)::=<<
$templateData.base64_encrypted_json_creds$
>>

generate_source_input_json_for_checkpoint(templateData)::=<<
$(["generate_source_input_json_for_checkpoint_",first(templateData.fetch_source_type).source_metadata_category])(templateData)$
>>

generate_source_input_json_for_checkpoint_file(templateData)::=<<
{
"processId":$templateData.input_data.process_id$,
"dataplaceId":$first(templateData.table_metadata_output).source_dataplace_id$,
"tableId":$first(templateData.table_metadata_output).file_id$,
"datamovementId":$templateData.input_data.data_movement_id$,
"jobScheduledUserId":"$templateData.input_data.job_scheduled_user_id$",
"jobScheduleId":$templateData.input_data.job_schedule_id$,
"batchId":$templateData.input_data.batch_id$,
"lastRunTimestamp":"$templateData.input_data.last_run_timestamp$"
}
>>

generate_source_input_json_for_checkpoint_relational(templateData)::=<<
{
"processId":$templateData.input_data.process_id$,
"dataplaceId":$first(templateData.table_metadata_output).dataplace_id$,
"tableId":$first(templateData.table_metadata_output).table_id$,
"datamovementId":$templateData.input_data.data_movement_id$,
"jobScheduledUserId":"$templateData.input_data.job_scheduled_user_id$",
"jobScheduleId":$templateData.input_data.job_schedule_id$,
"batchId":$templateData.input_data.batch_id$,
"lastRunTimestamp":"$templateData.input_data.last_run_timestamp$"
}
>>

generate_source_input_json_for_curation(templateData)::=<<
{
"processId":$templateData.input_data.process_id$,
"datamovementId":$templateData.input_data.data_movement_id$,
"jobScheduledUserId":"$templateData.input_data.job_scheduled_user_id$",
"jobScheduleId":$templateData.input_data.job_schedule_id$,
"batchId":$templateData.input_data.batch_id$,
"lastRunTimestamp":"$templateData.input_data.last_run_timestamp$"
}
>>

generate_almaren_artifact(templateData)::=<<

$if(!first(templateData.cdc_first_time).cdc_first_time && first(templateData.get_cdc_type).cdc_applied)$

$generate_cdc_artifact(templateData)$

$else$

$generate_new_artifact(templateData)$

$endif$

>>

generate_cdc_artifact(templateData)::=<<
import scala.util.{Try, Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}
import com.modak.common._
import com.modak.common.authenticateCredentials._
import com.modak.common.advanceTableOptions._
import com.modak.common.token._
import com.modak.common.token.Token._
import com.modak.checkpoint._
import io.circe.generic.auto._
import io.circe.syntax._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

    val logger = LogManager.getLogger("com.Artifact")
    logger.setLevel(Level.INFO)

    val inputJson = Token.parsedInputJson
    val datamovementId = inputJson.datamovementId
    val jobScheduleId = inputJson.jobScheduleId
    val jobScheduledUserId = inputJson.jobScheduledUserId
    val batchId = inputJson.batchId
    val processId = inputJson.processId
    val lastRunTimestamp = inputJson.lastRunTimestamp

    val checkpoint = Checkpoint(inputJson.processId, inputJson.dataplaceId, None, None, Some(inputJson.datamovementId))
    checkpoint.startDataMovement(inputJson.tableId)
  Try {
    import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
    import com.github.music.of.the.ainur.almaren.Almaren
    $if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
    import com.github.music.of.the.ainur.almaren.jdbcparallel.SourceJdbcParallelConn.SourceJdbcParallelImplicit
    import scala.util.Random
    $endif$
	$if( first(templateData.table_metadata_output).is_bigquery)$
	import com.github.music.of.the.ainur.almaren.bigquery.BigQuery.BigQueryImplicit
    $endif$
    val almaren = Almaren("nabu-sparkbot-ingestion")
    val spark = almaren.spark.getOrCreate()

	$if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
    val staticQuery = """SELECT $templateData.column_metadata_output:ApplyFunction();separator=","$
		                $if(first(templateData.sort_columns_data).sort_column_name)$,$templateData.sort_columns_data:tempColumns();separator=",\n | "$$endif$
		                $if(first(templateData.incremental_max_columns_with_column_ids).column_name)$,$templateData.incremental_max_columns_with_column_ids:incrementalColumns();separator=",\n | "$$endif$
	                    from $first(templateData.table_metadata_output).schema_name$.$first(templateData.table_metadata_output).table_name$"""
                        $(["partition_function_",first(templateData.table_metadata_output).source_type])(templateData)$
    val x=Random.shuffle(List($jdbcPartitionSubPartitionGeneration(templateData)$))
    spark.conf.set("mapredulce.fileoutputcommiter.marksuccessfuljobs", "false")
    $endif$

 $if(first(templateData.table_metadata_output).file_system_type && first(templateData.table_metadata_output).is_destination_hive)$
    $(["Configuration_",first(templateData.table_metadata_output).file_system_type])(templateData)$
 $elseif((first(templateData.table_metadata_output).is_file_system_type_s3a) && (first(templateData.table_metadata_output).is_filesystem_credential_type_aws))$
    $Configuration_s3a(templateData)$
 $endif$
    $if(first(templateData.table_metadata_output).is_bigquery)$
	$(["set_source_spark_confs_",first(templateData.table_metadata_output).source_type])(templateData)$
	spark.conf.set("viewsEnabled","true")
  	spark.conf.set("materializationDataset","$first(templateData.table_metadata_output).schema_name$");
	$endif$
 $if(first(templateData.table_metadata_output).is_default && first(templateData.table_metadata_output).create_backup_table)$
	val sourceDf1 = almaren.builder.
  	sourceJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,
                         """$([first(templateData.table_metadata_output).destination_type,"_destination_jdbcQuery"])(templateData)$""".stripMargin,
                          Some(Ldap.target.username), Some(Ldap.target.password), Map("fetchsize" -> Constants.DB.fetchSize))
                         .batch
  	val cnt1 = sourceDf1.count
        val sourceMetrics1 = Metrics.collect
        $(["set_destination_spark_confs",first(templateData.table_metadata_output).destination_type])(templateData)$
        almaren.builder
            .sourceDataFrame(sourceDf1)
            .alias("SOURCE_TABLE")
            .targetJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,"$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_backup_table_name$",SaveMode.Overwrite,Some(Ldap.target.username),Some(Ldap.target.password),Map()).batch;
 $endif$

    val sourceDf = almaren.builder
    $if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
       .sourceJdbcParallel($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
       $if(first(templateData.table_partition_output).is_sub_partition)$
       createSubPartitionQueries(staticQuery,x),$else$
       createPartitionQueries(staticQuery,x), $endif$
       $first(templateData.table_adv_opt_details_partition_details).maximum_parallel_connections$,
       Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
       .batch

    $else$
         $if(first(templateData.table_metadata_output).is_source_spark_hive)$
                     .sourceSql("""$([first(templateData.fetch_source_type).source,"_jdbcQuery"])(templateData)$"""$if(first(templateData.updated_wherecondition_replaced_with_default_and_max_values).incremental_load_where_clause)$.replaceAll("<%lastRunTimestamp%>", lastRunTimestamp)$endif$.stripMargin)
                  $elseif(first(templateData.table_metadata_output).is_bigquery)$
         			.sourceBigQuery("""$([first(templateData.table_metadata_output).source_type,"_jdbcQuery"])(templateData)$ """.stripMargin, Map("parentProject" -> "$first(templateData.table_metadata_output).project$", "project" -> "$first(templateData.table_metadata_output).project$"))
         		 $else$
         			.sourceJdbc($([first(templateData.fetch_source_type).source,"_source"])(templateData)$,
                     """$([first(templateData.fetch_source_type).source,"_jdbcQuery"])(templateData)$"""$if(first(templateData.updated_wherecondition_replaced_with_default_and_max_values).incremental_load_where_clause)$.replaceAll("<%lastRunTimestamp%>", lastRunTimestamp)$endif$.stripMargin,
                      Some(Ldap.source.username), Some(Ldap.source.password), Map("fetchsize" -> Constants.DB.fetchSize))
                  $endif$
                     .batch
	$endif$

    val cnt = sourceDf.count
    val sourceMetrics = Metrics.collect
    $(["set_destination_spark_confs_",first(templateData.table_metadata_output).destination_type])(templateData)$

val sourceNewDf = almaren.builder
        .sourceDataFrame(sourceDf)
        $if(first(templateData.virtual_column_metadata_output).spark_expression)$
        $templateData.virtual_column_metadata_output:SparkExpression();separator="\n"$ $endif$
        .alias("SOURCE_TABLE")
        .sql("""$destination_transformation_query(templateData)$ """.stripMargin)
        .batch

val targetDf = almaren.builder.sourceFile("parquet","$([first(templateData.table_metadata_output).destination_type,"_parquet_path"])(templateData)$",Map()).batch

$if(first(templateData.get_cdc_type).cdctype2 && !first(templateData.get_cdc_type).is_primary)$
$generate_cdctype2_hash(templateData)$
$elseif(first(templateData.get_cdc_type).cdctype2 && first(templateData.get_cdc_type).is_primary)$
$generate_cdctype2_primary(templateData)$
$elseif(!first(templateData.get_cdc_type).cdctype2 && first(templateData.get_cdc_type).is_primary && !first(templateData.cdc_first_time).cdc_first_time )$
$generate_upsert_primary(templateData)$
$endif$
$if(first(templateData.advanced_options_tables_flags_status).sort_by_columns)$
.$if(first(templateData.table_metadata_output).is_destination_hive)$sort$else$sortWithinPartitions$endif$($templateData.sort_by_columns_data:applySort();separator=", "$)
$endif$
.write.mode("overwrite")
$if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
.partitionBy($templateData.get_partition_cols:applyPartitions();separator=", "$)
$endif$
.format("parquet").save("$([first(templateData.table_metadata_output).destination_type,"_tmp_parquet_path"])(templateData)$")
Util.renameFilepath("$([first(templateData.table_metadata_output).destination_type,"_tmp_parquet_path"])(templateData)$","$([first(templateData.table_metadata_output).destination_type,"_parquet_path"])(templateData)$")

    $if(first(templateData.list_of_maps_of_incremental_columns).encoded_list_of_maps_of_incremental_columns)$
    val incrementalData=com.modak.common.advanceTableOptions.Incremental.getListOfColumnMaxValue(sourceDf, "$first(templateData.list_of_maps_of_incremental_columns).encoded_list_of_maps_of_incremental_columns$")
    checkpoint.insertAdvOptionsCheckpointData(inputJson.tableId,6,incrementalData)
    $endif$

    $if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
    val distinctValues = advanceTableOptions.DistinctValues.getListOfDistinctValues(sourceDf,"$first(templateData.get_list_of_maps_of_partition_columns).encoded_partition_cols$")
    checkpoint.insertAdvOptionsCheckpointData(inputJson.tableId,4,distinctValues)$endif$

     val targetMetrics = Metrics.collect
     val metrics = MetricsType(sourceMetrics.bytesRead,sourceMetrics.recordsRead,targetMetrics.bytesWritten,targetMetrics.recordsWritten)
     (metrics,cnt)
   } match {
     case Success(s) =>
         logger.info(s"Success \${inputJson.tableId}")
         checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true, s._1)
         checkpoint.insertDataMovementMetrics(inputJson.tableId, s._1,s._2)
     case Failure(f) =>
         logger.error(s"Failed \${inputJson.tableId}")
         logger.error(s"Error while ingestion", f)
         checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
         throw f
   }
>>

generate_cdctype2_hash(templateData)::=<<
 val finalDf = almaren.builder
     .sourceDataFrame(sourceNewDf)
     .sqlExpr(""" * """, """$first(templateData.get_cdc_info).hash_type$(concat($templateData.column_metadata_output:generate_source_cols();separator = ","$)$if(first(templateData.get_cdc_info).is_sha2)$,256 $endif$) as hash """).alias("source")
     .sourceDataFrame(targetDf)
     .sqlExpr(""" * """, """$first(templateData.get_cdc_info).hash_type$(concat($templateData.column_metadata_output:generate_target_cols();separator = ","$)$if(first(templateData.get_cdc_info).is_sha2)$,256 $endif$) as hash """).alias("target")
     .sql("""  select $templateData.column_metadata_output:generate_target_alias_cols();separator = ","$,
     target.$first(templateData.get_cdc_info).start_date_column_name$, case when target.$first(templateData.get_cdc_info).end_date_column_name$=cast('9999-12-31 00:00:00.000000' as string) then cast(current_timestamp as string) else target.$first(templateData.get_cdc_info).end_date_column_name$ end as $first(templateData.get_cdc_info).end_date_column_name$ , target.$first(templateData.get_cdc_info).job_id_column_name$ as $first(templateData.get_cdc_info).job_id_column_name$ , target.$first(templateData.get_cdc_info).prev_job_id_column_name$ as $first(templateData.get_cdc_info).prev_job_id_column_name$ from target
     left anti join source on target.hash = source.hash  """).alias("deleted_records")
     .sql("""  select $templateData.column_metadata_output:generate_source_alias_cols();separator = ","$,
     coalesce(target.$first(templateData.get_cdc_info).start_date_column_name$,cast(current_timestamp as string)) as $first(templateData.get_cdc_info).start_date_column_name$,
     coalesce(target.$first(templateData.get_cdc_info).end_date_column_name$,cast('9999-12-31 00:00:00.000000' as string)) as $first(templateData.get_cdc_info).end_date_column_name$ ,
     coalesce(target.$first(templateData.get_cdc_info).job_id_column_name$,cast($first(templateData.get_new_cdc_id).process_id$ as long)) as $first(templateData.get_cdc_info).job_id_column_name$ ,
     coalesce(target.$first(templateData.get_cdc_info).prev_job_id_column_name$,cast($first(templateData.get_new_cdc_id).prev_process_id$ as long)) as $first(templateData.get_cdc_info).prev_job_id_column_name$ from source
     left outer join target on source.hash = target.hash  """).alias("new_records")
     .sql(
     """  select * from deleted_records
             union all
         select * from new_records
         """).batch
finalDf.drop("hash")
>>

generate_cdctype2_primary(templateData)::=<<
 val finalDf = almaren.builder
     .sourceDataFrame(sourceNewDf)
     .sqlExpr(""" * """, """$first(templateData.get_cdc_info).hash_type$(concat($templateData.column_metadata_output:generate_source_cols();separator = ","$)$if(first(templateData.get_cdc_info).is_sha2)$,256 $endif$) as hash """).alias("source")
     .sourceDataFrame(targetDf)
     .sqlExpr(""" * """, """$first(templateData.get_cdc_info).hash_type$(concat($templateData.column_metadata_output:generate_target_cols();separator = ","$)$if(first(templateData.get_cdc_info).is_sha2)$,256 $endif$) as hash """).alias("target")
     .sql("""  select $templateData.column_metadata_output:generate_target_alias_cols();separator = ","$,
     target.$first(templateData.get_cdc_info).start_date_column_name$, case when target.$first(templateData.get_cdc_info).end_date_column_name$=cast('9999-12-31 00:00:00.000000' as string) then cast(current_timestamp as string) else target.$first(templateData.get_cdc_info).end_date_column_name$ end as $first(templateData.get_cdc_info).end_date_column_name$ , target.$first(templateData.get_cdc_info).job_id_column_name$ as $first(templateData.get_cdc_info).job_id_column_name$ , target.$first(templateData.get_cdc_info).prev_job_id_column_name$ as $first(templateData.get_cdc_info).prev_job_id_column_name$ from target
     left anti join source on
     $templateData.column_metadata_output:generate_primary_key_joins();separator = " "$
     target.hash = source.hash
     """).alias("deleted_records")
     .sql("""  select $templateData.column_metadata_output:generate_source_alias_cols();separator = ","$,
     coalesce(target.$first(templateData.get_cdc_info).start_date_column_name$,cast(current_timestamp as string)) as $first(templateData.get_cdc_info).start_date_column_name$,
     coalesce(target.$first(templateData.get_cdc_info).end_date_column_name$,cast('9999-12-31 00:00:00.000000' as string)) as $first(templateData.get_cdc_info).end_date_column_name$ ,
     coalesce(target.$first(templateData.get_cdc_info).job_id_column_name$,cast($first(templateData.get_new_cdc_id).process_id$ as long)) as $first(templateData.get_cdc_info).job_id_column_name$ ,
     coalesce(target.$first(templateData.get_cdc_info).prev_job_id_column_name$,cast($first(templateData.get_new_cdc_id).prev_process_id$ as long)) as $first(templateData.get_cdc_info).prev_job_id_column_name$ from source
     left outer join target on
    $templateData.column_metadata_output:generate_primary_key_joins();separator = " "$
    source.hash = target.hash
     """).alias("new_records")
     .sql(
     """  select * from deleted_records
             union all
         select * from new_records
         """).batch
finalDf.drop("hash")
>>

generate_upsert_primary(templateData)::=<<
 val finalDf = almaren.builder
     .sourceDataFrame(sourceNewDf).alias("source")
     .sourceDataFrame(targetDf).alias("target")
     .sql(""" select $templateData.column_metadata_output:generate_source_alias_cols();separator = ","$ from source
             union all
             select  $templateData.column_metadata_output:generate_target_alias_cols();separator = ","$ from target
             left anti join source on
             $templateData.get_primary_key_cols:generate_upsert_primary_key_joins();separator = " and \n"$
     """).batch
     finalDf
>>

generate_new_artifact(templateData)::=<<
import scala.util.{Try, Failure, Success}
import org.apache.log4j.{Level, Logger, LogManager}
import com.modak.common._
import com.modak.common.authenticateCredentials._
import com.modak.common.advanceTableOptions._
import com.modak.common.token._
import com.modak.common.token.Token._
import com.modak.checkpoint._
import io.circe.generic.auto._
import io.circe.syntax._
import org.apache.spark.sql._
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SaveMode

    val logger = LogManager.getLogger("com.Artifact")
    logger.setLevel(Level.INFO)

    val inputJson = Token.parsedInputJson
    val datamovementId = inputJson.datamovementId
    val jobScheduleId = inputJson.jobScheduleId
    val jobScheduledUserId = inputJson.jobScheduledUserId
    val batchId = inputJson.batchId
    val processId = inputJson.processId
    val lastRunTimestamp = inputJson.lastRunTimestamp
    val checkpoint = Checkpoint(inputJson.processId, inputJson.dataplaceId, None, None, Some(inputJson.datamovementId))
    checkpoint.startDataMovement(inputJson.tableId)
  Try {
    import com.github.music.of.the.ainur.almaren.builder.Core.Implicit
    import com.github.music.of.the.ainur.almaren.Almaren
    $if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
    import com.github.music.of.the.ainur.almaren.jdbcparallel.SourceJdbcParallelConn.SourceJdbcParallelImplicit
    import scala.util.Random
    $endif$
    $if( first(templateData.table_metadata_output).is_bigquery)$
    import com.github.music.of.the.ainur.almaren.bigquery.BigQuery.BigQueryImplicit
    $endif$
$(["generate_almaren_artifact_",first(templateData.fetch_source_type).source_metadata_category,"_to_",first(templateData.fetch_source_type).destination_metadata_category])(templateData)$
  } match {
    case Success(s) =>
        logger.info(s"Success \${inputJson.tableId}")
        checkpoint.endDatamovementWithSuccess(inputJson.tableId, verificationStatus = true, s._1)
        checkpoint.insertDataMovementMetrics(inputJson.tableId, s._1,s._2)
    case Failure(f) =>
        logger.error(s"Failed \${inputJson.tableId}")
        logger.error(s"Error while ingestion", f)
        checkpoint.endDatamovementWithError(inputJson.tableId, f.getLocalizedMessage, verificationStatus = false)
        throw f
  }
>>


generate_source_cols(templateData)::=<<
coalesce(cast(`$templateData.field_name$` as string),'-99&<%@#99-')
>>

generate_target_cols(templateData)::=<<
coalesce(cast(`$templateData.field_name$` as string),'-99&<%@#99-')
>>

generate_target_alias_cols(templateData)::=<<
target.$templateData.field_name$
>>

generate_source_alias_cols(templateData)::=<<
source.$templateData.field_name$
>>

generate_primary_key_joins(templateData)::=<<
$if(templateData.is_cdc_col)$target.$templateData.field_name$=source.$templateData.field_name$ and$endif$
>>

generate_upsert_primary_key_joins(templateData)::=<<
target.$templateData.field_name$=source.$templateData.field_name$
>>

hive_parquet_path(templateData)::=<<
$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$
>>

hive_tmp_parquet_path(templateData)::=<<
$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.get_cdc_type).temp_path$
>>

redshift_parquet_path(templateData)::=<<
$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$first(templateData.table_metadata_output).data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$
>>

redshift_tmp_parquet_path(templateData)::=<<
$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$first(templateData.table_metadata_output).data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.get_cdc_type).temp_path$
>>

azure_synapse_parquet_path(templateData)::=<<
$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet
>>

azure_synapse_tmp_parquet_path(templateData)::=<<
$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.get_cdc_type).temp_path$.parquet
>>

generate_almaren_artifact_file_to_relational(templateData)::=<<
    val almaren = Almaren("nabu-sparkbot-ingestion")
    val spark = almaren.spark.getOrCreate()

    $(["Configuration_",first(templateData.fetch_source_type).source])(templateData)$

    val sourceDf = almaren.builder
                    .sourceFile("$first(templateData.table_metadata_output).file_format$",
                    "$([first(templateData.fetch_source_type).source,"_source"])(templateData)$",
                    $file_options(first(templateData.table_metadata_output))$)
                    .batch

    val cnt = sourceDf.count
    val sourceMetrics = Metrics.collect

     $if(first(templateData.table_metadata_output).file_system_type && first(templateData.table_metadata_output).is_destination_hive)$
            $(["set_destination_spark_confs_",first(templateData.table_metadata_output).file_system_type])(templateData)$
         $elseif((first(templateData.table_metadata_output).is_file_system_type_s3a) && (first(templateData.table_metadata_output).is_filesystem_credential_type_aws))$
            $set_destination_spark_confs_s3a(templateData)$
         $endif$
     $(["set_destination_spark_confs_",first(templateData.table_metadata_output).destination_type])(templateData)$

    almaren.builder
        .sourceDataFrame(sourceDf)
        .alias("SOURCE_DF")
        .sql("""$(["sourceDf_transformation_query"])(templateData)$""".stripMargin)
        .alias("SOURCE_TABLE")
    $if(first(templateData.table_metadata_output).is_default)$
        .targetJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,"$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_table_name$",
        SaveMode.Overwrite,Some(Ldap.target.username),Some(Ldap.target.password),Map())
        .batch
    $else$
        .sql("""$(["destination_transformation_query_for_",first(templateData.fetch_source_type).source_metadata_category])(templateData)$""".stripMargin)
        .batch
        .write
        .format("$first(templateData.table_metadata_output).intermediate_type$")
        .mode("overwrite")
        .$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$ $endif$

    val targetMetrics = Metrics.collect
   $if(first(templateData.table_metadata_output).is_default)$
    val metrics = MetricsType(sourceMetrics.bytesRead,sourceMetrics.recordsRead,sourceMetrics.bytesRead,sourceMetrics.recordsRead)
   $else$
    val metrics = MetricsType(sourceMetrics.bytesRead,sourceMetrics.recordsRead,targetMetrics.bytesWritten,targetMetrics.recordsWritten)
   $endif$
    (metrics,cnt)
>>

generate_almaren_artifact_relational_to_relational(templateData)::=<<
    val almaren = Almaren("nabu-sparkbot-ingestion")
    val spark = almaren.spark.getOrCreate()

	$if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
    val staticQuery = """SELECT $templateData.column_metadata_output:ApplyFunction();separator=","$
		                $if(first(templateData.sort_columns_data).sort_column_name)$,$templateData.sort_columns_data:tempColumns();separator=",\n | "$$endif$
		                $if(first(templateData.incremental_max_columns_with_column_ids).column_name)$,$templateData.incremental_max_columns_with_column_ids:incrementalColumns();separator=",\n | "$$endif$
	                    from $first(templateData.table_metadata_output).schema_name$.$first(templateData.table_metadata_output).table_name$"""
                        $(["partition_function_",first(templateData.table_metadata_output).source_type])(templateData)$
    val x=Random.shuffle(List($jdbcPartitionSubPartitionGeneration(templateData)$))
    spark.conf.set("mapredulce.fileoutputcommiter.marksuccessfuljobs", "false")$endif$

 $if(first(templateData.table_metadata_output).file_system_type && first(templateData.table_metadata_output).is_destination_hive)$
    $(["Configuration_",first(templateData.table_metadata_output).file_system_type])(templateData)$
 $elseif((first(templateData.table_metadata_output).is_file_system_type_s3a) && (first(templateData.table_metadata_output).is_filesystem_credential_type_aws))$
    $Configuration_s3a(templateData)$
 $endif$
    $if(first(templateData.table_metadata_output).is_bigquery)$
	$(["set_source_spark_confs_",first(templateData.table_metadata_output).source_type])(templateData)$
	spark.conf.set("viewsEnabled","true")
  	spark.conf.set("materializationDataset","$first(templateData.table_metadata_output).schema_name$");
  	$endif$
 $if(first(templateData.table_metadata_output).is_default && first(templateData.table_metadata_output).create_backup_table)$
	val sourceDf1 = almaren.builder.
  	sourceJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,
                         """$([first(templateData.table_metadata_output).destination_type,"_destination_jdbcQuery"])(templateData)$""".stripMargin,
                          Some(Ldap.target.username), Some(Ldap.target.password), Map("fetchsize" -> Constants.DB.fetchSize))
                         .batch
  	val cnt1 = sourceDf1.count
        val sourceMetrics1 = Metrics.collect
        $(["set_destination_spark_confs",first(templateData.table_metadata_output).destination_type])(templateData)$
        almaren.builder
            .sourceDataFrame(sourceDf1)
            .alias("SOURCE_TABLE")
            .targetJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,"$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_backup_table_name$",SaveMode.Overwrite,Some(Ldap.target.username),Some(Ldap.target.password),Map()).batch;
 $endif$
    val sourceDf = almaren.builder
    $if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
       .sourceJdbcParallel($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
       $if(first(templateData.table_partition_output).is_sub_partition)$
       createSubPartitionQueries(staticQuery,x),$else$
       createPartitionQueries(staticQuery,x),$endif$
       $first(templateData.table_adv_opt_details_partition_details).maximum_parallel_connections$,
       Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
       .batch
    $else$
         $if(first(templateData.table_metadata_output).is_source_spark_hive)$
                     .sourceSql("""$([first(templateData.fetch_source_type).source,"_jdbcQuery"])(templateData)$"""$if(first(templateData.updated_wherecondition_replaced_with_default_and_max_values).incremental_load_where_clause)$.replaceAll("<%lastRunTimestamp%>", lastRunTimestamp)$endif$.stripMargin)
                  $elseif(first(templateData.table_metadata_output).is_bigquery)$
         			.sourceBigQuery("""$([first(templateData.table_metadata_output).source_type,"_jdbcQuery"])(templateData)$ """.stripMargin, Map("parentProject" -> "$first(templateData.table_metadata_output).project$", "project" -> "$first(templateData.table_metadata_output).project$"))
         		 $else$
         			.sourceJdbc($([first(templateData.fetch_source_type).source,"_source"])(templateData)$,
                     """$([first(templateData.fetch_source_type).source,"_jdbcQuery"])(templateData)$"""$if(first(templateData.updated_wherecondition_replaced_with_default_and_max_values).incremental_load_where_clause)$.replaceAll("<%lastRunTimestamp%>", lastRunTimestamp)$endif$.stripMargin,
                      Some(Ldap.source.username), Some(Ldap.source.password), Map("fetchsize" -> Constants.DB.fetchSize))
                  $endif$
                     .batch

	$endif$

    val cnt = sourceDf.count
    val sourceMetrics = Metrics.collect
    $(["set_destination_spark_confs_",first(templateData.table_metadata_output).destination_type])(templateData)$

    almaren.builder
        .sourceDataFrame(sourceDf)
        $if(first(templateData.virtual_column_metadata_output).spark_expression)$
        $templateData.virtual_column_metadata_output:SparkExpression();separator="\n"$ $endif$
        .alias("SOURCE_TABLE")
        .sql("""$destination_transformation_query(templateData)$ """.stripMargin)
        $if(first(templateData.cdc_first_time).cdc_first_time && first(templateData.get_cdc_type).cdctype2)$
        .sqlExpr(""" * """, """ cast(current_timestamp as string) as $first(templateData.get_cdc_info).start_date_column_name$ """, """cast('9999-12-31 00:00:00.000000' as string) as $first(templateData.get_cdc_info).end_date_column_name$ """, """ cast($first(templateData.get_new_cdc_id).process_id$ as long) as $first(templateData.get_cdc_info).job_id_column_name$ """, """ cast($first(templateData.get_new_cdc_id).prev_process_id$ as long) as $first(templateData.get_cdc_info).prev_job_id_column_name$ """)
        $endif$
        $if(first(templateData.table_metadata_output).is_default && first(templateData.table_metadata_output).rename_table_condition)$
        .targetJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,
        "$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).new_table_name$",
        SaveMode.$if(first(templateData.ingestion_mode).ingestion_mode_key && first(templateData.get_already_ingested_condition).already_ingested)$$first(templateData.ingestion_mode).save_mode$$else$Overwrite$endif$,Some(Ldap.target.username),Some(Ldap.target.password),Map())
        .batch$elseif(first(templateData.table_metadata_output).is_default)$
        .targetJdbc($([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$,
        "$first(templateData.table_metadata_output).destination_schema_name$.$first(templateData.table_metadata_output).destination_table_name$",
        SaveMode.$if(first(templateData.ingestion_mode).ingestion_mode_key && first(templateData.get_already_ingested_condition).already_ingested)$$first(templateData.ingestion_mode).save_mode$$else$Overwrite$endif$,Some(Ldap.target.username),Some(Ldap.target.password),Map())
        .batch$else$
        .batch$if(first(templateData.advanced_options_tables_flags_status).sort_by_columns)$
        .$if(first(templateData.table_metadata_output).is_destination_hive)$sort$else$sortWithinPartitions$endif$($templateData.sort_by_columns_data:applySort();separator=", "$)$endif$
        .write
        .format("$first(templateData.table_metadata_output).intermediate_type$")
        .mode($if(first(templateData.ingestion_mode).ingestion_mode_key && first(templateData.get_already_ingested_condition).already_ingested)$"$first(templateData.ingestion_mode).mode$"$else$"overwrite"$endif$)
        $if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
        .partitionBy($templateData.get_partition_cols:applyPartitions();separator=", "$)$endif$
        .$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$$endif$

    $if(first(templateData.list_of_maps_of_incremental_columns).encoded_list_of_maps_of_incremental_columns)$
    val incrementalData=com.modak.common.advanceTableOptions.Incremental.getListOfColumnMaxValue(sourceDf, "$first(templateData.list_of_maps_of_incremental_columns).encoded_list_of_maps_of_incremental_columns$")
    checkpoint.insertAdvOptionsCheckpointData(inputJson.tableId,6,incrementalData)  $endif$

    $if(first(templateData.advanced_options_tables_flags_status).table_partitions)$
    val distinctValues = advanceTableOptions.DistinctValues.getListOfDistinctValues(sourceDf,"$first(templateData.get_list_of_maps_of_partition_columns).encoded_partition_cols$")
    checkpoint.insertAdvOptionsCheckpointData(inputJson.tableId,4,distinctValues)$endif$

    val targetMetrics = Metrics.collect
       $if(first(templateData.table_metadata_output).is_default)$
        val metrics = MetricsType(sourceMetrics.bytesRead,sourceMetrics.recordsRead,sourceMetrics.bytesRead,sourceMetrics.recordsRead)
       $else$
    val metrics = MetricsType(sourceMetrics.bytesRead,sourceMetrics.recordsRead,targetMetrics.bytesWritten,targetMetrics.recordsWritten)
       $endif$
    (metrics,cnt)
>>

generate_almaren_artifact_relational_to_file(templateData)::=<<
    val almaren = Almaren("nabu-sparkbot-ingestion");
    val spark = almaren.spark.getOrCreate() ;
    val sourceDf = almaren.builder
$if(first(templateData.table_partition_output) && (first(templateData.table_adv_opt_details_partition_details).is_parallel_ingestion))$
                    .sourceJdbcParallel($([first(templateData.table_metadata_output).source_type,"_source"])(templateData)$,
                    List($([first(templateData.table_metadata_output).source_type,"_jdbcParallelQueries"])(templateData)$),
                    $first(templateData.table_adv_opt_details_partition_details).maximum_parallel_connections$,
                    Some(Ldap.source.username),Some(Ldap.source.password),Map("fetchsize" -> Constants.DB.fetchSize))
                    .batch
                    .write
                    .format("$first(templateData.table_metadata_output).intermediate_type$")
                    .mode($if(first(templateData.ingestion_mode).ingestion_mode_key && first(templateData.get_already_ingested_condition).already_ingested)$"$first(templateData.ingestion_mode).mode$"$else$"overwrite"$endif$)
                    .$([first(templateData.table_metadata_output).destination_type,"_destination_YYYY_MM_DD_HH_mm_SS_format"])(templateData)$
$else$
         $if(first(templateData.table_metadata_output).is_source_spark_hive)$
                     .sourceSql("""$([first(templateData.fetch_source_type).source,"_jdbcQuery"])(templateData)$""".stripMargin)
                  $elseif(first(templateData.table_metadata_output).is_bigquery)$
         			.sourceBigQuery("""$([first(templateData.table_metadata_output).source_type,"_jdbcQuery"])(templateData)$ """.stripMargin, Map("parentProject" -> "$first(templateData.table_metadata_output).project$", "project" -> "$first(templateData.table_metadata_output).project$"))
         		 $else$
         			.sourceJdbc($([first(templateData.fetch_source_type).source,"_source"])(templateData)$,
                     """$([first(templateData.fetch_source_type).source,"_jdbcQuery"])(templateData)$""".stripMargin,
                      Some(Ldap.source.username), Some(Ldap.source.password), Map("fetchsize" -> Constants.DB.fetchSize))
                  $endif$
                    .batch

    val cnt = sourceDf.count
    val sourceMetrics = Metrics.collect
     $(["set_destination_spark_confs_",first(templateData.table_metadata_output).destination_type])(templateData)$
    almaren.builder
        .sourceDataFrame(sourceDf)
        .alias("SOURCE_TABLE")
        .batch
        .write
        .format("$first(templateData.table_metadata_output).intermediate_type$")
        .mode($if(first(templateData.ingestion_mode).ingestion_mode_key && first(templateData.get_already_ingested_condition).already_ingested)$"$first(templateData.ingestion_mode).mode$"$else$"overwrite"$endif$)
        .$([first(templateData.table_metadata_output).destination_type,"_destination"])(templateData)$ $endif$

   val targetMetrics = Metrics.collect
   val metrics = MetricsType(sourceMetrics.bytesRead,sourceMetrics.recordsRead,targetMetrics.bytesWritten,targetMetrics.recordsWritten)
   (metrics,cnt)
>>

SparkExpression(data)::=<<
.sqlExpr("*","$data.spark_expression$")
>>

applySort(column_data)::=<<
$column_data.order_by$$column_data.sort_null_value$("$column_data.final_column_name$")
>>

applyPartitions(column_data)::=<<
"$column_data.field_name$"
>>

set_destination_spark_confs_hive(templateData)::=<<>>

set_destination_spark_confs_s3(templateData)::=<<
 val endPointTarget = spark.conf.get("spark.driver.target_aws_endPoint").toString
 Util.setUpSparkConfiguration("fs.s3a.endpoint",endPointTarget)
 Util.setUpSparkConfiguration("fs.s3a.access.key", Aws.target.access_id)
 Util.setUpSparkConfiguration("fs.s3a.secret.key", Aws.target.secret_access_key)
 Util.setUpSparkConfiguration("fs.s3a.fast.upload", "true")
 Util.setUpSparkConfiguration("fs.s3a.multiobjectdelete.enable", "false")
 Util.setUpSparkConfiguration("mapreduce.fileoutputcommitter.algorithm.version", "2")
 Util.setUpSparkConfiguration("fs.s3a.fast.upload.buffer", "bytebuffer")
 spark.conf.set("spark.speculation", "false")
 spark.conf.set("spark.sql.parquet.filterPushdown", "true")
 spark.conf.set("spark.sql.parquet.mergeSchema", "false")
 Util.setUpSparkConfiguration("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
>>

set_destination_spark_confs_bigquery(templateData)::=<<
 spark.conf.set("credentials", Util.encodeBase64(Gcp.target.asJson.noSpaces))
 Util.setUpSparkConfiguration("fs.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
 Util.setUpSparkConfiguration("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
 Util.setUpSparkConfiguration("fs.gs.auth.service.account.email",Gcp.target.client_email)
 Util.setUpSparkConfiguration("fs.gs.auth.service.account.private.key.id",Gcp.target.private_key_id)
 Util.setUpSparkConfiguration("fs.gs.auth.service.account.private.key",Gcp.target.private_key)
>>

set_source_spark_confs_bigquery(templateData)::=<<
 spark.conf.set("credentials", Util.encodeBase64(Gcp.source.asJson.noSpaces))
 Util.setUpSparkConfiguration("fs.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
 Util.setUpSparkConfiguration("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
 Util.setUpSparkConfiguration("fs.gs.auth.service.account.email",Gcp.source.client_email)
 Util.setUpSparkConfiguration("fs.gs.auth.service.account.private.key.id",Gcp.source.private_key_id)
 Util.setUpSparkConfiguration("fs.gs.auth.service.account.private.key",Gcp.source.private_key)
>>

set_destination_spark_confs_redshift(templateData)::=<<
$if(first(templateData.table_metadata_output).is_default && first(templateData.table_metadata_output).is_source_filetype)$
    val endPointTarget = spark.conf.get("spark.driver.target_aws_endPoint").toString
    Util.setUpSparkConfiguration("fs.s3a.endpoint",endPointTarget)
    Util.setUpSparkConfiguration("fs.s3a.access.key", Aws.target.access_id)
    Util.setUpSparkConfiguration("fs.s3a.secret.key", Aws.target.secret_access_key)
    Util.setUpSparkConfiguration("fs.s3a.fast.upload", "true")
    Util.setUpSparkConfiguration("fs.s3a.multiobjectdelete.enable", "false")
    Util.setUpSparkConfiguration("mapreduce.fileoutputcommitter.algorithm.version", "2")
    Util.setUpSparkConfiguration("fs.s3a.fast.upload.buffer", "bytebuffer")
    spark.conf.set("spark.speculation", "false")
    spark.conf.set("spark.sql.parquet.filterPushdown", "true")
    spark.conf.set("spark.sql.parquet.mergeSchema", "false")
    Util.setUpSparkConfiguration("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")$else$
$set_destination_spark_confs_s3(templateData)$
$endif$
>>

set_destination_spark_confs_postgres(templateData)::=<<

>>


set_destination_spark_confs_azure_synapse(templateData)::=<<
 $if(first(templateData.table_metadata_output).is_azure_oauth)$
   spark.conf.set(s"fs.azure.account.auth.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "OAuth")
   spark.conf.set(s"fs.azure.account.oauth.provider.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
   spark.conf.set(s"fs.azure.account.oauth2.client.id.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_id}")
   spark.conf.set(s"fs.azure.account.oauth2.client.secret.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_secret}")
   spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"https://login.microsoftonline.com/\${AzureOAuth.target.tenant_id}/oauth2/token")
 $else$
   spark.conf.set(s"fs.azure.account.key.\${AzureGen2.target.accountname}.dfs.core.windows.net", AzureGen2.target.accountkey)
 $endif$
>>

set_destination_spark_confs_adls_gen2(templateData)::=<<
  $if(first(templateData.table_metadata_output).is_azure_oauth)$
      spark.conf.set(s"fs.azure.account.auth.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "OAuth")
      spark.conf.set(s"fs.azure.account.oauth.provider.type.\${AzureOAuth.target.accountname}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
      spark.conf.set(s"fs.azure.account.oauth2.client.id.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_id}")
      spark.conf.set(s"fs.azure.account.oauth2.client.secret.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"\${AzureOAuth.target.client_secret}")
      spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.\${AzureOAuth.target.accountname}.dfs.core.windows.net", s"https://login.microsoftonline.com/\${AzureOAuth.target.tenant_id}/oauth2/token")
  $else$
      spark.conf.set(s"fs.azure.account.key.\${AzureGen2.target.accountname}.dfs.core.windows.net", AzureGen2.target.accountkey)
  $endif$
>>


set_destination_spark_confs_snowflake(templateData)::=  <<
$(["set_destination_spark_confs_",first(templateData.table_metadata_output).file_system_type])(templateData)$
>>

set_destination_spark_confs_s3a(templateData)::=<<
Util.setUpSparkConfiguration("fs.s3a.endpoint",$if(first(templateData.table_metadata_output).filesystem_endpoint)$"$first(templateData.table_metadata_output).filesystem_endpoint$" $else$ "" $endif$ )
val endPointTarget = spark.conf.get("spark.driver.target_aws_endPoint").toString
Util.setUpSparkConfiguration("fs.s3a.endpoint",endPointTarget)
Util.setUpSparkConfiguration("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")

$if(first(templateData.table_metadata_output).filesystem_credential_id)$
Util.setUpSparkConfiguration("fs.s3a.access.key", Aws.target.access_id)
Util.setUpSparkConfiguration("fs.s3a.secret.key", Aws.target.secret_access_key)
$endif$
Util.setUpSparkConfiguration("fs.s3a.fast.upload", "true")
Util.setUpSparkConfiguration("fs.s3a.multiobjectdelete.enable", "false")
Util.setUpSparkConfiguration("mapreduce.fileoutputcommitter.algorithm.version", "2")
Util.setUpSparkConfiguration("fs.s3a.fast.upload.buffer", "bytebuffer")
spark.conf.set("spark.speculation", "false")
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
spark.conf.set("spark.sql.parquet.mergeSchema", "false")
>>

set_destination_spark_confs_hdfs(templateData)::=<<

>>


Configuration_s3(templateData)::=<<
val endPointSource = spark.conf.get("spark.driver.source_aws_endPoint").toString
Util.setUpSparkConfiguration("fs.s3a.endpoint",endPointSource)
Util.setUpSparkConfiguration("fs.s3a.access.key", Aws.source.access_id)
Util.setUpSparkConfiguration("fs.s3a.secret.key", Aws.source.secret_access_key)
>>

Configuration_adls_gen2(templateData)::=<<
 $if(first(templateData.table_metadata_output).is_source_azure_oauth)$
	spark.conf.set(s"fs.azure.account.auth.type.\${AzureOAuth.source.accountname}.dfs.core.windows.net", "OAuth")
	spark.conf.set(s"fs.azure.account.oauth.provider.type.\${AzureOAuth.source.accountname}.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
	spark.conf.set(s"fs.azure.account.oauth2.client.id.\${AzureOAuth.source.accountname}.dfs.core.windows.net", s"\${AzureOAuth.source.client_id}")
	spark.conf.set(s"fs.azure.account.oauth2.client.secret.\${AzureOAuth.source.accountname}.dfs.core.windows.net", s"\${AzureOAuth.source.client_secret}")
	spark.conf.set(s"fs.azure.account.oauth2.client.endpoint.\${AzureOAuth.source.accountname}.dfs.core.windows.net", s"https://login.microsoftonline.com/\${AzureOAuth.source.tenant_id}/oauth2/token")
 $else$
	spark.conf.set(s"fs.azure.account.key.\${AzureGen2.source.accountname}.dfs.core.windows.net", AzureGen2.source.accountkey)
 $endif$
>>

Configuration_azure_synapse(templateData)::=<<
  spark.conf.set(s"fs.azure.account.key.\${AzureGen2.source.accountname}.dfs.core.windows.net", AzureGen2.source.accountkey)
>>

Configuration_gcs(templateData)::=<<
import io.circe.generic.auto._
import io.circe.syntax._

spark.conf.set("credentials", Util.encodeBase64(Gcp.source.asJson.noSpaces))
Util.setUpSparkConfiguration("fs.gs.impl","com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem")
Util.setUpSparkConfiguration("fs.AbstractFileSystem.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS")
Util.setUpSparkConfiguration("fs.gs.auth.service.account.email", Gcp.source.client_email)
Util.setUpSparkConfiguration("fs.gs.auth.service.account.private.key.id", Gcp.source.private_key_id)
Util.setUpSparkConfiguration("fs.gs.auth.service.account.private.key", Gcp.source.private_key)
>>

Configuration_unix(templateData)::=<<

>>

file_options(templateData)::=<<
Map("header" -> "$templateData.has_header$", "delimiter" -> "$templateData.delimiter$", $if(templateData.quote_char)$"quote" -> "\\$templateData.quote_char$",$endif$  $if(templateData.escape_char)$"escape" -> "\\$templateData.escape_char$",$endif$ "lineSep" -> "$templateData.linesep$")
>>

s3_source(templateData)::=<<
s3a://$first(templateData.table_metadata_output).source_s3_bucket$/$first(templateData.table_metadata_output).file_relative_path$
>>

adls_gen2_source(templateData)::=<<
abfss://$first(templateData.table_metadata_output).source_adls_gen2_container$@$first(templateData.table_metadata_output).source_storage_account$.dfs.core.windows.net/$first(templateData.table_metadata_output).file_relative_path$
>>

unix_source(templateData)::=<<
file://$first(templateData.table_metadata_output).file_absolute_path$
>>

gcs_source(templateData)::=<<
$first(templateData.table_metadata_output).file_absolute_path$
>>

file_azure_synapse_destination(templateData)::=<<
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()
>>

hive_source(templateData)::=<<
"$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

sap_hana_source(templateData)::=<<
"jdbc:sap://$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

oracle_source(templateData)::=<<
"jdbc:oracle:thin:@$first(templateData.table_metadata_output).source_url$:$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

mysql_source(templateData)::=<<
"jdbc:mysql://$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

postgres_source(templateData)::=<<
"jdbc:postgresql://$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

redshift_source(templateData)::=<<
"jdbc:redshift://$first(templateData.table_metadata_output).source_url$/$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

sql_server_source(templateData)::=<<
"jdbc:jtds:sqlserver://$first(templateData.table_metadata_output).source_url$;databaseName=$first(templateData.table_metadata_output).source_db$","$first(templateData.table_metadata_output).source_jdbc_driver$"
>>

hive_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$endif$
>>

hdfs_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).path$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).table_name$").save()$endif$
>>

bigquery_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).gcs_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).gcs_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$").save()
$endif$
>>

bigquery_file_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).gcs_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).gcs_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$").save()$endif$
>>

redshift_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$first(templateData.table_metadata_output).data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()
$else$
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$first(templateData.table_metadata_output).data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$").save()
$endif$
>>

s3_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).table_name$").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).table_name$").save()$endif$
>>

azure_synapse_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$endif$
>>

adls_gen2_destination(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$else$option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).source_dataplace_id$/$first(templateData.table_metadata_output).directory_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$endif$
>>

oracle_destination(templateData)::=<<
"jdbc:oracle:thin:@$first(templateData.table_metadata_output).destination_url$:$first(templateData.table_metadata_output).destination_db$","$first(templateData.table_metadata_output).destination_jdbc_driver$"
>>

postgres_destination(templateData)::=<<
"jdbc:postgresql://$first(templateData.table_metadata_output).destination_url$/$first(templateData.table_metadata_output).destination_db$","$first(templateData.table_metadata_output).destination_jdbc_driver$"
>>

redshift_relational_destination(templateData)::=<<
"jdbc:redshift://$first(templateData.table_metadata_output).destination_url$/$first(templateData.table_metadata_output).destination_db$","$first(templateData.table_metadata_output).destination_jdbc_driver$"
>>

adls_gen2_destination_YYYY_MM_DD_HH_mm_SS_format(templateData)::=<<
$if(first(templateData.table_metadata_output).source_db)$option("path","$([first(templateData.table_metadata_output)
.destination_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output)
.adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$first(templateData.table_metadata_output).upper_case_destination_table_name$/$first
(templateData.table_metadata_output).destination_folder_format$/$first(templateData.table_metadata_output)
.destination_table_name$.parquet").save()$else$option("path","$([first(templateData.table_metadata_output)
.file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output)
.adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$first(templateData.table_metadata_output).upper_case_destination_table_name$/$
(templateData.table_metadata_output).destination_folder_format$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()$endif$
>>

drop_hive_table(templateData)::=<<
DROP TABLE IF EXISTS $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$`
>>

drop_redshift_table(templateData)::=<<
DROP TABLE IF EXISTS $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$ cascade
>>

create_backup_hive_table(templateData)::=<<
CREATE TABLE IF NOT EXISTS `$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).backup_new_table_name$$else$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_backup_table_name$$endif$`
as SELECT * from `$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.query_input.table_metadata_output).destination_table_name$`
>>

create_bigquery_bkp_table(templateData)::=<<
CREATE OR REPLACE TABLE $templateData.destination_schema_name$.$templateData.destination_backup_table_name$ as select * from $templateData.destination_schema_name$.$templateData.destination_table_name$;
>>

create_bigquery_table(templateData)::=<<
CREATE OR REPLACE EXTERNAL TABLE $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
WITH PARTITION COLUMNS(
$templateData.query_input.metadata.data.data.templateData.query_input.get_partition_cols:getColumns();separator = ","$
)$endif$
OPTIONS(
format = 'parquet',
uris=['$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$']
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
,hive_partition_uri_prefix='$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).gcs_bucket$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).gcs_bucket$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$endif$'$endif$
)
>>

uris_drift(templateData)::=<<
uris=['$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$']
>>

create_redshift_table(templateData)::=<<
$(["create_redshift_table_",templateData.query_input.source_metadata_category])(templateData)$
>>

create_redshift_table_relational(templateData)::=<<
CREATE EXTERNAL TABLE $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
(
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
$templateData.query_input.metadata.data.data.templateData.query_input.get_non_partition_cols:getColumnsRedshift();separator = ","$$if(first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).add_audit_columns)$,
$templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_audit_cols:getUnQuotedCdcColumns();separator = ","$
$endif$
$else$
$templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:getColumnsRedshift();separator = ","$$if(first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).add_audit_columns)$,
$templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_audit_cols:getUnQuotedCdcColumns();separator = ","$
$endif$
$endif$
)$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
PARTITIONED BY ($templateData.query_input.metadata.data.data.templateData.query_input.get_partition_cols:getColumnsRedshift();separator = ","$ )$endif$

stored as $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).intermediate_type$
$location_drift(templateData)$

>>

create_redshift_table_file(templateData)::=<<
CREATE EXTERNAL TABLE $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
(
$templateData.query_input.metadata.data.data.templateData.query_input.column_metadata_output:getColumnsRedshift();separator = ","$
)
stored as $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).intermediate_type$
$location_drift(templateData)$

>>

alter_redshift_table(templateData)::=<<
ALTER TABLE $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
ADD
$templateData.query_input.metadata.botLogicOutputMap.get_conditions_for_alter:partition_statements(templateData);separator = "\n"$
>>

partition_statements(condition,templateData)::=<<
PARTITION ($condition.partition_condition$)
LOCATION '$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$$condition.partition_path$/'
>>

location_drift(templateData)::=<<
LOCATION
  '$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$'
>>

create_hive_table(templateData)::=<<
$(["create_hive_table_",templateData.query_input.source_metadata_category])(templateData)$
>>

create_hive_table_relational(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
(
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
$templateData.query_input.metadata.data.data.templateData.query_input.get_non_partition_cols:getColumns();separator = ","$$if(first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).add_audit_columns)$,
$templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_audit_cols:getCdcColumns();separator = ","$
$endif$
$else$
$templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:getColumns();separator = ","$$if(first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).add_audit_columns)$,
$templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_audit_cols:getCdcColumns();separator = ","$
$endif$
$endif$
)$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
 PARTITIONED BY ($templateData.query_input.metadata.data.data.templateData.query_input.get_partition_cols:getColumns();separator = ","$ )$endif$
$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).intermediate_type,"_drift"])(templateData)$
>>


getCdcColumns(columnMap)::=<<
`$columnMap.field_name$` $columnMap.destination_datatype_name$
>>

getUnQuotedCdcColumns(columnMap)::=<<
$columnMap.field_name$ $columnMap.destination_datatype_name$
>>

repair_hive_table(templateData)::=<<
MSCK REPAIR TABLE $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$`
>>

create_hive_table_file(templateData)::=<<
CREATE EXTERNAL TABLE if not exists $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.`$if(first(templateData.query_input.metadata.data.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$`
(
 $templateData.query_input.metadata.data.data.templateData.query_input.column_metadata_output:getColumns();separator = ","$
)
$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).intermediate_type,"_drift"])(templateData)$
>>


parquet_drift(templateData)::=<<
ROW FORMAT SERDE
  'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
STORED AS INPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
OUTPUTFORMAT
  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION
  '$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$'
>>

avro_drift(templateData)::=<<
ROW FORMAT SERDE
    'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT
    'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION
  '$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_type,"_location_drift"])(templateData)$'
>>

getColumns(columnMap)::=<<
$if(columnMap.istimezone)$
`$columnMap.field_name$` STRING, `$columnMap.field_name$_utc` $columnMap.destination_datatype_name$
$else$
`$columnMap.field_name$` $columnMap.destination_datatype_name$
$endif$
>>

redshift_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$endif$
>>

s3_location(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$endif$
>>

bigquery_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).gcs_bucket$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/*.parquet$else$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type_destination"])(templateData)$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).gcs_bucket$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/*.parquet$endif$
>>

hive_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$else$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/$endif$
>>

hdfs_location_drift(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db)$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).table_name$/$else$$([first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).path$/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).table_name$/$endif$
>>

hdfs_file_system_type(templateData)::=<<
hdfs://
>>

abfss_file_system_type(templateData)::=<<
abfss://
>>

s3a_file_system_type(templateData)::=<<
s3a:/
>>

S3A_file_system_type(templateData)::=<<
s3a:/
>>

s3_file_system_type(templateData)::=<<
s3a:/
>>

S3A_file_system_type_destination(templateData)::=<<
s3:/
>>

s3_file_system_type_destination(templateData)::=<<
s3:/
>>

adls_gen2_file_system_type(templateData)::=<<
abfss://
>>

gcs_file_system_type(templateData)::=<<
gs:/
>>

gcs_file_system_type_destination(templateData)::=<<
gs:/
>>

redshift_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "jdbc:redshift://$first(templateData.query_input.table_metadata_output).destination_url$/$first(templateData.query_input.table_metadata_output).destination_db$",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info": {
		"file_system_type" : "$first(templateData.query_input.table_metadata_output).file_system_type$",
		"bucket": "$first(templateData.query_input.table_metadata_output).s3_bucket$",
		"credential_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

azure_synapse_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"end_point": "$templateData.query_input.input_data.end_point$",
    "jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
    "jdbc_url": "jdbc:sqlserver://$first(templateData.query_input.table_metadata_output).destination_url$;database=$first(templateData.query_input.table_metadata_output).destination_db$;",
    "credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
    "credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
        "file_system_type" : "ADLS_GEN2",
        "credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
        "credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

azure_synapse_scoped_credential_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_azure_oauth)$
IF NOT EXISTS (select top 1 * from sys.database_scoped_credentials
where credential_identity='<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token'
and name = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential')
BEGIN
    CREATE DATABASE SCOPED CREDENTIAL [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential]
    WITH IDENTITY = '<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token',
    SECRET = '<client_secret>'
END;
$else$
IF NOT EXISTS (select top 1 * from sys.database_scoped_credentials
where credential_identity='$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$'
and name = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_credential')
BEGIN
    CREATE DATABASE SCOPED CREDENTIAL [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_credential]
    WITH IDENTITY = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$',
    SECRET = '<access_key>'
END;
$endif$
>>

azure_synapse_external_data_source_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_azure_oauth)$
IF NOT EXISTS (select top 1 * from sys.external_data_sources eds
inner join sys.database_scoped_credentials dsc on eds.credential_id = dsc.credential_id
where location ='abfss://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net'
and credential_identity='<client_id>@https://login.microsoftonline.com/<tenant_id>/oauth2/token'
and eds.name = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source')
BEGIN
CREATE EXTERNAL DATA SOURCE
[$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source]
WITH
(
	LOCATION = 'abfss://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net',
	CREDENTIAL = [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_credential],
	TYPE = HADOOP
)
$else$
IF NOT EXISTS (select top 1 * from sys.external_data_sources eds
inner join sys.database_scoped_credentials dsc on eds.credential_id = dsc.credential_id
where location ='abfss://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net'
and credential_identity='$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$' and
eds.name = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source')
BEGIN
CREATE EXTERNAL DATA SOURCE [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source]
WITH
(
	LOCATION = 'abfss://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$@$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$.dfs.core.windows.net',
	CREDENTIAL = [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_credential],
	TYPE = HADOOP
)
END;
$endif$
>>

azure_synapse_external_file_format_create(templateData)::=<<
IF NOT EXISTS (select top 1 * from sys.external_file_formats where name = 'parquetFileFormat' and data_compression = 'org.apache.hadoop.io.compress.SnappyCodec' and format_type = 'PARQUET')
BEGIN
CREATE EXTERNAL FILE FORMAT parquetFileFormat
WITH
(
    FORMAT_TYPE = PARQUET,
    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
)
END;
>>

azure_synapse_external_table_create(templateData)::=<<
$(["azure_synapse_external_table_create_",templateData.query_input.source_metadata_category])(templateData)$
>>

azure_synapse_external_table_create_relational(templateData)::=<<
IF EXISTS (SELECT top 1 * FROM sys.external_tables et
INNER JOIN sys.external_data_sources ds
ON et.data_source_id = ds.data_source_id
INNER JOIN sys.external_file_formats ff
ON et.file_format_id = ff.file_format_id
INNER JOIN sys.schemas s
ON s.schema_id = et.schema_id
where et.name = '$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$'
and s.name = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$')
BEGIN
DROP EXTERNAL TABLE $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$](
$templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:getColumnsAzureSynapse();separator = ",\n"$$if(first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).add_audit_columns)$,
$templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_audit_cols:getUnQuotedCdcColumns();separator = ",\n"$
$endif$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END
ELSE
BEGIN

CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$](
$templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:getColumnsAzureSynapse();separator = ",\n"$$if(first(templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_info).add_audit_columns)$,
$templateData.query_input.metadata.data.data.templateData.query_input.get_cdc_audit_cols:getUnQuotedCdcColumns();separator = ",\n"$
$endif$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END;
>>

azure_synapse_external_table_create_file(templateData)::=<<
IF EXISTS (SELECT top 1 * FROM sys.external_tables et
INNER JOIN sys.external_data_sources ds
ON et.data_source_id = ds.data_source_id
INNER JOIN sys.external_file_formats ff
ON et.file_format_id = ff.file_format_id
INNER JOIN sys.schemas s
ON s.schema_id = et.schema_id
where et.name = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$'
and s.name = '$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$')
BEGIN
DROP EXTERNAL TABLE $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.data.templateData.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END
ELSE
BEGIN
CREATE EXTERNAL TABLE [$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$].[$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$](
$templateData.query_input.metadata.data.data.templateData.query_input.column_metadata_output:getColumnsAzureSynapse();separator = ",\n"$
)
WITH (
	LOCATION = '$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).directory_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet',
    DATA_SOURCE = [$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_azure_oauth)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_oauth2_data_source$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_data_source$endif$],
    FILE_FORMAT = parquetFileFormat,
    REJECT_TYPE = VALUE,
    REJECT_VALUE = 0
)
END;
>>

getColumnsbigquery(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50), $columnMap.field_name$_utc $columnMap.destination_datatype_name$
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$
$endif$
>>

getColumnsRedshift(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50), $columnMap.field_name$_utc $columnMap.destination_datatype_name$
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$
$endif$
>>

getColumnsAzureSynapse(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50), $columnMap.field_name$_utc $columnMap.destination_datatype_name$
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$
$endif$
>>

get_destination_type(templateData)::=<<
"$templateData.query_input.input_data.destination$"
>>

bigquery_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "jdbc:bigquery://https://www.googleapis.com/bigquery/v2:443",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
		"file_system_type" : "gcs",
		"credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

hive_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    $if(first(templateData.query_input.table_metadata_output).is_destination_credential_kerberos)$
    "authentication_type":"kerberos",
    $else$
    "authentication_type":"auth_token",
    $endif$
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	$if(first(templateData.query_input.table_metadata_output).is_apache_jdbc_driver)$
    "jdbc_driver": "org.apache.hive.jdbc.HiveDriver",
    $else$
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	$endif$
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "$first(templateData.query_input.table_metadata_output).destination_url$",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
		"file_system_type" : "$first(templateData.query_input.table_metadata_output).file_system_type$",
		"credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

Configuration_s3a(templateData)::=<<
Util.setUpSparkConfiguration("fs.s3a.endpoint",$if(first(templateData.table_metadata_output).filesystem_endpoint)$"$first(templateData.table_metadata_output).filesystem_endpoint$" $else$ "" $endif$ )
val endPointTarget = spark.conf.get("spark.driver.target_aws_endPoint").toString
Util.setUpSparkConfiguration("fs.s3a.endpoint",endPointTarget)
Util.setUpSparkConfiguration("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
$if(first(templateData.table_metadata_output).filesystem_credential_id)$
Util.setUpSparkConfiguration("fs.s3a.access.key", Aws.target.access_id)
Util.setUpSparkConfiguration("fs.s3a.secret.key", Aws.target.secret_access_key)
$endif$
Util.setUpSparkConfiguration("fs.s3a.fast.upload", "true")
Util.setUpSparkConfiguration("fs.s3a.multiobjectdelete.enable", "false")
Util.setUpSparkConfiguration("mapreduce.fileoutputcommitter.algorithm.version", "2")
Util.setUpSparkConfiguration("fs.s3a.fast.upload.buffer", "bytebuffer")
spark.conf.set("spark.speculation", "false")
spark.conf.set("spark.sql.parquet.filterPushdown", "true")
spark.conf.set("spark.sql.parquet.mergeSchema", "false")
>>

Configuration_hdfs(templateData)::=<<

>>

oracle_parquet_snowflake_intermediate_stg_function()::=<<
case
                when (b.source_datatype_name = 'number') then
                (
                        CASE
                    when (data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
                     else 'AsIs'
                END
                )
                when (b.source_datatype_name = 'float') then
            (
                    CASE
                                when (a.data_precision between 0 and 53) then 'ParquetStringToDoubleConvert'
                                else 'AsIs'
                        END
                )
                else b.intermediate_stg_function
        end
>>

oracle_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

oracle_parquet_snowflake_inconsistent_datatype() ::=<<
case
    when (b.source_datatype_name in ('number','decimal')) then
    (
        case
            when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
            else 'number_to_varchar'
        end
    )
    when (b.source_datatype_name ~* 'interval year') then 'interval_year_to_month_to_varchar'
    when (b.source_datatype_name ~* 'interval day') then 'interval_day_to_second_to_varchar'
    when (b.source_datatype_name in ('rowid','urowid','xmltype','blob','date'))then concat(b.source_datatype_name,'_to_varchar')
    when (b.source_datatype_name = 'float') then
    (
        case
            when (a.data_precision <=53) then null
            else 'float_to_varchar'
        end
    )
    when (b.source_datatype_name ~* 'local time zone') then 'timestamp_with_local_time_zone_to_varchar'
    when (b.source_datatype_name ~* 'time zone') then 'timestamp_with_time_zone_to_varchar'
    when (b.source_datatype_name ~* 'timestamp') then 'timestamp_to_varchar'
    else null
end
>>

oracle_parquet_snowflake_destination_datatype_name()::=<<
case
           when (b.source_datatype_name = 'number') then
           (
              CASE
                 when ((a.data_scale <= 37) and (a.data_precision <= 38)) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
                 else 'STRING'
              END
           )
           when (b.source_datatype_name = 'float') then
           (
                  case
                         when (a.data_precision between 0 and 53) then 'DOUBLE'
                         else 'STRING'
                  END
           )
           when(b.source_datatype_name = 'varchar2' or b.source_datatype_name = 'nvarchar2') then
           (
              CASE
                 when (a.data_length between 1 and 4000) then concat(b.destination_datatype_name,'(',a.data_length,')')
                 else 'STRING'
              END
           )
           when(b.source_datatype_name = 'char' or b.source_datatype_name = 'nchar') then
           (
              CASE
                 when (a.data_length between 1 and 2000) then concat(b.destination_datatype_name,'(',a.data_length,')')
                 else 'STRING'
              END
           )
           else b.destination_datatype_name
        end
>>

sql_server_parquet_snowflake_destination_datatype_name()::=<<
CASE
        when (b.source_datatype_name in ('char','varchar','nchar','nvarchar')) then
        (
            CASE
                when (a.data_length = -1) then concat(b.destination_datatype_name,'(',16777216,')')
                else concat(b.destination_datatype_name,'(', a.data_length,')')
            END
        )
        when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
        (
            CASE
                when (data_precision between 1 and 38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
                else 'STRING'
            END
        )
        else b.destination_datatype_name
END
>>

sql_server_parquet_snowflake_intermediate_stg_function()::=<<
CASE
        when (b.source_datatype_name = 'decimal' or b.source_datatype_name = 'numeric') then
        (
            CASE
                when (a.data_scale <= 37 and (a.data_precision <= 38)) then b.intermediate_stg_function
                else 'AsIs'
            END
        )
        else b.intermediate_stg_function
END
>>

mysql_parquet_snowflake_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    else b.intermediate_stg_function
END
>>

mysql_parquet_snowflake_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('CHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('VARCHAR','(',a.data_length,')')
			else 'STRING'
		END
    )
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('DECIMAL','(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
    )
    else b.destination_datatype_name
END
>>

postgres_parquet_snowflake_destination_datatype_name()::=<<
CASE
            when (b.source_datatype_name = 'character' and a.data_length = null ) then b.destination_datatype_name

            when (b.source_datatype_name = 'character' and a.data_length is not null) then concat('char','(',a.data_length,')')

            when (b.source_datatype_name = 'character varying' or b.source_datatype_name = 'bit' or b.source_datatype_name = 'bit varying') then concat('varchar','(',a.data_length,')')

            when (b.source_datatype_name = 'numeric') then
            (
                CASE
                    when ((a.data_scale<=37) and a.data_precision <=38) then concat(b.destination_datatype_name,'(',a.data_precision,',',a.data_scale,')')
                    else concat('varchar','(',a.data_precision+1,')')
            END
            )
            else b.destination_datatype_name
        end
>>

postgres_parquet_snowflake_intermediate_stg_function()::=<<
CASE
            when (b.source_datatype_name = 'character') then
            (
                CASE
                    when (a.data_length between 1 and 255) then b.intermediate_stg_function
                    else 'AsIs'
                END
            )
            when (b.source_datatype_name='character varying') then
            (
                CASE
                    when (a.data_length between 1 and 65535) then b.intermediate_stg_function
                    else 'AsIs'
                END
            )
            when (b.source_datatype_name = 'numeric') then
            (
                CASE
                    when (a.data_precision between 1 and 38) then b.intermediate_stg_function
                    else 'AsIs'
                END
            )
            else b.intermediate_stg_function
        end
>>

sql_server_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

postgres_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

mysql_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

sql_server_parquet_snowflake_inconsistent_datatype() ::=<<
case
when (b.source_datatype_name in ('bit','datetime2','time','datetimeoffset','binary','varbinary','timestamp','smalldatetime','datetime')) then concat(b.source_datatype_name,'_to_string')
else null
end
>>

mysql_parquet_snowflake_inconsistent_datatype() ::=<<
case
when (b.source_datatype_name = 'decimal') then
(
case
when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
else 'decimal_to_string'
end
)
when (b.source_datatype_name = 'year') then concat(b.source_datatype_name,'_to_int')
when (b.source_datatype_name in ('time','bit','binary','varbinary','tinyblob','blob','mediumblob','longblob','timestamp','datetime')) then concat(b.source_datatype_name,'_to_string')
else null
end
>>

postgres_parquet_snowflake_inconsistent_datatype() ::=<<
case
                when (b.source_datatype_name = 'numeric') then
                (
                    case
                        when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
                        else 'numeric_to_varchar'
                    end
                )
                when (b.source_datatype_name = 'time without time zone') then 'time_without_time_zone_to_varchar'
                when (b.source_datatype_name = 'time with time zone') then 'time_with_time_zone_to_varchar'
                when (b.source_datatype_name = 'timestamp with time zone') then 'timestamp_with_time_zone_to_varchar'
                when (b.source_datatype_name = 'timestamp without time zone') then 'timestamp_without_time_zone_to_varchar'
                when (b.source_datatype_name in ('money','bit','abstime')) then concat(b.source_datatype_name,'_to_varchar')
                else null
            end
>>

hive_parquet_snowflake_source_stg_function()::=<<
$get_source_stg_function()$
>>

hive_parquet_snowflake_inconsistent_datatype() ::=<<
case
when (b.source_datatype_name = 'decimal') then
(
case
when (a.data_scale <= 37 and (a.data_precision <= 38)) then null
else 'decimal_to_string'
end
)
when (b.source_datatype_name in ('binary','map','struct','array')) then concat(b.source_datatype_name,'_to_string')
else null
end
>>

hive_parquet_snowflake_intermediate_stg_function()::=<<
CASE
    when (b.source_datatype_name='decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then b.intermediate_stg_function
			else 'AsIs'
		END
    )
    else b.intermediate_stg_function
END
>>

hive_parquet_snowflake_destination_datatype_name()::=<<
CASE
    when (b.source_datatype_name = 'char') then
    (
		CASE
			when (a.data_length between 1 and 255) then concat('char','(',a.data_length,')')
			else 'string'
		END
    )
    when (b.source_datatype_name = 'varchar') then
    (
		CASE
			when (a.data_length between 1 and 65535) then concat('varchar','(',a.data_length,')')
			else 'string'
		END
    )
    when (b.source_datatype_name = 'decimal') then
    (
		CASE
			when (a.data_precision between 1 and 38) then concat('decimal','(',a.data_precision,',',a.data_scale,')')
			else 'STRING'
		END
    )
    else b.destination_datatype_name
END
>>

snowflake_database_config(templateData)::=<<
{
	"jwt_token":"$templateData.query_input.input_data.jwt_token$",
    "authentication_type":"auth_token",
	"truststore":false,
	"end_point": "$templateData.query_input.input_data.end_point$",
	"jdbc_driver": "$first(templateData.query_input.table_metadata_output).destination_jdbc_driver$",
	"maxconnectionsperpartition": "1",
	"minconnectionsperpartition" : "0",
	"jdbc_url": "jdbc:snowflake://$first(templateData.query_input.table_metadata_output).destination_url$",
	"credential_id": "$first(templateData.query_input.table_metadata_output).destination_credential_id$",
	"credential_type_id" : "$first(templateData.query_input.table_metadata_output).destination_credential_type_id$",
	"file_system_info" : {
		"file_system_type" : "$first(templateData.query_input.table_metadata_output).file_system_type$",
		"credential_id": "$first(templateData.query_input.table_metadata_output).filesystem_credential_id$",
		"credential_type_id" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type_id$",
		"credential_type" : "$first(templateData.query_input.table_metadata_output).filesystem_credential_type$"
    }
}
>>

snowflake_destination(templateData)::=<<
$(["snowflake_",first(templateData.table_metadata_output).file_system_type,"_destination"])(templateData)$
>>

snowflake_s3_destination(templateData)::=<<
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$/$first(templateData.table_metadata_output).s3_bucket$/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$").save()
>>

snowflake_adls_gen2_destination(templateData)::=<<
option("path","$([first(templateData.table_metadata_output).file_system_type,"_file_system_type"])(templateData)$$first(templateData.table_metadata_output).adls_gen2_container$@$first(templateData.table_metadata_output).storage_account$.dfs.core.windows.net/$templateData.input_data.data_movement_id$/$first(templateData.table_metadata_output).dataplace_id$/$first(templateData.table_metadata_output).source_db$/$first(templateData.table_metadata_output).schema_name$/$first(templateData.table_metadata_output).destination_table_name$.parquet").save()
>>

snowflake_location_drift(templateData)::=<<
$(["snowflake_",first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).file_system_type,"_location_drift"])(templateData)$
>>

snowflake_s3_location_drift(templateData)::=<<
"$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_stage"/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$/
>>

snowflake_adls_gen2_location_drift(templateData)::=<<
"$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_stage"/$templateData.query_input.metadata.data.data.templateData.query_input.input_data.data_movement_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).dataplace_id$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).source_db$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).schema_name$/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$.parquet/
>>

create_snowflake_bkp_table(templateData)::=<<
CREATE OR REPLACE TABLE "$templateData.destination_db$"."$templateData.destination_schema_name$"."$templateData.destination_backup_table_name$" as select * from "$templateData.destination_db$"."$templateData.destination_schema_name$"."$templateData.destination_table_name$";
>>

get_partition_information_test(templateData)::=<<
with cte1 as(
select schema_name,table_name,partition_name,column_name,column_id,partition_function_name ,
case when partition_position is null then 1 else partition_position end as partition_position,sub_partition_name,
case when sub_partition_name is not null then true
else false
end as is_sub_partition
from
(select
t.schema_name,t.table_name,t.partition_name,t4.column_name,t4.column_id,t.partition_function_name , t.partition_position ,t2.sub_partition_name
from (select pi2.schema_name ,pi2.partition_name , pti.partition_function_name  , pi2.partition_position,
pti.partition_type,pti.table_id,pti.table_name
from nabu.partition_table_info pti
left join nabu.partitions_info pi2 on pti.table_id = pi2.table_id
where pti.table_id in ($templateData.input_data.where_condition$) and pti.valid_to_ts = '9999-12-31' and pi2.valid_to_ts = '9999-12-31' ) t
left join
(select sub_partition_name,spi.partition_name from nabu.sub_partitions_info spi
where spi.table_id in ($templateData.input_data.where_condition$) and spi.valid_to_ts = '9999-12-31') t2 on t.partition_name = t2.partition_name
left join (select column_name,column_id,pci.table_id from nabu.partition_column_info pci
where pci.table_id in ($templateData.input_data.where_condition$) and pci.valid_to_ts = '9999-12-31') t4 on t.table_id  = t4.table_id
)t3
)
select a.*,
case when is_int is true then a.partition_name else concat(column_name,'=', E'\'', split_part(partition_name, '=',2),E'\'') end as column_value
from(
select cte1.*, case when data_type like '%decimal%' or data_type like '%int%' then true else false end as is_int
from cte1,
nabu.dataplace_column_metadata_physical a where a.column_id = cte1.column_id and cte1.partition_name = partition_name
and a.valid_to_ts ='9999-12-31'
) a
>>

snowflake_storage_integration_create(templateData)::=<<
$(["snowflake_",templateData.query_input.database_config.file_system_info.file_system_type,"_stage_create"])(templateData)$
>>

snowflake_stage_create(templateData)::=<<
$(["snowflake_",templateData.query_input.database_config.file_system_info.file_system_type,"_stage_create"])(templateData)$
>>

snowflake_adls_gen2_storage_integration_create(templateData)::=<<
CREATE STORAGE INTEGRATION IF NOT EXISTS "$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_storage_integration"
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = AZURE
AZURE_TENANT_ID = '<tenant_id>'
STORAGE_ALLOWED_LOCATIONS = ('azure://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$')
>>

snowflake_adls_gen2_stage_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_azure_oauth)$
CREATE STAGE IF NOT EXISTS "$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_stage"
STORAGE_INTEGRATION = $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$..$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_storage_integration
URL='azure://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$'
$else$
CREATE STAGE IF NOT EXISTS "$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_stage"
credentials=(azure_sas_token='<sas_token>')
URL='azure://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).storage_account$.blob.core.windows.net/$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).adls_gen2_container$'
$endif$
>>

snowflake_s3_storage_integration_create(templateData)::=<<
CREATE STORAGE INTEGRATION IF NOT EXISTS "$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_storage_integration"
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = S3
STORAGE_AWS_ROLE_ARN = '<iam_role>'
STORAGE_ALLOWED_LOCATIONS = ('s3://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$/')
>>

snowflake_s3_stage_create(templateData)::=<<
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).is_aws_iam)$
CREATE STAGE IF NOT EXISTS "$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_stage"
URL = 's3://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$/'
STORAGE_INTEGRATION = $first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$.$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$.$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_storage_integration
FILE_FORMAT = (TYPE=$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_file_format$)
$else$
CREATE STAGE IF NOT EXISTS
"$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$_$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).filesystem_credential_epoch$_stage"
URL = 's3://$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).s3_bucket$/'
CREDENTIALS = (AWS_KEY_ID = '<aws_access_id>' AWS_SECRET_KEY = '<aws_secret_key>')
FILE_FORMAT = (TYPE=$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_file_format$)
$endif$
>>

create_snowflake_table(templateData)::=<<
CREATE OR REPLACE EXTERNAL TABLE
"$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_db$"."$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_schema_name$"."$if(first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).rename_table_condition)$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).new_table_name$$else$$first(templateData.query_input.metadata.data.data.templateData.query_input.table_metadata_output).destination_table_name$$endif$"
(
$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
$templateData.query_input.metadata.data.data.templateData.query_input.get_non_partition_cols:getColumnsSnowflake();separator = ","$,
$templateData.query_input.metadata.data.data.templateData.query_input.get_partition_cols:getPartitionColumnsSnowflake();separator = ","$$else$
$templateData.query_input.metadata.data.data.templateData.query_input.destination_transformation_query_details_output:getColumnsSnowflake();separator = ","$$endif$
)$if(first(templateData.query_input.metadata.data.data.templateData.query_input.advanced_options_tables_flags_status).table_partitions)$
PARTITION BY($templateData.query_input.metadata.data.data.templateData.query_input.get_partition_cols:get_cte_names();separator = ","$)$endif$
WITH LOCATION = @$snowflake_location_drift(templateData)$
FILE_FORMAT = (TYPE= parquet)
AUTO_REFRESH = FALSE
PATTERN='.*[.]parquet'
>>

getPartitionColumnsSnowflake(columnMap)::=<<
$columnMap.field_name$ $columnMap.destination_datatype_name$ AS (split_part(split_part(metadata\$filename,'/',$columnMap.pos$),'=',2)::$columnMap.destination_datatype_name$)
>>

getColumnsSnowflake(columnMap)::=<<
$if(columnMap.istimezone)$
$columnMap.field_name$ VARCHAR(50) AS (VALUE:$columnMap.field_name$::VARCHAR(50)), $columnMap.field_name$_utc $columnMap.destination_datatype_name$ AS (VALUE:$columnMap.field_name$_utc::$columnMap.destination_datatype_name$)
$else$
$columnMap.field_name$ $columnMap.destination_datatype_name$ AS (VALUE:$columnMap.field_name$::$columnMap.destination_datatype_name$)
$endif$
>>

