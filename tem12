https://gist.github.com/SwaroopSittula/a78b496a0ee95a5abd9c021f7ec670a8

https/repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/8.2.0.jre8/mssql-jdbc-8.2.0.jre8.jar
"spark.nabu.compute_engine_libs_path":"/dbfs/tmp/dev-v3-1-2-0/nabu/17001329078704491/0/application.conf"

import com.typesafe.config.{Config, ConfigFactory}
import java.io.File
 
val configPath = "" //provide a dbfs path in which application.conf is uploaded
val config = ConfigFactory.parseFile(new File(configPath))
val url: String = config.getString("com.modak.db.url")
println ("URL: " + url)


[15:46] Manasa Kandimalla
import org.apache.spark.sql.SparkSession
 
val spark: SparkSession = SparkSession.builder().enableHiveSupport().getOrCreate()
 
val df = spark.read.format("jdbc").option("url", "").option("driver","").option("query", "").option("user", "").option("password", "").option("queryTimeout",s"""0""").option("fetchsize",s"""5000""").load()
 
df.show(false)

https://gist.github.com/manasa-kandimalla/6feaa06723b8ade7bc4402081a8b5210

https://gist.github.com/sb1205/d86a5409270ef36d9cad1ee1ee6d92c9#file-almaren
($resolveContactJson(first(data.input_data.data).contact_info)$)
https://gist.github.com/sb1205/41c645cad9df99dcba3b1e12bfd47765#file-query

https://gist.github.com/sb1205/4a14bb8b90b9b0b09067e5585e5cdf6c#file-parition_almaren

https://gist.github.com/sb1205/d0c41fd7bda4c98046c07003a516b50a#file-parallel

https://gist.github.com/sb1205/0e6636a6fe1c026da19465ee60fa45ff#file-templates_partition

Overall metrics for 3 tables:

 As a part of testng the features in qa env story, 
worked on ingesting oracle source assets with and without partitions
and also uo and asset level computes, 
cureently working on ingested recon and metadata validations.


THXFEES metrics for one partion (i.e partition no 15)

main query - Select where between (low_key subquery) and (high_key subquery) - 1 min 51 sec

low_key subquery - 167 ms

high_key subquery - 178 ms

main query - Select where between (57460) and (57999) - 412 ms

 

THXCOCD metrics for one partition (i.e partition no 1)

main query - Select where between (low_key subquery) and (high_key subquery) - 11 min 58 sec

low_key subquery - 216 ms

high_key subquery - 292 ms

main query - Select where between (57460) and (57999) - 1 min 14 sec

 

THXMOCL metrics for one partition (i.e partition no 9)

main query - Select where between (low_key subquery) and (high_key subquery) - 27 sec

low_key subquery - 174 ms

high_key subquery - 164 ms

main query - Select where between (57460) and (57999) - 247 ms

THXFEES

HASH PARALLEL-

Hash column: SCHEDULE_NBR

No of connections: 3

Records ingested: 540143642

Time taken for ingestion: 2 hrs 22 mins 47 secs



SRC PARALLEL-

Partition column: SCHEDULE_NBR

No of connections: 3

Records ingested: 540143642

Time taken for ingestion: 3 hrs 58 mins 25 secs

select * from nabu.dataplace_table_metadata_physical dtmp 
inner join nabu.dataplace_physical dp on dtmp.dataplace_id = dp.dataplace_id 
where dp.dataplace_sub_component_id =2 and table_type = 'V' and table_size is not null

 https://mvsforums.com/helpboards/viewtopic.php?p=11074#11074
CREATE FUNCTION TAN (X DOUBLE)
  RETURNS DOUBLE
  LANGUAGE SQL
  CONTAINS SQL
  DETERMINISTIC
  RETURN SIN(X)/COS(X);
 
gcloud_itoken = subprocess.check_output(["gcloud","auth", "print-identity-token"])
gcloud_itoken_str = gcloud_itoken.decode().strip()

select * from nabu.dataplace_table_metadata_physical dtmp 
inner join nabu.dataplace_physical dp on dtmp.dataplace_id = dp.dataplace_id 
where dp.dataplace_sub_component_id =2 and table_type = 'M' and table_size is not null



	val query="select * from nabu.dba  ".split(" ")
	val schema_nam=query(query.length-1).split(".")
	val schema_name="nabu.da".split("""\.""")
	print(schema_name(0), schema_name(1))
	
	
	var query="select from   nabu.dba  from where ca_id= n.a".split(" ").map(_.trim)
	query=query.filter(_ !="")
	query.foreach(println) 
	
	
	var index= query.indexOf("from")
	val schema_name=query(index+1).split("""\.""")

SELECT 
    t.name AS [Table], 
    c.name AS [Partitioning Column],
    TYPE_NAME(c.user_type_id) AS [Column Type],
    ps.name AS [Partition Scheme] 
FROM sys.tables AS t   
JOIN sys.indexes AS i   
    ON t.[object_id] = i.[object_id]   
    AND i.[type] <= 1
JOIN sys.partition_schemes AS ps   
    ON ps.data_space_id = i.data_space_id   
JOIN sys.index_columns AS ic   
    ON ic.[object_id] = i.[object_id]   
    AND ic.index_id = i.index_id   
    AND ic.partition_ordinal >= 1 
JOIN sys.columns AS c   
    ON t.[object_id] = c.[object_id]   
    AND ic.column_id = c.column_id   
WHERE t.name = 'Movies';

select count(*)
from sys.partitions
where object_id = object_id(@YourTableNameHere)

SELECT * FROM Cats
WHERE $PARTITION.CatsPartitionFunction(CatId) = 1;

select f.name as NameHere,f.type_desc as TypeHere
,(case when f.boundary_value_on_right=0 then 'LEFT' else 'Right' end) as LeftORRightHere
,v.value,v.boundary_id,t.name from sys.partition_functions f
inner join  sys.partition_range_values v
on f.function_id = v.function_id
inner join sys.partition_parameters p
on f.function_id = p.function_id
inner join sys.types t
on t.system_type_id = p.system_type_id


[12:35] Akanksha Chinthapally

Can you please try the same pipeline with giving this package in the dataproc scripts

org.apache.spark:spark-avro_2.12:3.1.3

${NABU_SPARK_BOT_HOME}/jars/https/repo1.maven.org/maven2/org/apache/spark/spark-avro_2.12/3.1.3/spark-avro_2.12-3.1.3.jar,\

heart 1
https://stackoverflow.com/questions/44805101/sql-concatenate-rows-into-one-field-db2
AnalysisException:
	Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of "Apache Avro Data Source Guide".
